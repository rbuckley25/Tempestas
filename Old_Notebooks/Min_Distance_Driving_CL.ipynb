{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832eda18",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Min Distance RL with CARLA CL & TL 12/01/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d54a80",
   "metadata": {},
   "source": [
    "At the recommednation of one of the authors of the paper I will try to adapt the following [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to work acheive the same results as in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257b48f",
   "metadata": {},
   "source": [
    "Notes: Pygame only runs on python 3.7, pytorch must be install direclty into the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d2016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "import gym_carla\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09526aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "connecting to Carla server...\n",
      "Carla server connected!\n",
      "WeatherParameters(cloudiness=5.000000, cloudiness=5.000000, precipitation=0.000000, precipitation_deposits=0.000000, wind_intensity=10.000000, sun_azimuth_angle=-1.000000, sun_altitude_angle=45.000000, fog_density=2.000000, fog_distance=0.750000, fog_falloff=0.100000, wetness=0.000000, scattering_intensity=1.000000, mie_scattering_scale=0.030000, rayleigh_scattering_scale=0.033100)\n",
      "372\n"
     ]
    }
   ],
   "source": [
    "# parameters for the gym_carla environment\n",
    "params = {\n",
    "    'number_of_vehicles': 0,\n",
    "    'number_of_walkers': 0,\n",
    "    'display_size': 256,  # screen size of bird-eye render\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': True,  # whether to use discrete control space\n",
    "    'discrete_acc': [2.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-0.3, 0.0, 0.3],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.3, 0.3],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.lincoln*',  # filter for defining ego vehicle\n",
    "    'port': 2000,  # connection port\n",
    "    'town': 'Town04',  # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 350,  # maximum timesteps per episode\n",
    "    'max_waypt': 12,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 1.8,  # threshold for out of lane\n",
    "    'desired_speed': 3,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 20,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "    'use_fixed':'E',\n",
    "    'weather':'ClearNoon'\n",
    "}\n",
    "\n",
    "# Set gym-carla environment\n",
    "env = gym.make('carla-v0', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d45005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5df687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lin2 = nn.Linear(9,30)\n",
    "        self.lin3 = nn.Linear(30,50)\n",
    "        self.lin4 = nn.Linear(50,25)\n",
    "        self.lin5 = nn.Linear(25,9)\n",
    "        self.lin6 = nn.Linear(9,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        x = F.relu(self.lin6(x))\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869cac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100\n",
    "TARGET_UPDATE = 50\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "test_net = DQN(n_actions).to(device)\n",
    "test_net.load_state_dict(torch.load('./model_params_CL/model_4.best'))\n",
    "#policy_net = DQN(n_actions).to(device)\n",
    "policy_net = test_net\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Model <= 7\n",
    "optimizer = optim.RMSprop(policy_net.parameters(),lr=0.005)\n",
    "\n",
    "#Model 8 & from 2022 \n",
    "#optimizer = optim.Adam(policy_net.parameters(),lr=0.005)\n",
    "\n",
    "\n",
    "memory = ReplayMemory(7500)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #arg max select the idex of the largest value and view changes shape from (1,) to (1,1)\n",
    "            #try test net\n",
    "            return policy_net(state.float()).argmax().view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18320225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    #reshape state_batch for nn\n",
    "  \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    \n",
    "    # selects column of output that was selceted \n",
    "    state_action_values = policy_net(torch.reshape(state_batch,(BATCH_SIZE,1,9)).float()).gather(1,action_batch)\n",
    "    \n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(torch.reshape(non_final_next_states,(list(non_final_next_states.shape)[0]//9,1,9)).float()).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c90a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 200\n",
    "use_fixed_idx = 0\n",
    "env.use_fixed = 'H'\n",
    "levels = ['E','M','H']\n",
    "next_lvl = 200\n",
    "min_overall_loss = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    rewards = []\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0],ego_dir[1][0],ego_dir[1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    loss = episode_loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _ , reward, done, info  = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if not done:\n",
    "            \n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            next_state = torch.tensor(next_state)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = optimize_model()\n",
    "        \n",
    "        #if loss is better than previous minimum save mode\n",
    "        if loss is not None and loss < episode_loss:\n",
    "            episode_loss = loss\n",
    "            writer.add_scalar(\"Loss/train\", loss, i_episode)\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "            \n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    len_episode = t+1\n",
    "    writer.add_scalar(\"Lenght/Epoch\", len_episode, i_episode)\n",
    "    \n",
    "    if i_episode > 0 and i_episode % next_lvl == 0:\n",
    "        #saving model after each level\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/model_6_'+levels[use_fixed_idx]+'.final')\n",
    "        print(levels[use_fixed_idx])\n",
    "        use_fixed_idx += 1\n",
    "        env.use_fixed = levels[use_fixed_idx]\n",
    "        steps_done = 0\n",
    "    \n",
    "    #save model if better than previous episode\n",
    "    if episode_loss < min_overall_loss:\n",
    "        min_overall_loss = episode_loss\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/model_6.best')\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa528f",
   "metadata": {},
   "source": [
    "### Visualise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef6c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (lin2): Linear(in_features=9, out_features=30, bias=True)\n",
       "  (lin3): Linear(in_features=30, out_features=50, bias=True)\n",
       "  (lin4): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (lin5): Linear(in_features=25, out_features=9, bias=True)\n",
       "  (lin6): Linear(in_features=9, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net = DQN(n_actions).to(device)\n",
    "test_net.load_state_dict(torch.load('./model_params_CL/model_6.best'))\n",
    "test_net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21657e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-5\n",
      "-6.041349910791865\n",
      "-5.925084851211872\n",
      "-6.2055622508155635\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "env.use_fixed = 'H'\n",
    "use_fixed_idx = 0\n",
    "levels = ['E','M','H']\n",
    "next_lvl = 4\n",
    "min_overall_loss = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    rewards = 0\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0],ego_dir[1][0],ego_dir[1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    loss = episode_loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an actionii#\n",
    "        with torch.no_grad():\n",
    "            action = test_net(state.float()).argmax().view(1,1)\n",
    "        _ , reward, done, info  = env.step(action.item())\n",
    "        rewards += reward\n",
    "            \n",
    "        #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "        pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "        ang = np.asarray(info['angular_vel'])\n",
    "        acc = np.asarray(info['acceleration'])\n",
    "        steer = np.asarray(info['steer'])\n",
    "        next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "        #update state\n",
    "        state = torch.tensor(next_state)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            print(reward)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141838a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
