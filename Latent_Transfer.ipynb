{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import carla\n",
    "from utils import AE_initalize_weights\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        #latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = PerceptionNet()\n",
    "servant = PerceptionNet()\n",
    "gan = CycleGan()\n",
    "\n",
    "master.to(device)\n",
    "master.load_state_dict(torch.load('./AE_params/model_43.best'))\n",
    "master.eval()\n",
    "\n",
    "servant.to(device)\n",
    "servant.apply(AE_initalize_weights)\n",
    "servant.train()\n",
    "\n",
    "gan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_transform = CropResizeTransform(64,14,50,100,(128,128))\n",
    "town03_data = CustomImageDataset('ClearNoon','Town03',test=False)\n",
    "town03_test_data = CustomImageDataset('ClearNoon','Town03',test=True)\n",
    "town04_data = CustomImageDataset('ClearNoon','Town04',test=False)\n",
    "town04_test_data = CustomImageDataset('ClearNoon','Town04',test=True)\n",
    "town07_data = CustomImageDataset('ClearNoon','Town07',test=False)\n",
    "#flipped data\n",
    "town03_data_hf = CustomImageDataset('ClearNoon','Town03',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town03_test_data_hf = CustomImageDataset('ClearNoon','Town03',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town04_data_hf = CustomImageDataset('ClearNoon','Town04',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town04_test_data_hf = CustomImageDataset('ClearNoon','Town04',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town07_data_hf = CustomImageDataset('ClearNoon','Town07',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "\n",
    "train_data = ConcatDataset([town03_data,town04_data,town07_data,town03_data_hf,town04_data_hf,town07_data_hf])\n",
    "test_data = ConcatDataset([town03_test_data,town04_test_data,town03_test_data_hf,town04_test_data_hf])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,recode_dict):\n",
    "    global step\n",
    "    servant.train()\n",
    "    writer = SummaryWriter()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb = data[0]\n",
    "        target = data[1]\n",
    "        target = recode_tags(target,recode_dict)\n",
    "        batch_size = target.shape[0]\n",
    "        #preds = F.one_hot(preds.to(torch.int64))\n",
    "        target = target.reshape(batch_size,128,128)\n",
    "        target = target.to(device,dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        #dont need latent space output while training\n",
    "        y_batch,_ = model(rgb)\n",
    "        loss = loss_fn(y_batch,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_size * batch_idx, len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))\n",
    "\n",
    "        writer.add_scalar(\"AE Loss\", loss.item() / len(data[0]),step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "          epoch, avg_loss))\n",
    "    writer.flush()\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def test(epoch,recode_dict):\n",
    "    model.eval()\n",
    "    writer = SummaryWriter()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            rgb = data[0]\n",
    "            target = data[1]\n",
    "            batch_size = target.shape[0]\n",
    "            target = recode_tags(target,recode_dict)\n",
    "            #preds = F.one_hot(preds.to(torch.int64))\n",
    "            target = target.permute(0,3,1,2)\n",
    "            target= target.reshape(batch_size,128,128)\n",
    "            target = target.to(device,dtype=torch.long)\n",
    "            y_batch,_ = model(rgb)\n",
    "            test_loss += loss_fn(y_batch,target).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.8f}'.format(test_loss))\n",
    "    writer.add_scalar(\"AE test Loss\", test_loss,epoch)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "for epoch in range(1, 20 + 1):\n",
    "    step = 0\n",
    "    smallest_loss = 1000\n",
    "    \n",
    "    avg_loss = train(epoch,recode_dict)\n",
    "    if avg_loss < smallest_loss:\n",
    "        torch.save(model.state_dict(), './AE_params/model_44.best')\n",
    "    test(epoch,recode_dict)\n",
    "    torch.save(model.state_dict(), './AE_params/model_44.final')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
