{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "class GANImageDataset(Dataset):\n",
    "    def __init__(self, weather, town, model, transform=None, target_transform=None):\n",
    "        self.dir = './Datasets/'+weather+'/'+town+'/'+model+'/test_latest/images'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.real = glob.glob(self.dir+'/*real.png')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.real))\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = self.real[idx]\n",
    "        image = read_image(img_path)\n",
    "        label_name_split = self.real[idx].split('_')\n",
    "        label_name = label_name_split[0]+'_'+label_name_split[1]+'_fake.png'\n",
    "        label = read_image( label_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import carla\n",
    "from utils import AE_initalize_weights\n",
    "from utils import generate_semantic_im\n",
    "from utils import replace\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = PerceptionNet()\n",
    "servant = PerceptionNet()\n",
    "\n",
    "master.to(device)\n",
    "master.load_state_dict(torch.load('./AE_params/model_46.final'))\n",
    "master.eval()\n",
    "\n",
    "servant.to(device)\n",
    "servant.apply(AE_initalize_weights)\n",
    "servant.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GANImageDataset('dry-wet','Town04','wetcyclegan4')\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_data = GANImageDataset('dry-wet','Town04','wetcyclegan4')\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21026"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(servant.parameters(), lr=0.0003)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/21026 (0%)]\tLoss: 0.290350\n",
      "Train Epoch: 1 [64/21026 (0%)]\tLoss: 0.319945\n",
      "Train Epoch: 1 [128/21026 (1%)]\tLoss: 0.303181\n",
      "Train Epoch: 1 [192/21026 (1%)]\tLoss: 0.280903\n",
      "Train Epoch: 1 [256/21026 (1%)]\tLoss: 0.274981\n",
      "Train Epoch: 1 [320/21026 (2%)]\tLoss: 0.312454\n",
      "Train Epoch: 1 [384/21026 (2%)]\tLoss: 0.335438\n",
      "Train Epoch: 1 [448/21026 (2%)]\tLoss: 0.325439\n",
      "Train Epoch: 1 [512/21026 (2%)]\tLoss: 0.318673\n",
      "Train Epoch: 1 [576/21026 (3%)]\tLoss: 0.274362\n",
      "Train Epoch: 1 [640/21026 (3%)]\tLoss: 0.288796\n",
      "Train Epoch: 1 [704/21026 (3%)]\tLoss: 0.274422\n",
      "Train Epoch: 1 [768/21026 (4%)]\tLoss: 0.302207\n",
      "Train Epoch: 1 [832/21026 (4%)]\tLoss: 0.300175\n",
      "Train Epoch: 1 [896/21026 (4%)]\tLoss: 0.252194\n",
      "Train Epoch: 1 [960/21026 (5%)]\tLoss: 0.318100\n",
      "Train Epoch: 1 [1024/21026 (5%)]\tLoss: 0.314378\n",
      "Train Epoch: 1 [1088/21026 (5%)]\tLoss: 0.281259\n",
      "Train Epoch: 1 [1152/21026 (5%)]\tLoss: 0.298572\n",
      "Train Epoch: 1 [1216/21026 (6%)]\tLoss: 0.294165\n",
      "Train Epoch: 1 [1280/21026 (6%)]\tLoss: 0.264663\n",
      "Train Epoch: 1 [1344/21026 (6%)]\tLoss: 0.346631\n",
      "Train Epoch: 1 [1408/21026 (7%)]\tLoss: 0.303410\n",
      "Train Epoch: 1 [1472/21026 (7%)]\tLoss: 0.319282\n",
      "Train Epoch: 1 [1536/21026 (7%)]\tLoss: 0.337331\n",
      "Train Epoch: 1 [1600/21026 (8%)]\tLoss: 0.289392\n",
      "Train Epoch: 1 [1664/21026 (8%)]\tLoss: 0.320558\n",
      "Train Epoch: 1 [1728/21026 (8%)]\tLoss: 0.320498\n",
      "Train Epoch: 1 [1792/21026 (9%)]\tLoss: 0.291084\n",
      "Train Epoch: 1 [1856/21026 (9%)]\tLoss: 0.304613\n",
      "Train Epoch: 1 [1920/21026 (9%)]\tLoss: 0.301773\n",
      "Train Epoch: 1 [1984/21026 (9%)]\tLoss: 0.259236\n",
      "Train Epoch: 1 [2048/21026 (10%)]\tLoss: 0.264896\n",
      "Train Epoch: 1 [2112/21026 (10%)]\tLoss: 0.366950\n",
      "Train Epoch: 1 [2176/21026 (10%)]\tLoss: 0.326161\n",
      "Train Epoch: 1 [2240/21026 (11%)]\tLoss: 0.295755\n",
      "Train Epoch: 1 [2304/21026 (11%)]\tLoss: 0.313076\n",
      "Train Epoch: 1 [2368/21026 (11%)]\tLoss: 0.277107\n",
      "Train Epoch: 1 [2432/21026 (12%)]\tLoss: 0.271972\n",
      "Train Epoch: 1 [2496/21026 (12%)]\tLoss: 0.277241\n",
      "Train Epoch: 1 [2560/21026 (12%)]\tLoss: 0.254095\n",
      "Train Epoch: 1 [2624/21026 (12%)]\tLoss: 0.295963\n",
      "Train Epoch: 1 [2688/21026 (13%)]\tLoss: 0.345183\n",
      "Train Epoch: 1 [2752/21026 (13%)]\tLoss: 0.297393\n",
      "Train Epoch: 1 [2816/21026 (13%)]\tLoss: 0.289737\n",
      "Train Epoch: 1 [2880/21026 (14%)]\tLoss: 0.281718\n",
      "Train Epoch: 1 [2944/21026 (14%)]\tLoss: 0.331436\n",
      "Train Epoch: 1 [3008/21026 (14%)]\tLoss: 0.288022\n",
      "Train Epoch: 1 [3072/21026 (15%)]\tLoss: 0.317109\n",
      "Train Epoch: 1 [3136/21026 (15%)]\tLoss: 0.302613\n",
      "Train Epoch: 1 [3200/21026 (15%)]\tLoss: 0.306045\n",
      "Train Epoch: 1 [3264/21026 (16%)]\tLoss: 0.285047\n",
      "Train Epoch: 1 [3328/21026 (16%)]\tLoss: 0.287635\n",
      "Train Epoch: 1 [3392/21026 (16%)]\tLoss: 0.327823\n",
      "Train Epoch: 1 [3456/21026 (16%)]\tLoss: 0.285317\n",
      "Train Epoch: 1 [3520/21026 (17%)]\tLoss: 0.245856\n",
      "Train Epoch: 1 [3584/21026 (17%)]\tLoss: 0.281445\n",
      "Train Epoch: 1 [3648/21026 (17%)]\tLoss: 0.302671\n",
      "Train Epoch: 1 [3712/21026 (18%)]\tLoss: 0.333366\n",
      "Train Epoch: 1 [3776/21026 (18%)]\tLoss: 0.276137\n",
      "Train Epoch: 1 [3840/21026 (18%)]\tLoss: 0.276419\n",
      "Train Epoch: 1 [3904/21026 (19%)]\tLoss: 0.282139\n",
      "Train Epoch: 1 [3968/21026 (19%)]\tLoss: 0.271653\n",
      "Train Epoch: 1 [4032/21026 (19%)]\tLoss: 0.297782\n",
      "Train Epoch: 1 [4096/21026 (19%)]\tLoss: 0.296916\n",
      "Train Epoch: 1 [4160/21026 (20%)]\tLoss: 0.252768\n",
      "Train Epoch: 1 [4224/21026 (20%)]\tLoss: 0.303202\n",
      "Train Epoch: 1 [4288/21026 (20%)]\tLoss: 0.283398\n",
      "Train Epoch: 1 [4352/21026 (21%)]\tLoss: 0.317870\n",
      "Train Epoch: 1 [4416/21026 (21%)]\tLoss: 0.313115\n",
      "Train Epoch: 1 [4480/21026 (21%)]\tLoss: 0.266844\n",
      "Train Epoch: 1 [4544/21026 (22%)]\tLoss: 0.287637\n",
      "Train Epoch: 1 [4608/21026 (22%)]\tLoss: 0.305080\n",
      "Train Epoch: 1 [4672/21026 (22%)]\tLoss: 0.315807\n",
      "Train Epoch: 1 [4736/21026 (22%)]\tLoss: 0.294945\n",
      "Train Epoch: 1 [4800/21026 (23%)]\tLoss: 0.328872\n",
      "Train Epoch: 1 [4864/21026 (23%)]\tLoss: 0.257475\n",
      "Train Epoch: 1 [4928/21026 (23%)]\tLoss: 0.258197\n",
      "Train Epoch: 1 [4992/21026 (24%)]\tLoss: 0.282607\n",
      "Train Epoch: 1 [5056/21026 (24%)]\tLoss: 0.309948\n",
      "Train Epoch: 1 [5120/21026 (24%)]\tLoss: 0.275532\n",
      "Train Epoch: 1 [5184/21026 (25%)]\tLoss: 0.258275\n",
      "Train Epoch: 1 [5248/21026 (25%)]\tLoss: 0.307068\n",
      "Train Epoch: 1 [5312/21026 (25%)]\tLoss: 0.316353\n",
      "Train Epoch: 1 [5376/21026 (26%)]\tLoss: 0.298280\n",
      "Train Epoch: 1 [5440/21026 (26%)]\tLoss: 0.317381\n",
      "Train Epoch: 1 [5504/21026 (26%)]\tLoss: 0.264802\n",
      "Train Epoch: 1 [5568/21026 (26%)]\tLoss: 0.320254\n",
      "Train Epoch: 1 [5632/21026 (27%)]\tLoss: 0.259069\n",
      "Train Epoch: 1 [5696/21026 (27%)]\tLoss: 0.278543\n",
      "Train Epoch: 1 [5760/21026 (27%)]\tLoss: 0.251100\n",
      "Train Epoch: 1 [5824/21026 (28%)]\tLoss: 0.284163\n",
      "Train Epoch: 1 [5888/21026 (28%)]\tLoss: 0.249495\n",
      "Train Epoch: 1 [5952/21026 (28%)]\tLoss: 0.314936\n",
      "Train Epoch: 1 [6016/21026 (29%)]\tLoss: 0.311578\n",
      "Train Epoch: 1 [6080/21026 (29%)]\tLoss: 0.305285\n",
      "Train Epoch: 1 [6144/21026 (29%)]\tLoss: 0.311553\n",
      "Train Epoch: 1 [6208/21026 (29%)]\tLoss: 0.263287\n",
      "Train Epoch: 1 [6272/21026 (30%)]\tLoss: 0.255757\n",
      "Train Epoch: 1 [6336/21026 (30%)]\tLoss: 0.289093\n",
      "Train Epoch: 1 [6400/21026 (30%)]\tLoss: 0.276794\n",
      "Train Epoch: 1 [6464/21026 (31%)]\tLoss: 0.276047\n",
      "Train Epoch: 1 [6528/21026 (31%)]\tLoss: 0.315933\n",
      "Train Epoch: 1 [6592/21026 (31%)]\tLoss: 0.264983\n",
      "Train Epoch: 1 [6656/21026 (32%)]\tLoss: 0.238439\n",
      "Train Epoch: 1 [6720/21026 (32%)]\tLoss: 0.277399\n",
      "Train Epoch: 1 [6784/21026 (32%)]\tLoss: 0.305585\n",
      "Train Epoch: 1 [6848/21026 (33%)]\tLoss: 0.270531\n",
      "Train Epoch: 1 [6912/21026 (33%)]\tLoss: 0.286511\n",
      "Train Epoch: 1 [6976/21026 (33%)]\tLoss: 0.303253\n",
      "Train Epoch: 1 [7040/21026 (33%)]\tLoss: 0.280925\n",
      "Train Epoch: 1 [7104/21026 (34%)]\tLoss: 0.310607\n",
      "Train Epoch: 1 [7168/21026 (34%)]\tLoss: 0.277268\n",
      "Train Epoch: 1 [7232/21026 (34%)]\tLoss: 0.267283\n",
      "Train Epoch: 1 [7296/21026 (35%)]\tLoss: 0.267071\n",
      "Train Epoch: 1 [7360/21026 (35%)]\tLoss: 0.266674\n",
      "Train Epoch: 1 [7424/21026 (35%)]\tLoss: 0.277624\n",
      "Train Epoch: 1 [7488/21026 (36%)]\tLoss: 0.361955\n",
      "Train Epoch: 1 [7552/21026 (36%)]\tLoss: 0.278269\n",
      "Train Epoch: 1 [7616/21026 (36%)]\tLoss: 0.257812\n",
      "Train Epoch: 1 [7680/21026 (36%)]\tLoss: 0.288479\n",
      "Train Epoch: 1 [7744/21026 (37%)]\tLoss: 0.273320\n",
      "Train Epoch: 1 [7808/21026 (37%)]\tLoss: 0.285630\n",
      "Train Epoch: 1 [7872/21026 (37%)]\tLoss: 0.263490\n",
      "Train Epoch: 1 [7936/21026 (38%)]\tLoss: 0.262378\n",
      "Train Epoch: 1 [8000/21026 (38%)]\tLoss: 0.260932\n",
      "Train Epoch: 1 [8064/21026 (38%)]\tLoss: 0.294741\n",
      "Train Epoch: 1 [8128/21026 (39%)]\tLoss: 0.296491\n",
      "Train Epoch: 1 [8192/21026 (39%)]\tLoss: 0.264786\n",
      "Train Epoch: 1 [8256/21026 (39%)]\tLoss: 0.274619\n",
      "Train Epoch: 1 [8320/21026 (40%)]\tLoss: 0.281849\n",
      "Train Epoch: 1 [8384/21026 (40%)]\tLoss: 0.296716\n",
      "Train Epoch: 1 [8448/21026 (40%)]\tLoss: 0.268572\n",
      "Train Epoch: 1 [8512/21026 (40%)]\tLoss: 0.274957\n",
      "Train Epoch: 1 [8576/21026 (41%)]\tLoss: 0.309132\n",
      "Train Epoch: 1 [8640/21026 (41%)]\tLoss: 0.232365\n",
      "Train Epoch: 1 [8704/21026 (41%)]\tLoss: 0.270459\n",
      "Train Epoch: 1 [8768/21026 (42%)]\tLoss: 0.267241\n",
      "Train Epoch: 1 [8832/21026 (42%)]\tLoss: 0.243389\n",
      "Train Epoch: 1 [8896/21026 (42%)]\tLoss: 0.269952\n",
      "Train Epoch: 1 [8960/21026 (43%)]\tLoss: 0.261383\n",
      "Train Epoch: 1 [9024/21026 (43%)]\tLoss: 0.253365\n",
      "Train Epoch: 1 [9088/21026 (43%)]\tLoss: 0.282136\n",
      "Train Epoch: 1 [9152/21026 (43%)]\tLoss: 0.306560\n",
      "Train Epoch: 1 [9216/21026 (44%)]\tLoss: 0.274963\n",
      "Train Epoch: 1 [9280/21026 (44%)]\tLoss: 0.240882\n",
      "Train Epoch: 1 [9344/21026 (44%)]\tLoss: 0.255419\n",
      "Train Epoch: 1 [9408/21026 (45%)]\tLoss: 0.311057\n",
      "Train Epoch: 1 [9472/21026 (45%)]\tLoss: 0.289540\n",
      "Train Epoch: 1 [9536/21026 (45%)]\tLoss: 0.261903\n",
      "Train Epoch: 1 [9600/21026 (46%)]\tLoss: 0.262739\n",
      "Train Epoch: 1 [9664/21026 (46%)]\tLoss: 0.326921\n",
      "Train Epoch: 1 [9728/21026 (46%)]\tLoss: 0.234140\n",
      "Train Epoch: 1 [9792/21026 (47%)]\tLoss: 0.250347\n",
      "Train Epoch: 1 [9856/21026 (47%)]\tLoss: 0.252259\n",
      "Train Epoch: 1 [9920/21026 (47%)]\tLoss: 0.259997\n",
      "Train Epoch: 1 [9984/21026 (47%)]\tLoss: 0.219658\n",
      "Train Epoch: 1 [10048/21026 (48%)]\tLoss: 0.256888\n",
      "Train Epoch: 1 [10112/21026 (48%)]\tLoss: 0.259659\n",
      "Train Epoch: 1 [10176/21026 (48%)]\tLoss: 0.258512\n",
      "Train Epoch: 1 [10240/21026 (49%)]\tLoss: 0.268242\n",
      "Train Epoch: 1 [10304/21026 (49%)]\tLoss: 0.289839\n",
      "Train Epoch: 1 [10368/21026 (49%)]\tLoss: 0.310709\n",
      "Train Epoch: 1 [10432/21026 (50%)]\tLoss: 0.310384\n",
      "Train Epoch: 1 [10496/21026 (50%)]\tLoss: 0.274047\n",
      "Train Epoch: 1 [10560/21026 (50%)]\tLoss: 0.264921\n",
      "Train Epoch: 1 [10624/21026 (50%)]\tLoss: 0.264153\n",
      "Train Epoch: 1 [10688/21026 (51%)]\tLoss: 0.250276\n",
      "Train Epoch: 1 [10752/21026 (51%)]\tLoss: 0.290140\n",
      "Train Epoch: 1 [10816/21026 (51%)]\tLoss: 0.276794\n",
      "Train Epoch: 1 [10880/21026 (52%)]\tLoss: 0.283176\n",
      "Train Epoch: 1 [10944/21026 (52%)]\tLoss: 0.258999\n",
      "Train Epoch: 1 [11008/21026 (52%)]\tLoss: 0.231471\n",
      "Train Epoch: 1 [11072/21026 (53%)]\tLoss: 0.294690\n",
      "Train Epoch: 1 [11136/21026 (53%)]\tLoss: 0.244579\n",
      "Train Epoch: 1 [11200/21026 (53%)]\tLoss: 0.252841\n",
      "Train Epoch: 1 [11264/21026 (53%)]\tLoss: 0.257137\n",
      "Train Epoch: 1 [11328/21026 (54%)]\tLoss: 0.276697\n",
      "Train Epoch: 1 [11392/21026 (54%)]\tLoss: 0.242201\n",
      "Train Epoch: 1 [11456/21026 (54%)]\tLoss: 0.280868\n",
      "Train Epoch: 1 [11520/21026 (55%)]\tLoss: 0.246142\n",
      "Train Epoch: 1 [11584/21026 (55%)]\tLoss: 0.264888\n",
      "Train Epoch: 1 [11648/21026 (55%)]\tLoss: 0.271562\n",
      "Train Epoch: 1 [11712/21026 (56%)]\tLoss: 0.277677\n",
      "Train Epoch: 1 [11776/21026 (56%)]\tLoss: 0.308489\n",
      "Train Epoch: 1 [11840/21026 (56%)]\tLoss: 0.276336\n",
      "Train Epoch: 1 [11904/21026 (57%)]\tLoss: 0.267168\n",
      "Train Epoch: 1 [11968/21026 (57%)]\tLoss: 0.255333\n",
      "Train Epoch: 1 [12032/21026 (57%)]\tLoss: 0.295461\n",
      "Train Epoch: 1 [12096/21026 (57%)]\tLoss: 0.237168\n",
      "Train Epoch: 1 [12160/21026 (58%)]\tLoss: 0.257284\n",
      "Train Epoch: 1 [12224/21026 (58%)]\tLoss: 0.336244\n",
      "Train Epoch: 1 [12288/21026 (58%)]\tLoss: 0.229871\n",
      "Train Epoch: 1 [12352/21026 (59%)]\tLoss: 0.234625\n",
      "Train Epoch: 1 [12416/21026 (59%)]\tLoss: 0.233501\n",
      "Train Epoch: 1 [12480/21026 (59%)]\tLoss: 0.255396\n",
      "Train Epoch: 1 [12544/21026 (60%)]\tLoss: 0.293385\n",
      "Train Epoch: 1 [12608/21026 (60%)]\tLoss: 0.264800\n",
      "Train Epoch: 1 [12672/21026 (60%)]\tLoss: 0.234613\n",
      "Train Epoch: 1 [12736/21026 (60%)]\tLoss: 0.289922\n",
      "Train Epoch: 1 [12800/21026 (61%)]\tLoss: 0.275408\n",
      "Train Epoch: 1 [12864/21026 (61%)]\tLoss: 0.244473\n",
      "Train Epoch: 1 [12928/21026 (61%)]\tLoss: 0.247930\n",
      "Train Epoch: 1 [12992/21026 (62%)]\tLoss: 0.282393\n",
      "Train Epoch: 1 [13056/21026 (62%)]\tLoss: 0.261438\n",
      "Train Epoch: 1 [13120/21026 (62%)]\tLoss: 0.251105\n",
      "Train Epoch: 1 [13184/21026 (63%)]\tLoss: 0.235193\n",
      "Train Epoch: 1 [13248/21026 (63%)]\tLoss: 0.231494\n",
      "Train Epoch: 1 [13312/21026 (63%)]\tLoss: 0.243604\n",
      "Train Epoch: 1 [13376/21026 (64%)]\tLoss: 0.261920\n",
      "Train Epoch: 1 [13440/21026 (64%)]\tLoss: 0.226563\n",
      "Train Epoch: 1 [13504/21026 (64%)]\tLoss: 0.260683\n",
      "Train Epoch: 1 [13568/21026 (64%)]\tLoss: 0.210979\n",
      "Train Epoch: 1 [13632/21026 (65%)]\tLoss: 0.278461\n",
      "Train Epoch: 1 [13696/21026 (65%)]\tLoss: 0.287272\n",
      "Train Epoch: 1 [13760/21026 (65%)]\tLoss: 0.231919\n",
      "Train Epoch: 1 [13824/21026 (66%)]\tLoss: 0.273892\n",
      "Train Epoch: 1 [13888/21026 (66%)]\tLoss: 0.291326\n",
      "Train Epoch: 1 [13952/21026 (66%)]\tLoss: 0.238554\n",
      "Train Epoch: 1 [14016/21026 (67%)]\tLoss: 0.296529\n",
      "Train Epoch: 1 [14080/21026 (67%)]\tLoss: 0.241053\n",
      "Train Epoch: 1 [14144/21026 (67%)]\tLoss: 0.247708\n",
      "Train Epoch: 1 [14208/21026 (67%)]\tLoss: 0.254781\n",
      "Train Epoch: 1 [14272/21026 (68%)]\tLoss: 0.206272\n",
      "Train Epoch: 1 [14336/21026 (68%)]\tLoss: 0.262229\n",
      "Train Epoch: 1 [14400/21026 (68%)]\tLoss: 0.259548\n",
      "Train Epoch: 1 [14464/21026 (69%)]\tLoss: 0.233292\n",
      "Train Epoch: 1 [14528/21026 (69%)]\tLoss: 0.268270\n",
      "Train Epoch: 1 [14592/21026 (69%)]\tLoss: 0.218255\n",
      "Train Epoch: 1 [14656/21026 (70%)]\tLoss: 0.233426\n",
      "Train Epoch: 1 [14720/21026 (70%)]\tLoss: 0.264435\n",
      "Train Epoch: 1 [14784/21026 (70%)]\tLoss: 0.267587\n",
      "Train Epoch: 1 [14848/21026 (71%)]\tLoss: 0.233331\n",
      "Train Epoch: 1 [14912/21026 (71%)]\tLoss: 0.254041\n",
      "Train Epoch: 1 [14976/21026 (71%)]\tLoss: 0.241562\n",
      "Train Epoch: 1 [15040/21026 (71%)]\tLoss: 0.211548\n",
      "Train Epoch: 1 [15104/21026 (72%)]\tLoss: 0.242648\n",
      "Train Epoch: 1 [15168/21026 (72%)]\tLoss: 0.246437\n",
      "Train Epoch: 1 [15232/21026 (72%)]\tLoss: 0.268873\n",
      "Train Epoch: 1 [15296/21026 (73%)]\tLoss: 0.261173\n",
      "Train Epoch: 1 [15360/21026 (73%)]\tLoss: 0.239757\n",
      "Train Epoch: 1 [15424/21026 (73%)]\tLoss: 0.225387\n",
      "Train Epoch: 1 [15488/21026 (74%)]\tLoss: 0.290832\n",
      "Train Epoch: 1 [15552/21026 (74%)]\tLoss: 0.259262\n",
      "Train Epoch: 1 [15616/21026 (74%)]\tLoss: 0.235707\n",
      "Train Epoch: 1 [15680/21026 (74%)]\tLoss: 0.219128\n",
      "Train Epoch: 1 [15744/21026 (75%)]\tLoss: 0.256642\n",
      "Train Epoch: 1 [15808/21026 (75%)]\tLoss: 0.272553\n",
      "Train Epoch: 1 [15872/21026 (75%)]\tLoss: 0.224746\n",
      "Train Epoch: 1 [15936/21026 (76%)]\tLoss: 0.229414\n",
      "Train Epoch: 1 [16000/21026 (76%)]\tLoss: 0.284042\n",
      "Train Epoch: 1 [16064/21026 (76%)]\tLoss: 0.246089\n",
      "Train Epoch: 1 [16128/21026 (77%)]\tLoss: 0.236543\n",
      "Train Epoch: 1 [16192/21026 (77%)]\tLoss: 0.247023\n",
      "Train Epoch: 1 [16256/21026 (77%)]\tLoss: 0.258653\n",
      "Train Epoch: 1 [16320/21026 (78%)]\tLoss: 0.268052\n",
      "Train Epoch: 1 [16384/21026 (78%)]\tLoss: 0.266459\n",
      "Train Epoch: 1 [16448/21026 (78%)]\tLoss: 0.247440\n",
      "Train Epoch: 1 [16512/21026 (78%)]\tLoss: 0.210936\n",
      "Train Epoch: 1 [16576/21026 (79%)]\tLoss: 0.254928\n",
      "Train Epoch: 1 [16640/21026 (79%)]\tLoss: 0.255465\n",
      "Train Epoch: 1 [16704/21026 (79%)]\tLoss: 0.254592\n",
      "Train Epoch: 1 [16768/21026 (80%)]\tLoss: 0.230585\n",
      "Train Epoch: 1 [16832/21026 (80%)]\tLoss: 0.250658\n",
      "Train Epoch: 1 [16896/21026 (80%)]\tLoss: 0.273684\n",
      "Train Epoch: 1 [16960/21026 (81%)]\tLoss: 0.249055\n",
      "Train Epoch: 1 [17024/21026 (81%)]\tLoss: 0.238725\n",
      "Train Epoch: 1 [17088/21026 (81%)]\tLoss: 0.237620\n",
      "Train Epoch: 1 [17152/21026 (81%)]\tLoss: 0.244037\n",
      "Train Epoch: 1 [17216/21026 (82%)]\tLoss: 0.221950\n",
      "Train Epoch: 1 [17280/21026 (82%)]\tLoss: 0.238447\n",
      "Train Epoch: 1 [17344/21026 (82%)]\tLoss: 0.254198\n",
      "Train Epoch: 1 [17408/21026 (83%)]\tLoss: 0.232114\n",
      "Train Epoch: 1 [17472/21026 (83%)]\tLoss: 0.255930\n",
      "Train Epoch: 1 [17536/21026 (83%)]\tLoss: 0.252912\n",
      "Train Epoch: 1 [17600/21026 (84%)]\tLoss: 0.273558\n",
      "Train Epoch: 1 [17664/21026 (84%)]\tLoss: 0.243159\n",
      "Train Epoch: 1 [17728/21026 (84%)]\tLoss: 0.319361\n",
      "Train Epoch: 1 [17792/21026 (84%)]\tLoss: 0.269189\n",
      "Train Epoch: 1 [17856/21026 (85%)]\tLoss: 0.239214\n",
      "Train Epoch: 1 [17920/21026 (85%)]\tLoss: 0.243963\n",
      "Train Epoch: 1 [17984/21026 (85%)]\tLoss: 0.248251\n",
      "Train Epoch: 1 [18048/21026 (86%)]\tLoss: 0.238345\n",
      "Train Epoch: 1 [18112/21026 (86%)]\tLoss: 0.223621\n",
      "Train Epoch: 1 [18176/21026 (86%)]\tLoss: 0.223981\n",
      "Train Epoch: 1 [18240/21026 (87%)]\tLoss: 0.203341\n",
      "Train Epoch: 1 [18304/21026 (87%)]\tLoss: 0.213585\n",
      "Train Epoch: 1 [18368/21026 (87%)]\tLoss: 0.220615\n",
      "Train Epoch: 1 [18432/21026 (88%)]\tLoss: 0.224715\n",
      "Train Epoch: 1 [18496/21026 (88%)]\tLoss: 0.247494\n",
      "Train Epoch: 1 [18560/21026 (88%)]\tLoss: 0.229064\n",
      "Train Epoch: 1 [18624/21026 (88%)]\tLoss: 0.228943\n",
      "Train Epoch: 1 [18688/21026 (89%)]\tLoss: 0.218972\n",
      "Train Epoch: 1 [18752/21026 (89%)]\tLoss: 0.273988\n",
      "Train Epoch: 1 [18816/21026 (89%)]\tLoss: 0.221308\n",
      "Train Epoch: 1 [18880/21026 (90%)]\tLoss: 0.266733\n",
      "Train Epoch: 1 [18944/21026 (90%)]\tLoss: 0.222997\n",
      "Train Epoch: 1 [19008/21026 (90%)]\tLoss: 0.237671\n",
      "Train Epoch: 1 [19072/21026 (91%)]\tLoss: 0.199127\n",
      "Train Epoch: 1 [19136/21026 (91%)]\tLoss: 0.256193\n",
      "Train Epoch: 1 [19200/21026 (91%)]\tLoss: 0.226024\n",
      "Train Epoch: 1 [19264/21026 (91%)]\tLoss: 0.237800\n",
      "Train Epoch: 1 [19328/21026 (92%)]\tLoss: 0.245020\n",
      "Train Epoch: 1 [19392/21026 (92%)]\tLoss: 0.215488\n",
      "Train Epoch: 1 [19456/21026 (92%)]\tLoss: 0.198382\n",
      "Train Epoch: 1 [19520/21026 (93%)]\tLoss: 0.210608\n",
      "Train Epoch: 1 [19584/21026 (93%)]\tLoss: 0.247667\n",
      "Train Epoch: 1 [19648/21026 (93%)]\tLoss: 0.255853\n",
      "Train Epoch: 1 [19712/21026 (94%)]\tLoss: 0.227945\n",
      "Train Epoch: 1 [19776/21026 (94%)]\tLoss: 0.232726\n",
      "Train Epoch: 1 [19840/21026 (94%)]\tLoss: 0.242921\n",
      "Train Epoch: 1 [19904/21026 (95%)]\tLoss: 0.209292\n",
      "Train Epoch: 1 [19968/21026 (95%)]\tLoss: 0.263211\n",
      "Train Epoch: 1 [20032/21026 (95%)]\tLoss: 0.254149\n",
      "Train Epoch: 1 [20096/21026 (95%)]\tLoss: 0.252219\n",
      "Train Epoch: 1 [20160/21026 (96%)]\tLoss: 0.247282\n",
      "Train Epoch: 1 [20224/21026 (96%)]\tLoss: 0.233778\n",
      "Train Epoch: 1 [20288/21026 (96%)]\tLoss: 0.237941\n",
      "Train Epoch: 1 [20352/21026 (97%)]\tLoss: 0.205915\n",
      "Train Epoch: 1 [20416/21026 (97%)]\tLoss: 0.258930\n",
      "Train Epoch: 1 [20480/21026 (97%)]\tLoss: 0.228356\n",
      "Train Epoch: 1 [20544/21026 (98%)]\tLoss: 0.245635\n",
      "Train Epoch: 1 [20608/21026 (98%)]\tLoss: 0.275235\n",
      "Train Epoch: 1 [20672/21026 (98%)]\tLoss: 0.253040\n",
      "Train Epoch: 1 [20736/21026 (98%)]\tLoss: 0.217492\n",
      "Train Epoch: 1 [20800/21026 (99%)]\tLoss: 0.198132\n",
      "Train Epoch: 1 [20864/21026 (99%)]\tLoss: 0.236284\n",
      "Train Epoch: 1 [20928/21026 (99%)]\tLoss: 0.189433\n",
      "Train Epoch: 1 [11152/21026 (100%)]\tLoss: 0.349057\n",
      "====> Epoch: 1 Average loss: 0.00839778\n",
      "Train Epoch: 2 [0/21026 (0%)]\tLoss: 0.189789\n",
      "Train Epoch: 2 [64/21026 (0%)]\tLoss: 0.199185\n",
      "Train Epoch: 2 [128/21026 (1%)]\tLoss: 0.197478\n",
      "Train Epoch: 2 [192/21026 (1%)]\tLoss: 0.210695\n",
      "Train Epoch: 2 [256/21026 (1%)]\tLoss: 0.233442\n",
      "Train Epoch: 2 [320/21026 (2%)]\tLoss: 0.196013\n",
      "Train Epoch: 2 [384/21026 (2%)]\tLoss: 0.179181\n",
      "Train Epoch: 2 [448/21026 (2%)]\tLoss: 0.191744\n",
      "Train Epoch: 2 [512/21026 (2%)]\tLoss: 0.182009\n",
      "Train Epoch: 2 [576/21026 (3%)]\tLoss: 0.209411\n",
      "Train Epoch: 2 [640/21026 (3%)]\tLoss: 0.157176\n",
      "Train Epoch: 2 [704/21026 (3%)]\tLoss: 0.181553\n",
      "Train Epoch: 2 [768/21026 (4%)]\tLoss: 0.167732\n",
      "Train Epoch: 2 [832/21026 (4%)]\tLoss: 0.207926\n",
      "Train Epoch: 2 [896/21026 (4%)]\tLoss: 0.172641\n",
      "Train Epoch: 2 [960/21026 (5%)]\tLoss: 0.195183\n",
      "Train Epoch: 2 [1024/21026 (5%)]\tLoss: 0.180528\n",
      "Train Epoch: 2 [1088/21026 (5%)]\tLoss: 0.198013\n",
      "Train Epoch: 2 [1152/21026 (5%)]\tLoss: 0.192361\n",
      "Train Epoch: 2 [1216/21026 (6%)]\tLoss: 0.208466\n",
      "Train Epoch: 2 [1280/21026 (6%)]\tLoss: 0.198255\n",
      "Train Epoch: 2 [1344/21026 (6%)]\tLoss: 0.230350\n",
      "Train Epoch: 2 [1408/21026 (7%)]\tLoss: 0.164742\n",
      "Train Epoch: 2 [1472/21026 (7%)]\tLoss: 0.210438\n",
      "Train Epoch: 2 [1536/21026 (7%)]\tLoss: 0.199390\n",
      "Train Epoch: 2 [1600/21026 (8%)]\tLoss: 0.174288\n",
      "Train Epoch: 2 [1664/21026 (8%)]\tLoss: 0.211312\n",
      "Train Epoch: 2 [1728/21026 (8%)]\tLoss: 0.186651\n",
      "Train Epoch: 2 [1792/21026 (9%)]\tLoss: 0.160611\n",
      "Train Epoch: 2 [1856/21026 (9%)]\tLoss: 0.180809\n",
      "Train Epoch: 2 [1920/21026 (9%)]\tLoss: 0.190901\n",
      "Train Epoch: 2 [1984/21026 (9%)]\tLoss: 0.164516\n",
      "Train Epoch: 2 [2048/21026 (10%)]\tLoss: 0.183370\n",
      "Train Epoch: 2 [2112/21026 (10%)]\tLoss: 0.210885\n",
      "Train Epoch: 2 [2176/21026 (10%)]\tLoss: 0.231534\n",
      "Train Epoch: 2 [2240/21026 (11%)]\tLoss: 0.201678\n",
      "Train Epoch: 2 [2304/21026 (11%)]\tLoss: 0.182239\n",
      "Train Epoch: 2 [2368/21026 (11%)]\tLoss: 0.166857\n",
      "Train Epoch: 2 [2432/21026 (12%)]\tLoss: 0.205364\n",
      "Train Epoch: 2 [2496/21026 (12%)]\tLoss: 0.202451\n",
      "Train Epoch: 2 [2560/21026 (12%)]\tLoss: 0.202252\n",
      "Train Epoch: 2 [2624/21026 (12%)]\tLoss: 0.189455\n",
      "Train Epoch: 2 [2688/21026 (13%)]\tLoss: 0.176016\n",
      "Train Epoch: 2 [2752/21026 (13%)]\tLoss: 0.197921\n",
      "Train Epoch: 2 [2816/21026 (13%)]\tLoss: 0.210137\n",
      "Train Epoch: 2 [2880/21026 (14%)]\tLoss: 0.178772\n",
      "Train Epoch: 2 [2944/21026 (14%)]\tLoss: 0.178051\n",
      "Train Epoch: 2 [3008/21026 (14%)]\tLoss: 0.193568\n",
      "Train Epoch: 2 [3072/21026 (15%)]\tLoss: 0.197713\n",
      "Train Epoch: 2 [3136/21026 (15%)]\tLoss: 0.185325\n",
      "Train Epoch: 2 [3200/21026 (15%)]\tLoss: 0.176500\n",
      "Train Epoch: 2 [3264/21026 (16%)]\tLoss: 0.203371\n",
      "Train Epoch: 2 [3328/21026 (16%)]\tLoss: 0.161654\n",
      "Train Epoch: 2 [3392/21026 (16%)]\tLoss: 0.193554\n",
      "Train Epoch: 2 [3456/21026 (16%)]\tLoss: 0.199223\n",
      "Train Epoch: 2 [3520/21026 (17%)]\tLoss: 0.189493\n",
      "Train Epoch: 2 [3584/21026 (17%)]\tLoss: 0.170284\n",
      "Train Epoch: 2 [3648/21026 (17%)]\tLoss: 0.175309\n",
      "Train Epoch: 2 [3712/21026 (18%)]\tLoss: 0.174066\n",
      "Train Epoch: 2 [3776/21026 (18%)]\tLoss: 0.200309\n",
      "Train Epoch: 2 [3840/21026 (18%)]\tLoss: 0.176426\n",
      "Train Epoch: 2 [3904/21026 (19%)]\tLoss: 0.171449\n",
      "Train Epoch: 2 [3968/21026 (19%)]\tLoss: 0.175398\n",
      "Train Epoch: 2 [4032/21026 (19%)]\tLoss: 0.181834\n",
      "Train Epoch: 2 [4096/21026 (19%)]\tLoss: 0.207087\n",
      "Train Epoch: 2 [4160/21026 (20%)]\tLoss: 0.168176\n",
      "Train Epoch: 2 [4224/21026 (20%)]\tLoss: 0.180532\n",
      "Train Epoch: 2 [4288/21026 (20%)]\tLoss: 0.184825\n",
      "Train Epoch: 2 [4352/21026 (21%)]\tLoss: 0.194871\n",
      "Train Epoch: 2 [4416/21026 (21%)]\tLoss: 0.160347\n",
      "Train Epoch: 2 [4480/21026 (21%)]\tLoss: 0.175994\n",
      "Train Epoch: 2 [4544/21026 (22%)]\tLoss: 0.180752\n",
      "Train Epoch: 2 [4608/21026 (22%)]\tLoss: 0.199569\n",
      "Train Epoch: 2 [4672/21026 (22%)]\tLoss: 0.162796\n",
      "Train Epoch: 2 [4736/21026 (22%)]\tLoss: 0.182566\n",
      "Train Epoch: 2 [4800/21026 (23%)]\tLoss: 0.178090\n",
      "Train Epoch: 2 [4864/21026 (23%)]\tLoss: 0.184055\n",
      "Train Epoch: 2 [4928/21026 (23%)]\tLoss: 0.197110\n",
      "Train Epoch: 2 [4992/21026 (24%)]\tLoss: 0.202372\n",
      "Train Epoch: 2 [5056/21026 (24%)]\tLoss: 0.172225\n",
      "Train Epoch: 2 [5120/21026 (24%)]\tLoss: 0.181980\n",
      "Train Epoch: 2 [5184/21026 (25%)]\tLoss: 0.177374\n",
      "Train Epoch: 2 [5248/21026 (25%)]\tLoss: 0.159380\n",
      "Train Epoch: 2 [5312/21026 (25%)]\tLoss: 0.205430\n",
      "Train Epoch: 2 [5376/21026 (26%)]\tLoss: 0.170664\n",
      "Train Epoch: 2 [5440/21026 (26%)]\tLoss: 0.176194\n",
      "Train Epoch: 2 [5504/21026 (26%)]\tLoss: 0.195457\n",
      "Train Epoch: 2 [5568/21026 (26%)]\tLoss: 0.167908\n",
      "Train Epoch: 2 [5632/21026 (27%)]\tLoss: 0.176641\n",
      "Train Epoch: 2 [5696/21026 (27%)]\tLoss: 0.204469\n",
      "Train Epoch: 2 [5760/21026 (27%)]\tLoss: 0.162699\n",
      "Train Epoch: 2 [5824/21026 (28%)]\tLoss: 0.192195\n",
      "Train Epoch: 2 [5888/21026 (28%)]\tLoss: 0.187156\n",
      "Train Epoch: 2 [5952/21026 (28%)]\tLoss: 0.179928\n",
      "Train Epoch: 2 [6016/21026 (29%)]\tLoss: 0.179437\n",
      "Train Epoch: 2 [6080/21026 (29%)]\tLoss: 0.177661\n",
      "Train Epoch: 2 [6144/21026 (29%)]\tLoss: 0.211739\n",
      "Train Epoch: 2 [6208/21026 (29%)]\tLoss: 0.202911\n",
      "Train Epoch: 2 [6272/21026 (30%)]\tLoss: 0.199236\n",
      "Train Epoch: 2 [6336/21026 (30%)]\tLoss: 0.210640\n",
      "Train Epoch: 2 [6400/21026 (30%)]\tLoss: 0.184645\n",
      "Train Epoch: 2 [6464/21026 (31%)]\tLoss: 0.167645\n",
      "Train Epoch: 2 [6528/21026 (31%)]\tLoss: 0.176024\n",
      "Train Epoch: 2 [6592/21026 (31%)]\tLoss: 0.206532\n",
      "Train Epoch: 2 [6656/21026 (32%)]\tLoss: 0.204729\n",
      "Train Epoch: 2 [6720/21026 (32%)]\tLoss: 0.192909\n",
      "Train Epoch: 2 [6784/21026 (32%)]\tLoss: 0.168594\n",
      "Train Epoch: 2 [6848/21026 (33%)]\tLoss: 0.197968\n",
      "Train Epoch: 2 [6912/21026 (33%)]\tLoss: 0.175294\n",
      "Train Epoch: 2 [6976/21026 (33%)]\tLoss: 0.191805\n",
      "Train Epoch: 2 [7040/21026 (33%)]\tLoss: 0.224324\n",
      "Train Epoch: 2 [7104/21026 (34%)]\tLoss: 0.168971\n",
      "Train Epoch: 2 [7168/21026 (34%)]\tLoss: 0.209819\n",
      "Train Epoch: 2 [7232/21026 (34%)]\tLoss: 0.188616\n",
      "Train Epoch: 2 [7296/21026 (35%)]\tLoss: 0.170133\n",
      "Train Epoch: 2 [7360/21026 (35%)]\tLoss: 0.231662\n",
      "Train Epoch: 2 [7424/21026 (35%)]\tLoss: 0.209281\n",
      "Train Epoch: 2 [7488/21026 (36%)]\tLoss: 0.197427\n",
      "Train Epoch: 2 [7552/21026 (36%)]\tLoss: 0.175969\n",
      "Train Epoch: 2 [7616/21026 (36%)]\tLoss: 0.202915\n",
      "Train Epoch: 2 [7680/21026 (36%)]\tLoss: 0.204918\n",
      "Train Epoch: 2 [7744/21026 (37%)]\tLoss: 0.189503\n",
      "Train Epoch: 2 [7808/21026 (37%)]\tLoss: 0.210829\n",
      "Train Epoch: 2 [7872/21026 (37%)]\tLoss: 0.187922\n",
      "Train Epoch: 2 [7936/21026 (38%)]\tLoss: 0.200890\n",
      "Train Epoch: 2 [8000/21026 (38%)]\tLoss: 0.183856\n",
      "Train Epoch: 2 [8064/21026 (38%)]\tLoss: 0.177652\n",
      "Train Epoch: 2 [8128/21026 (39%)]\tLoss: 0.221772\n",
      "Train Epoch: 2 [8192/21026 (39%)]\tLoss: 0.170682\n",
      "Train Epoch: 2 [8256/21026 (39%)]\tLoss: 0.171101\n",
      "Train Epoch: 2 [8320/21026 (40%)]\tLoss: 0.175929\n",
      "Train Epoch: 2 [8384/21026 (40%)]\tLoss: 0.195561\n",
      "Train Epoch: 2 [8448/21026 (40%)]\tLoss: 0.165746\n",
      "Train Epoch: 2 [8512/21026 (40%)]\tLoss: 0.165505\n",
      "Train Epoch: 2 [8576/21026 (41%)]\tLoss: 0.188699\n",
      "Train Epoch: 2 [8640/21026 (41%)]\tLoss: 0.193242\n",
      "Train Epoch: 2 [8704/21026 (41%)]\tLoss: 0.168985\n",
      "Train Epoch: 2 [8768/21026 (42%)]\tLoss: 0.166795\n",
      "Train Epoch: 2 [8832/21026 (42%)]\tLoss: 0.182112\n",
      "Train Epoch: 2 [8896/21026 (42%)]\tLoss: 0.181257\n",
      "Train Epoch: 2 [8960/21026 (43%)]\tLoss: 0.155737\n",
      "Train Epoch: 2 [9024/21026 (43%)]\tLoss: 0.170505\n",
      "Train Epoch: 2 [9088/21026 (43%)]\tLoss: 0.168608\n",
      "Train Epoch: 2 [9152/21026 (43%)]\tLoss: 0.174072\n",
      "Train Epoch: 2 [9216/21026 (44%)]\tLoss: 0.173204\n",
      "Train Epoch: 2 [9280/21026 (44%)]\tLoss: 0.169662\n",
      "Train Epoch: 2 [9344/21026 (44%)]\tLoss: 0.174067\n",
      "Train Epoch: 2 [9408/21026 (45%)]\tLoss: 0.191800\n",
      "Train Epoch: 2 [9472/21026 (45%)]\tLoss: 0.163594\n",
      "Train Epoch: 2 [9536/21026 (45%)]\tLoss: 0.153736\n",
      "Train Epoch: 2 [9600/21026 (46%)]\tLoss: 0.179053\n",
      "Train Epoch: 2 [9664/21026 (46%)]\tLoss: 0.184909\n",
      "Train Epoch: 2 [9728/21026 (46%)]\tLoss: 0.192310\n",
      "Train Epoch: 2 [9792/21026 (47%)]\tLoss: 0.176598\n",
      "Train Epoch: 2 [9856/21026 (47%)]\tLoss: 0.183291\n",
      "Train Epoch: 2 [9920/21026 (47%)]\tLoss: 0.168525\n",
      "Train Epoch: 2 [9984/21026 (47%)]\tLoss: 0.178637\n",
      "Train Epoch: 2 [10048/21026 (48%)]\tLoss: 0.184919\n",
      "Train Epoch: 2 [10112/21026 (48%)]\tLoss: 0.172005\n",
      "Train Epoch: 2 [10176/21026 (48%)]\tLoss: 0.180158\n",
      "Train Epoch: 2 [10240/21026 (49%)]\tLoss: 0.173899\n",
      "Train Epoch: 2 [10304/21026 (49%)]\tLoss: 0.171879\n",
      "Train Epoch: 2 [10368/21026 (49%)]\tLoss: 0.181593\n",
      "Train Epoch: 2 [10432/21026 (50%)]\tLoss: 0.190975\n",
      "Train Epoch: 2 [10496/21026 (50%)]\tLoss: 0.210988\n",
      "Train Epoch: 2 [10560/21026 (50%)]\tLoss: 0.180510\n",
      "Train Epoch: 2 [10624/21026 (50%)]\tLoss: 0.184237\n",
      "Train Epoch: 2 [10688/21026 (51%)]\tLoss: 0.193115\n",
      "Train Epoch: 2 [10752/21026 (51%)]\tLoss: 0.171200\n",
      "Train Epoch: 2 [10816/21026 (51%)]\tLoss: 0.173824\n",
      "Train Epoch: 2 [10880/21026 (52%)]\tLoss: 0.202229\n",
      "Train Epoch: 2 [10944/21026 (52%)]\tLoss: 0.167984\n",
      "Train Epoch: 2 [11008/21026 (52%)]\tLoss: 0.210798\n",
      "Train Epoch: 2 [11072/21026 (53%)]\tLoss: 0.206232\n",
      "Train Epoch: 2 [11136/21026 (53%)]\tLoss: 0.198888\n",
      "Train Epoch: 2 [11200/21026 (53%)]\tLoss: 0.188633\n",
      "Train Epoch: 2 [11264/21026 (53%)]\tLoss: 0.188053\n",
      "Train Epoch: 2 [11328/21026 (54%)]\tLoss: 0.166122\n",
      "Train Epoch: 2 [11392/21026 (54%)]\tLoss: 0.179203\n",
      "Train Epoch: 2 [11456/21026 (54%)]\tLoss: 0.184090\n",
      "Train Epoch: 2 [11520/21026 (55%)]\tLoss: 0.185415\n",
      "Train Epoch: 2 [11584/21026 (55%)]\tLoss: 0.195879\n",
      "Train Epoch: 2 [11648/21026 (55%)]\tLoss: 0.178105\n",
      "Train Epoch: 2 [11712/21026 (56%)]\tLoss: 0.191533\n",
      "Train Epoch: 2 [11776/21026 (56%)]\tLoss: 0.185056\n",
      "Train Epoch: 2 [11840/21026 (56%)]\tLoss: 0.189398\n",
      "Train Epoch: 2 [11904/21026 (57%)]\tLoss: 0.176108\n",
      "Train Epoch: 2 [11968/21026 (57%)]\tLoss: 0.169209\n",
      "Train Epoch: 2 [12032/21026 (57%)]\tLoss: 0.166679\n",
      "Train Epoch: 2 [12096/21026 (57%)]\tLoss: 0.161159\n",
      "Train Epoch: 2 [12160/21026 (58%)]\tLoss: 0.200139\n",
      "Train Epoch: 2 [12224/21026 (58%)]\tLoss: 0.188115\n",
      "Train Epoch: 2 [12288/21026 (58%)]\tLoss: 0.190597\n",
      "Train Epoch: 2 [12352/21026 (59%)]\tLoss: 0.236428\n",
      "Train Epoch: 2 [12416/21026 (59%)]\tLoss: 0.192743\n",
      "Train Epoch: 2 [12480/21026 (59%)]\tLoss: 0.191034\n",
      "Train Epoch: 2 [12544/21026 (60%)]\tLoss: 0.167933\n",
      "Train Epoch: 2 [12608/21026 (60%)]\tLoss: 0.192839\n",
      "Train Epoch: 2 [12672/21026 (60%)]\tLoss: 0.167377\n",
      "Train Epoch: 2 [12736/21026 (60%)]\tLoss: 0.183487\n",
      "Train Epoch: 2 [12800/21026 (61%)]\tLoss: 0.165596\n",
      "Train Epoch: 2 [12864/21026 (61%)]\tLoss: 0.172634\n",
      "Train Epoch: 2 [12928/21026 (61%)]\tLoss: 0.203887\n",
      "Train Epoch: 2 [12992/21026 (62%)]\tLoss: 0.194027\n",
      "Train Epoch: 2 [13056/21026 (62%)]\tLoss: 0.187710\n",
      "Train Epoch: 2 [13120/21026 (62%)]\tLoss: 0.156189\n",
      "Train Epoch: 2 [13184/21026 (63%)]\tLoss: 0.221142\n",
      "Train Epoch: 2 [13248/21026 (63%)]\tLoss: 0.172278\n",
      "Train Epoch: 2 [13312/21026 (63%)]\tLoss: 0.200812\n",
      "Train Epoch: 2 [13376/21026 (64%)]\tLoss: 0.153580\n",
      "Train Epoch: 2 [13440/21026 (64%)]\tLoss: 0.184751\n",
      "Train Epoch: 2 [13504/21026 (64%)]\tLoss: 0.164687\n",
      "Train Epoch: 2 [13568/21026 (64%)]\tLoss: 0.187871\n",
      "Train Epoch: 2 [13632/21026 (65%)]\tLoss: 0.187399\n",
      "Train Epoch: 2 [13696/21026 (65%)]\tLoss: 0.198213\n",
      "Train Epoch: 2 [13760/21026 (65%)]\tLoss: 0.195326\n",
      "Train Epoch: 2 [13824/21026 (66%)]\tLoss: 0.168688\n",
      "Train Epoch: 2 [13888/21026 (66%)]\tLoss: 0.186284\n",
      "Train Epoch: 2 [13952/21026 (66%)]\tLoss: 0.164108\n",
      "Train Epoch: 2 [14016/21026 (67%)]\tLoss: 0.189511\n",
      "Train Epoch: 2 [14080/21026 (67%)]\tLoss: 0.185597\n",
      "Train Epoch: 2 [14144/21026 (67%)]\tLoss: 0.151448\n",
      "Train Epoch: 2 [14208/21026 (67%)]\tLoss: 0.174016\n",
      "Train Epoch: 2 [14272/21026 (68%)]\tLoss: 0.153549\n",
      "Train Epoch: 2 [14336/21026 (68%)]\tLoss: 0.167461\n",
      "Train Epoch: 2 [14400/21026 (68%)]\tLoss: 0.168412\n",
      "Train Epoch: 2 [14464/21026 (69%)]\tLoss: 0.161430\n",
      "Train Epoch: 2 [14528/21026 (69%)]\tLoss: 0.185284\n",
      "Train Epoch: 2 [14592/21026 (69%)]\tLoss: 0.177694\n",
      "Train Epoch: 2 [14656/21026 (70%)]\tLoss: 0.190483\n",
      "Train Epoch: 2 [14720/21026 (70%)]\tLoss: 0.174711\n",
      "Train Epoch: 2 [14784/21026 (70%)]\tLoss: 0.189395\n",
      "Train Epoch: 2 [14848/21026 (71%)]\tLoss: 0.174966\n",
      "Train Epoch: 2 [14912/21026 (71%)]\tLoss: 0.177546\n",
      "Train Epoch: 2 [14976/21026 (71%)]\tLoss: 0.190098\n",
      "Train Epoch: 2 [15040/21026 (71%)]\tLoss: 0.184215\n",
      "Train Epoch: 2 [15104/21026 (72%)]\tLoss: 0.159119\n",
      "Train Epoch: 2 [15168/21026 (72%)]\tLoss: 0.180867\n",
      "Train Epoch: 2 [15232/21026 (72%)]\tLoss: 0.223624\n",
      "Train Epoch: 2 [15296/21026 (73%)]\tLoss: 0.204576\n",
      "Train Epoch: 2 [15360/21026 (73%)]\tLoss: 0.176217\n",
      "Train Epoch: 2 [15424/21026 (73%)]\tLoss: 0.171163\n",
      "Train Epoch: 2 [15488/21026 (74%)]\tLoss: 0.196995\n",
      "Train Epoch: 2 [15552/21026 (74%)]\tLoss: 0.198729\n",
      "Train Epoch: 2 [15616/21026 (74%)]\tLoss: 0.189856\n",
      "Train Epoch: 2 [15680/21026 (74%)]\tLoss: 0.178593\n",
      "Train Epoch: 2 [15744/21026 (75%)]\tLoss: 0.195378\n",
      "Train Epoch: 2 [15808/21026 (75%)]\tLoss: 0.176260\n",
      "Train Epoch: 2 [15872/21026 (75%)]\tLoss: 0.184553\n",
      "Train Epoch: 2 [15936/21026 (76%)]\tLoss: 0.195016\n",
      "Train Epoch: 2 [16000/21026 (76%)]\tLoss: 0.167754\n",
      "Train Epoch: 2 [16064/21026 (76%)]\tLoss: 0.218842\n",
      "Train Epoch: 2 [16128/21026 (77%)]\tLoss: 0.171802\n",
      "Train Epoch: 2 [16192/21026 (77%)]\tLoss: 0.171171\n",
      "Train Epoch: 2 [16256/21026 (77%)]\tLoss: 0.159710\n",
      "Train Epoch: 2 [16320/21026 (78%)]\tLoss: 0.189140\n",
      "Train Epoch: 2 [16384/21026 (78%)]\tLoss: 0.175828\n",
      "Train Epoch: 2 [16448/21026 (78%)]\tLoss: 0.174218\n",
      "Train Epoch: 2 [16512/21026 (78%)]\tLoss: 0.180051\n",
      "Train Epoch: 2 [16576/21026 (79%)]\tLoss: 0.198678\n",
      "Train Epoch: 2 [16640/21026 (79%)]\tLoss: 0.184153\n",
      "Train Epoch: 2 [16704/21026 (79%)]\tLoss: 0.194883\n",
      "Train Epoch: 2 [16768/21026 (80%)]\tLoss: 0.177846\n",
      "Train Epoch: 2 [16832/21026 (80%)]\tLoss: 0.167844\n",
      "Train Epoch: 2 [16896/21026 (80%)]\tLoss: 0.203449\n",
      "Train Epoch: 2 [16960/21026 (81%)]\tLoss: 0.191626\n",
      "Train Epoch: 2 [17024/21026 (81%)]\tLoss: 0.184433\n",
      "Train Epoch: 2 [17088/21026 (81%)]\tLoss: 0.172297\n",
      "Train Epoch: 2 [17152/21026 (81%)]\tLoss: 0.163231\n",
      "Train Epoch: 2 [17216/21026 (82%)]\tLoss: 0.171408\n",
      "Train Epoch: 2 [17280/21026 (82%)]\tLoss: 0.163230\n",
      "Train Epoch: 2 [17344/21026 (82%)]\tLoss: 0.151306\n",
      "Train Epoch: 2 [17408/21026 (83%)]\tLoss: 0.231441\n",
      "Train Epoch: 2 [17472/21026 (83%)]\tLoss: 0.197981\n",
      "Train Epoch: 2 [17536/21026 (83%)]\tLoss: 0.160771\n",
      "Train Epoch: 2 [17600/21026 (84%)]\tLoss: 0.174399\n",
      "Train Epoch: 2 [17664/21026 (84%)]\tLoss: 0.179192\n",
      "Train Epoch: 2 [17728/21026 (84%)]\tLoss: 0.201932\n",
      "Train Epoch: 2 [17792/21026 (84%)]\tLoss: 0.226600\n",
      "Train Epoch: 2 [17856/21026 (85%)]\tLoss: 0.197487\n",
      "Train Epoch: 2 [17920/21026 (85%)]\tLoss: 0.168229\n",
      "Train Epoch: 2 [17984/21026 (85%)]\tLoss: 0.165932\n",
      "Train Epoch: 2 [18048/21026 (86%)]\tLoss: 0.187529\n",
      "Train Epoch: 2 [18112/21026 (86%)]\tLoss: 0.211909\n",
      "Train Epoch: 2 [18176/21026 (86%)]\tLoss: 0.166864\n",
      "Train Epoch: 2 [18240/21026 (87%)]\tLoss: 0.164820\n",
      "Train Epoch: 2 [18304/21026 (87%)]\tLoss: 0.198944\n",
      "Train Epoch: 2 [18368/21026 (87%)]\tLoss: 0.182877\n",
      "Train Epoch: 2 [18432/21026 (88%)]\tLoss: 0.190618\n",
      "Train Epoch: 2 [18496/21026 (88%)]\tLoss: 0.165975\n",
      "Train Epoch: 2 [18560/21026 (88%)]\tLoss: 0.197932\n",
      "Train Epoch: 2 [18624/21026 (88%)]\tLoss: 0.161316\n",
      "Train Epoch: 2 [18688/21026 (89%)]\tLoss: 0.210515\n",
      "Train Epoch: 2 [18752/21026 (89%)]\tLoss: 0.131995\n",
      "Train Epoch: 2 [18816/21026 (89%)]\tLoss: 0.184067\n",
      "Train Epoch: 2 [18880/21026 (90%)]\tLoss: 0.184663\n",
      "Train Epoch: 2 [18944/21026 (90%)]\tLoss: 0.165138\n",
      "Train Epoch: 2 [19008/21026 (90%)]\tLoss: 0.162056\n",
      "Train Epoch: 2 [19072/21026 (91%)]\tLoss: 0.185147\n",
      "Train Epoch: 2 [19136/21026 (91%)]\tLoss: 0.158599\n",
      "Train Epoch: 2 [19200/21026 (91%)]\tLoss: 0.207749\n",
      "Train Epoch: 2 [19264/21026 (91%)]\tLoss: 0.168231\n",
      "Train Epoch: 2 [19328/21026 (92%)]\tLoss: 0.168555\n",
      "Train Epoch: 2 [19392/21026 (92%)]\tLoss: 0.162179\n",
      "Train Epoch: 2 [19456/21026 (92%)]\tLoss: 0.183319\n",
      "Train Epoch: 2 [19520/21026 (93%)]\tLoss: 0.153882\n",
      "Train Epoch: 2 [19584/21026 (93%)]\tLoss: 0.191668\n",
      "Train Epoch: 2 [19648/21026 (93%)]\tLoss: 0.144119\n",
      "Train Epoch: 2 [19712/21026 (94%)]\tLoss: 0.184048\n",
      "Train Epoch: 2 [19776/21026 (94%)]\tLoss: 0.180672\n",
      "Train Epoch: 2 [19840/21026 (94%)]\tLoss: 0.172668\n",
      "Train Epoch: 2 [19904/21026 (95%)]\tLoss: 0.193634\n",
      "Train Epoch: 2 [19968/21026 (95%)]\tLoss: 0.188868\n",
      "Train Epoch: 2 [20032/21026 (95%)]\tLoss: 0.185902\n",
      "Train Epoch: 2 [20096/21026 (95%)]\tLoss: 0.218178\n",
      "Train Epoch: 2 [20160/21026 (96%)]\tLoss: 0.186340\n",
      "Train Epoch: 2 [20224/21026 (96%)]\tLoss: 0.177025\n",
      "Train Epoch: 2 [20288/21026 (96%)]\tLoss: 0.215960\n",
      "Train Epoch: 2 [20352/21026 (97%)]\tLoss: 0.146618\n",
      "Train Epoch: 2 [20416/21026 (97%)]\tLoss: 0.199867\n",
      "Train Epoch: 2 [20480/21026 (97%)]\tLoss: 0.186162\n",
      "Train Epoch: 2 [20544/21026 (98%)]\tLoss: 0.169164\n",
      "Train Epoch: 2 [20608/21026 (98%)]\tLoss: 0.171396\n",
      "Train Epoch: 2 [20672/21026 (98%)]\tLoss: 0.200488\n",
      "Train Epoch: 2 [20736/21026 (98%)]\tLoss: 0.196726\n",
      "Train Epoch: 2 [20800/21026 (99%)]\tLoss: 0.141473\n",
      "Train Epoch: 2 [20864/21026 (99%)]\tLoss: 0.165091\n",
      "Train Epoch: 2 [20928/21026 (99%)]\tLoss: 0.184350\n",
      "Train Epoch: 2 [11152/21026 (100%)]\tLoss: 0.172055\n",
      "====> Epoch: 2 Average loss: 0.00576897\n",
      "Train Epoch: 3 [0/21026 (0%)]\tLoss: 0.130622\n",
      "Train Epoch: 3 [64/21026 (0%)]\tLoss: 0.163408\n",
      "Train Epoch: 3 [128/21026 (1%)]\tLoss: 0.150919\n",
      "Train Epoch: 3 [192/21026 (1%)]\tLoss: 0.137836\n",
      "Train Epoch: 3 [256/21026 (1%)]\tLoss: 0.165787\n",
      "Train Epoch: 3 [320/21026 (2%)]\tLoss: 0.149431\n",
      "Train Epoch: 3 [384/21026 (2%)]\tLoss: 0.148939\n",
      "Train Epoch: 3 [448/21026 (2%)]\tLoss: 0.141594\n",
      "Train Epoch: 3 [512/21026 (2%)]\tLoss: 0.151184\n",
      "Train Epoch: 3 [576/21026 (3%)]\tLoss: 0.139808\n",
      "Train Epoch: 3 [640/21026 (3%)]\tLoss: 0.138759\n",
      "Train Epoch: 3 [704/21026 (3%)]\tLoss: 0.136950\n",
      "Train Epoch: 3 [768/21026 (4%)]\tLoss: 0.148811\n",
      "Train Epoch: 3 [832/21026 (4%)]\tLoss: 0.146008\n",
      "Train Epoch: 3 [896/21026 (4%)]\tLoss: 0.136888\n",
      "Train Epoch: 3 [960/21026 (5%)]\tLoss: 0.144881\n",
      "Train Epoch: 3 [1024/21026 (5%)]\tLoss: 0.140663\n",
      "Train Epoch: 3 [1088/21026 (5%)]\tLoss: 0.126536\n",
      "Train Epoch: 3 [1152/21026 (5%)]\tLoss: 0.145244\n",
      "Train Epoch: 3 [1216/21026 (6%)]\tLoss: 0.138115\n",
      "Train Epoch: 3 [1280/21026 (6%)]\tLoss: 0.136573\n",
      "Train Epoch: 3 [1344/21026 (6%)]\tLoss: 0.147825\n",
      "Train Epoch: 3 [1408/21026 (7%)]\tLoss: 0.148396\n",
      "Train Epoch: 3 [1472/21026 (7%)]\tLoss: 0.157348\n",
      "Train Epoch: 3 [1536/21026 (7%)]\tLoss: 0.130872\n",
      "Train Epoch: 3 [1600/21026 (8%)]\tLoss: 0.129038\n",
      "Train Epoch: 3 [1664/21026 (8%)]\tLoss: 0.150665\n",
      "Train Epoch: 3 [1728/21026 (8%)]\tLoss: 0.138350\n",
      "Train Epoch: 3 [1792/21026 (9%)]\tLoss: 0.156397\n",
      "Train Epoch: 3 [1856/21026 (9%)]\tLoss: 0.153575\n",
      "Train Epoch: 3 [1920/21026 (9%)]\tLoss: 0.140569\n",
      "Train Epoch: 3 [1984/21026 (9%)]\tLoss: 0.123763\n",
      "Train Epoch: 3 [2048/21026 (10%)]\tLoss: 0.130346\n",
      "Train Epoch: 3 [2112/21026 (10%)]\tLoss: 0.157508\n",
      "Train Epoch: 3 [2176/21026 (10%)]\tLoss: 0.134346\n",
      "Train Epoch: 3 [2240/21026 (11%)]\tLoss: 0.143185\n",
      "Train Epoch: 3 [2304/21026 (11%)]\tLoss: 0.147652\n",
      "Train Epoch: 3 [2368/21026 (11%)]\tLoss: 0.138551\n",
      "Train Epoch: 3 [2432/21026 (12%)]\tLoss: 0.141084\n",
      "Train Epoch: 3 [2496/21026 (12%)]\tLoss: 0.135810\n",
      "Train Epoch: 3 [2560/21026 (12%)]\tLoss: 0.133801\n",
      "Train Epoch: 3 [2624/21026 (12%)]\tLoss: 0.148077\n",
      "Train Epoch: 3 [2688/21026 (13%)]\tLoss: 0.136236\n",
      "Train Epoch: 3 [2752/21026 (13%)]\tLoss: 0.132380\n",
      "Train Epoch: 3 [2816/21026 (13%)]\tLoss: 0.138783\n",
      "Train Epoch: 3 [2880/21026 (14%)]\tLoss: 0.123743\n",
      "Train Epoch: 3 [2944/21026 (14%)]\tLoss: 0.138398\n",
      "Train Epoch: 3 [3008/21026 (14%)]\tLoss: 0.141520\n",
      "Train Epoch: 3 [3072/21026 (15%)]\tLoss: 0.136405\n",
      "Train Epoch: 3 [3136/21026 (15%)]\tLoss: 0.129175\n",
      "Train Epoch: 3 [3200/21026 (15%)]\tLoss: 0.142547\n",
      "Train Epoch: 3 [3264/21026 (16%)]\tLoss: 0.153167\n",
      "Train Epoch: 3 [3328/21026 (16%)]\tLoss: 0.147885\n",
      "Train Epoch: 3 [3392/21026 (16%)]\tLoss: 0.141255\n",
      "Train Epoch: 3 [3456/21026 (16%)]\tLoss: 0.149392\n",
      "Train Epoch: 3 [3520/21026 (17%)]\tLoss: 0.151036\n",
      "Train Epoch: 3 [3584/21026 (17%)]\tLoss: 0.143559\n",
      "Train Epoch: 3 [3648/21026 (17%)]\tLoss: 0.125558\n",
      "Train Epoch: 3 [3712/21026 (18%)]\tLoss: 0.151949\n",
      "Train Epoch: 3 [3776/21026 (18%)]\tLoss: 0.139522\n",
      "Train Epoch: 3 [3840/21026 (18%)]\tLoss: 0.145435\n",
      "Train Epoch: 3 [3904/21026 (19%)]\tLoss: 0.136705\n",
      "Train Epoch: 3 [3968/21026 (19%)]\tLoss: 0.136442\n",
      "Train Epoch: 3 [4032/21026 (19%)]\tLoss: 0.133596\n",
      "Train Epoch: 3 [4096/21026 (19%)]\tLoss: 0.142167\n",
      "Train Epoch: 3 [4160/21026 (20%)]\tLoss: 0.140023\n",
      "Train Epoch: 3 [4224/21026 (20%)]\tLoss: 0.124596\n",
      "Train Epoch: 3 [4288/21026 (20%)]\tLoss: 0.135255\n",
      "Train Epoch: 3 [4352/21026 (21%)]\tLoss: 0.126753\n",
      "Train Epoch: 3 [4416/21026 (21%)]\tLoss: 0.143031\n",
      "Train Epoch: 3 [4480/21026 (21%)]\tLoss: 0.136812\n",
      "Train Epoch: 3 [4544/21026 (22%)]\tLoss: 0.138485\n",
      "Train Epoch: 3 [4608/21026 (22%)]\tLoss: 0.127288\n",
      "Train Epoch: 3 [4672/21026 (22%)]\tLoss: 0.155521\n",
      "Train Epoch: 3 [4736/21026 (22%)]\tLoss: 0.142562\n",
      "Train Epoch: 3 [4800/21026 (23%)]\tLoss: 0.117287\n",
      "Train Epoch: 3 [4864/21026 (23%)]\tLoss: 0.140700\n",
      "Train Epoch: 3 [4928/21026 (23%)]\tLoss: 0.139883\n",
      "Train Epoch: 3 [4992/21026 (24%)]\tLoss: 0.132752\n",
      "Train Epoch: 3 [5056/21026 (24%)]\tLoss: 0.139248\n",
      "Train Epoch: 3 [5120/21026 (24%)]\tLoss: 0.138622\n",
      "Train Epoch: 3 [5184/21026 (25%)]\tLoss: 0.128304\n",
      "Train Epoch: 3 [5248/21026 (25%)]\tLoss: 0.132374\n",
      "Train Epoch: 3 [5312/21026 (25%)]\tLoss: 0.145412\n",
      "Train Epoch: 3 [5376/21026 (26%)]\tLoss: 0.126948\n",
      "Train Epoch: 3 [5440/21026 (26%)]\tLoss: 0.149366\n",
      "Train Epoch: 3 [5504/21026 (26%)]\tLoss: 0.127421\n",
      "Train Epoch: 3 [5568/21026 (26%)]\tLoss: 0.136156\n",
      "Train Epoch: 3 [5632/21026 (27%)]\tLoss: 0.142274\n",
      "Train Epoch: 3 [5696/21026 (27%)]\tLoss: 0.151748\n",
      "Train Epoch: 3 [5760/21026 (27%)]\tLoss: 0.133525\n",
      "Train Epoch: 3 [5824/21026 (28%)]\tLoss: 0.136677\n",
      "Train Epoch: 3 [5888/21026 (28%)]\tLoss: 0.134137\n",
      "Train Epoch: 3 [5952/21026 (28%)]\tLoss: 0.131657\n",
      "Train Epoch: 3 [6016/21026 (29%)]\tLoss: 0.136368\n",
      "Train Epoch: 3 [6080/21026 (29%)]\tLoss: 0.136636\n",
      "Train Epoch: 3 [6144/21026 (29%)]\tLoss: 0.139255\n",
      "Train Epoch: 3 [6208/21026 (29%)]\tLoss: 0.145441\n",
      "Train Epoch: 3 [6272/21026 (30%)]\tLoss: 0.150770\n",
      "Train Epoch: 3 [6336/21026 (30%)]\tLoss: 0.143913\n",
      "Train Epoch: 3 [6400/21026 (30%)]\tLoss: 0.128598\n",
      "Train Epoch: 3 [6464/21026 (31%)]\tLoss: 0.133942\n",
      "Train Epoch: 3 [6528/21026 (31%)]\tLoss: 0.139616\n",
      "Train Epoch: 3 [6592/21026 (31%)]\tLoss: 0.138571\n",
      "Train Epoch: 3 [6656/21026 (32%)]\tLoss: 0.128783\n",
      "Train Epoch: 3 [6720/21026 (32%)]\tLoss: 0.132013\n",
      "Train Epoch: 3 [6784/21026 (32%)]\tLoss: 0.131744\n",
      "Train Epoch: 3 [6848/21026 (33%)]\tLoss: 0.167591\n",
      "Train Epoch: 3 [6912/21026 (33%)]\tLoss: 0.148724\n",
      "Train Epoch: 3 [6976/21026 (33%)]\tLoss: 0.154505\n",
      "Train Epoch: 3 [7040/21026 (33%)]\tLoss: 0.155464\n",
      "Train Epoch: 3 [7104/21026 (34%)]\tLoss: 0.131290\n",
      "Train Epoch: 3 [7168/21026 (34%)]\tLoss: 0.130051\n",
      "Train Epoch: 3 [7232/21026 (34%)]\tLoss: 0.141473\n",
      "Train Epoch: 3 [7296/21026 (35%)]\tLoss: 0.149854\n",
      "Train Epoch: 3 [7360/21026 (35%)]\tLoss: 0.138768\n",
      "Train Epoch: 3 [7424/21026 (35%)]\tLoss: 0.125187\n",
      "Train Epoch: 3 [7488/21026 (36%)]\tLoss: 0.128850\n",
      "Train Epoch: 3 [7552/21026 (36%)]\tLoss: 0.143082\n",
      "Train Epoch: 3 [7616/21026 (36%)]\tLoss: 0.155185\n",
      "Train Epoch: 3 [7680/21026 (36%)]\tLoss: 0.127889\n",
      "Train Epoch: 3 [7744/21026 (37%)]\tLoss: 0.147298\n",
      "Train Epoch: 3 [7808/21026 (37%)]\tLoss: 0.154976\n",
      "Train Epoch: 3 [7872/21026 (37%)]\tLoss: 0.147607\n",
      "Train Epoch: 3 [7936/21026 (38%)]\tLoss: 0.163982\n",
      "Train Epoch: 3 [8000/21026 (38%)]\tLoss: 0.153025\n",
      "Train Epoch: 3 [8064/21026 (38%)]\tLoss: 0.133606\n",
      "Train Epoch: 3 [8128/21026 (39%)]\tLoss: 0.149088\n",
      "Train Epoch: 3 [8192/21026 (39%)]\tLoss: 0.129180\n",
      "Train Epoch: 3 [8256/21026 (39%)]\tLoss: 0.127272\n",
      "Train Epoch: 3 [8320/21026 (40%)]\tLoss: 0.146675\n",
      "Train Epoch: 3 [8384/21026 (40%)]\tLoss: 0.131099\n",
      "Train Epoch: 3 [8448/21026 (40%)]\tLoss: 0.124008\n",
      "Train Epoch: 3 [8512/21026 (40%)]\tLoss: 0.129543\n",
      "Train Epoch: 3 [8576/21026 (41%)]\tLoss: 0.142575\n",
      "Train Epoch: 3 [8640/21026 (41%)]\tLoss: 0.137369\n",
      "Train Epoch: 3 [8704/21026 (41%)]\tLoss: 0.124879\n",
      "Train Epoch: 3 [8768/21026 (42%)]\tLoss: 0.139725\n",
      "Train Epoch: 3 [8832/21026 (42%)]\tLoss: 0.153776\n",
      "Train Epoch: 3 [8896/21026 (42%)]\tLoss: 0.140822\n",
      "Train Epoch: 3 [8960/21026 (43%)]\tLoss: 0.140324\n",
      "Train Epoch: 3 [9024/21026 (43%)]\tLoss: 0.136849\n",
      "Train Epoch: 3 [9088/21026 (43%)]\tLoss: 0.134371\n",
      "Train Epoch: 3 [9152/21026 (43%)]\tLoss: 0.149848\n",
      "Train Epoch: 3 [9216/21026 (44%)]\tLoss: 0.148764\n",
      "Train Epoch: 3 [9280/21026 (44%)]\tLoss: 0.149745\n",
      "Train Epoch: 3 [9344/21026 (44%)]\tLoss: 0.122805\n",
      "Train Epoch: 3 [9408/21026 (45%)]\tLoss: 0.138659\n",
      "Train Epoch: 3 [9472/21026 (45%)]\tLoss: 0.148101\n",
      "Train Epoch: 3 [9536/21026 (45%)]\tLoss: 0.130563\n",
      "Train Epoch: 3 [9600/21026 (46%)]\tLoss: 0.146067\n",
      "Train Epoch: 3 [9664/21026 (46%)]\tLoss: 0.145387\n",
      "Train Epoch: 3 [9728/21026 (46%)]\tLoss: 0.158827\n",
      "Train Epoch: 3 [9792/21026 (47%)]\tLoss: 0.163128\n",
      "Train Epoch: 3 [9856/21026 (47%)]\tLoss: 0.141175\n",
      "Train Epoch: 3 [9920/21026 (47%)]\tLoss: 0.136008\n",
      "Train Epoch: 3 [9984/21026 (47%)]\tLoss: 0.127139\n",
      "Train Epoch: 3 [10048/21026 (48%)]\tLoss: 0.150871\n",
      "Train Epoch: 3 [10112/21026 (48%)]\tLoss: 0.144936\n",
      "Train Epoch: 3 [10176/21026 (48%)]\tLoss: 0.147733\n",
      "Train Epoch: 3 [10240/21026 (49%)]\tLoss: 0.140370\n",
      "Train Epoch: 3 [10304/21026 (49%)]\tLoss: 0.126048\n",
      "Train Epoch: 3 [10368/21026 (49%)]\tLoss: 0.150717\n",
      "Train Epoch: 3 [10432/21026 (50%)]\tLoss: 0.137735\n",
      "Train Epoch: 3 [10496/21026 (50%)]\tLoss: 0.171248\n",
      "Train Epoch: 3 [10560/21026 (50%)]\tLoss: 0.163546\n",
      "Train Epoch: 3 [10624/21026 (50%)]\tLoss: 0.172377\n",
      "Train Epoch: 3 [10688/21026 (51%)]\tLoss: 0.162708\n",
      "Train Epoch: 3 [10752/21026 (51%)]\tLoss: 0.149989\n",
      "Train Epoch: 3 [10816/21026 (51%)]\tLoss: 0.158882\n",
      "Train Epoch: 3 [10880/21026 (52%)]\tLoss: 0.138826\n",
      "Train Epoch: 3 [10944/21026 (52%)]\tLoss: 0.151339\n",
      "Train Epoch: 3 [11008/21026 (52%)]\tLoss: 0.157842\n",
      "Train Epoch: 3 [11072/21026 (53%)]\tLoss: 0.145887\n",
      "Train Epoch: 3 [11136/21026 (53%)]\tLoss: 0.150045\n",
      "Train Epoch: 3 [11200/21026 (53%)]\tLoss: 0.157747\n",
      "Train Epoch: 3 [11264/21026 (53%)]\tLoss: 0.143973\n",
      "Train Epoch: 3 [11328/21026 (54%)]\tLoss: 0.126810\n",
      "Train Epoch: 3 [11392/21026 (54%)]\tLoss: 0.138994\n",
      "Train Epoch: 3 [11456/21026 (54%)]\tLoss: 0.153625\n",
      "Train Epoch: 3 [11520/21026 (55%)]\tLoss: 0.143554\n",
      "Train Epoch: 3 [11584/21026 (55%)]\tLoss: 0.125092\n",
      "Train Epoch: 3 [11648/21026 (55%)]\tLoss: 0.167054\n",
      "Train Epoch: 3 [11712/21026 (56%)]\tLoss: 0.150334\n",
      "Train Epoch: 3 [11776/21026 (56%)]\tLoss: 0.146341\n",
      "Train Epoch: 3 [11840/21026 (56%)]\tLoss: 0.148059\n",
      "Train Epoch: 3 [11904/21026 (57%)]\tLoss: 0.143152\n",
      "Train Epoch: 3 [11968/21026 (57%)]\tLoss: 0.148076\n",
      "Train Epoch: 3 [12032/21026 (57%)]\tLoss: 0.134963\n",
      "Train Epoch: 3 [12096/21026 (57%)]\tLoss: 0.132064\n",
      "Train Epoch: 3 [12160/21026 (58%)]\tLoss: 0.151229\n",
      "Train Epoch: 3 [12224/21026 (58%)]\tLoss: 0.152114\n",
      "Train Epoch: 3 [12288/21026 (58%)]\tLoss: 0.141002\n",
      "Train Epoch: 3 [12352/21026 (59%)]\tLoss: 0.156776\n",
      "Train Epoch: 3 [12416/21026 (59%)]\tLoss: 0.137041\n",
      "Train Epoch: 3 [12480/21026 (59%)]\tLoss: 0.141963\n",
      "Train Epoch: 3 [12544/21026 (60%)]\tLoss: 0.144925\n",
      "Train Epoch: 3 [12608/21026 (60%)]\tLoss: 0.152940\n",
      "Train Epoch: 3 [12672/21026 (60%)]\tLoss: 0.137191\n",
      "Train Epoch: 3 [12736/21026 (60%)]\tLoss: 0.142163\n",
      "Train Epoch: 3 [12800/21026 (61%)]\tLoss: 0.131803\n",
      "Train Epoch: 3 [12864/21026 (61%)]\tLoss: 0.145763\n",
      "Train Epoch: 3 [12928/21026 (61%)]\tLoss: 0.119953\n",
      "Train Epoch: 3 [12992/21026 (62%)]\tLoss: 0.137410\n",
      "Train Epoch: 3 [13056/21026 (62%)]\tLoss: 0.144485\n",
      "Train Epoch: 3 [13120/21026 (62%)]\tLoss: 0.131443\n",
      "Train Epoch: 3 [13184/21026 (63%)]\tLoss: 0.153185\n",
      "Train Epoch: 3 [13248/21026 (63%)]\tLoss: 0.146304\n",
      "Train Epoch: 3 [13312/21026 (63%)]\tLoss: 0.139113\n",
      "Train Epoch: 3 [13376/21026 (64%)]\tLoss: 0.157765\n",
      "Train Epoch: 3 [13440/21026 (64%)]\tLoss: 0.136488\n",
      "Train Epoch: 3 [13504/21026 (64%)]\tLoss: 0.157494\n",
      "Train Epoch: 3 [13568/21026 (64%)]\tLoss: 0.152531\n",
      "Train Epoch: 3 [13632/21026 (65%)]\tLoss: 0.156930\n",
      "Train Epoch: 3 [13696/21026 (65%)]\tLoss: 0.144975\n",
      "Train Epoch: 3 [13760/21026 (65%)]\tLoss: 0.143763\n",
      "Train Epoch: 3 [13824/21026 (66%)]\tLoss: 0.145126\n",
      "Train Epoch: 3 [13888/21026 (66%)]\tLoss: 0.139548\n",
      "Train Epoch: 3 [13952/21026 (66%)]\tLoss: 0.148505\n",
      "Train Epoch: 3 [14016/21026 (67%)]\tLoss: 0.156736\n",
      "Train Epoch: 3 [14080/21026 (67%)]\tLoss: 0.148510\n",
      "Train Epoch: 3 [14144/21026 (67%)]\tLoss: 0.145420\n",
      "Train Epoch: 3 [14208/21026 (67%)]\tLoss: 0.140964\n",
      "Train Epoch: 3 [14272/21026 (68%)]\tLoss: 0.141249\n",
      "Train Epoch: 3 [14336/21026 (68%)]\tLoss: 0.142045\n",
      "Train Epoch: 3 [14400/21026 (68%)]\tLoss: 0.146411\n",
      "Train Epoch: 3 [14464/21026 (69%)]\tLoss: 0.144690\n",
      "Train Epoch: 3 [14528/21026 (69%)]\tLoss: 0.143965\n",
      "Train Epoch: 3 [14592/21026 (69%)]\tLoss: 0.134186\n",
      "Train Epoch: 3 [14656/21026 (70%)]\tLoss: 0.138013\n",
      "Train Epoch: 3 [14720/21026 (70%)]\tLoss: 0.130157\n",
      "Train Epoch: 3 [14784/21026 (70%)]\tLoss: 0.134901\n",
      "Train Epoch: 3 [14848/21026 (71%)]\tLoss: 0.130156\n",
      "Train Epoch: 3 [14912/21026 (71%)]\tLoss: 0.156945\n",
      "Train Epoch: 3 [14976/21026 (71%)]\tLoss: 0.131776\n",
      "Train Epoch: 3 [15040/21026 (71%)]\tLoss: 0.142850\n",
      "Train Epoch: 3 [15104/21026 (72%)]\tLoss: 0.130946\n",
      "Train Epoch: 3 [15168/21026 (72%)]\tLoss: 0.157060\n",
      "Train Epoch: 3 [15232/21026 (72%)]\tLoss: 0.147536\n",
      "Train Epoch: 3 [15296/21026 (73%)]\tLoss: 0.144299\n",
      "Train Epoch: 3 [15360/21026 (73%)]\tLoss: 0.154997\n",
      "Train Epoch: 3 [15424/21026 (73%)]\tLoss: 0.131658\n",
      "Train Epoch: 3 [15488/21026 (74%)]\tLoss: 0.141291\n",
      "Train Epoch: 3 [15552/21026 (74%)]\tLoss: 0.129333\n",
      "Train Epoch: 3 [15616/21026 (74%)]\tLoss: 0.130553\n",
      "Train Epoch: 3 [15680/21026 (74%)]\tLoss: 0.139539\n",
      "Train Epoch: 3 [15744/21026 (75%)]\tLoss: 0.135937\n",
      "Train Epoch: 3 [15808/21026 (75%)]\tLoss: 0.122455\n",
      "Train Epoch: 3 [15872/21026 (75%)]\tLoss: 0.134993\n",
      "Train Epoch: 3 [15936/21026 (76%)]\tLoss: 0.181017\n",
      "Train Epoch: 3 [16000/21026 (76%)]\tLoss: 0.133734\n",
      "Train Epoch: 3 [16064/21026 (76%)]\tLoss: 0.115832\n",
      "Train Epoch: 3 [16128/21026 (77%)]\tLoss: 0.132573\n",
      "Train Epoch: 3 [16192/21026 (77%)]\tLoss: 0.161817\n",
      "Train Epoch: 3 [16256/21026 (77%)]\tLoss: 0.126561\n",
      "Train Epoch: 3 [16320/21026 (78%)]\tLoss: 0.138364\n",
      "Train Epoch: 3 [16384/21026 (78%)]\tLoss: 0.135384\n",
      "Train Epoch: 3 [16448/21026 (78%)]\tLoss: 0.143914\n",
      "Train Epoch: 3 [16512/21026 (78%)]\tLoss: 0.169372\n",
      "Train Epoch: 3 [16576/21026 (79%)]\tLoss: 0.167976\n",
      "Train Epoch: 3 [16640/21026 (79%)]\tLoss: 0.146650\n",
      "Train Epoch: 3 [16704/21026 (79%)]\tLoss: 0.164904\n",
      "Train Epoch: 3 [16768/21026 (80%)]\tLoss: 0.152808\n",
      "Train Epoch: 3 [16832/21026 (80%)]\tLoss: 0.130757\n",
      "Train Epoch: 3 [16896/21026 (80%)]\tLoss: 0.157467\n",
      "Train Epoch: 3 [16960/21026 (81%)]\tLoss: 0.150349\n",
      "Train Epoch: 3 [17024/21026 (81%)]\tLoss: 0.138284\n",
      "Train Epoch: 3 [17088/21026 (81%)]\tLoss: 0.148533\n",
      "Train Epoch: 3 [17152/21026 (81%)]\tLoss: 0.151508\n",
      "Train Epoch: 3 [17216/21026 (82%)]\tLoss: 0.143856\n",
      "Train Epoch: 3 [17280/21026 (82%)]\tLoss: 0.147287\n",
      "Train Epoch: 3 [17344/21026 (82%)]\tLoss: 0.165663\n",
      "Train Epoch: 3 [17408/21026 (83%)]\tLoss: 0.155941\n",
      "Train Epoch: 3 [17472/21026 (83%)]\tLoss: 0.118328\n",
      "Train Epoch: 3 [17536/21026 (83%)]\tLoss: 0.131786\n",
      "Train Epoch: 3 [17600/21026 (84%)]\tLoss: 0.137569\n",
      "Train Epoch: 3 [17664/21026 (84%)]\tLoss: 0.131163\n",
      "Train Epoch: 3 [17728/21026 (84%)]\tLoss: 0.127205\n",
      "Train Epoch: 3 [17792/21026 (84%)]\tLoss: 0.134763\n",
      "Train Epoch: 3 [17856/21026 (85%)]\tLoss: 0.142266\n",
      "Train Epoch: 3 [17920/21026 (85%)]\tLoss: 0.135923\n",
      "Train Epoch: 3 [17984/21026 (85%)]\tLoss: 0.129176\n",
      "Train Epoch: 3 [18048/21026 (86%)]\tLoss: 0.124114\n",
      "Train Epoch: 3 [18112/21026 (86%)]\tLoss: 0.140813\n",
      "Train Epoch: 3 [18176/21026 (86%)]\tLoss: 0.155542\n",
      "Train Epoch: 3 [18240/21026 (87%)]\tLoss: 0.163014\n",
      "Train Epoch: 3 [18304/21026 (87%)]\tLoss: 0.139599\n",
      "Train Epoch: 3 [18368/21026 (87%)]\tLoss: 0.144114\n",
      "Train Epoch: 3 [18432/21026 (88%)]\tLoss: 0.131207\n",
      "Train Epoch: 3 [18496/21026 (88%)]\tLoss: 0.138158\n",
      "Train Epoch: 3 [18560/21026 (88%)]\tLoss: 0.145287\n",
      "Train Epoch: 3 [18624/21026 (88%)]\tLoss: 0.125410\n",
      "Train Epoch: 3 [18688/21026 (89%)]\tLoss: 0.139480\n",
      "Train Epoch: 3 [18752/21026 (89%)]\tLoss: 0.145900\n",
      "Train Epoch: 3 [18816/21026 (89%)]\tLoss: 0.136354\n",
      "Train Epoch: 3 [18880/21026 (90%)]\tLoss: 0.142645\n",
      "Train Epoch: 3 [18944/21026 (90%)]\tLoss: 0.148399\n",
      "Train Epoch: 3 [19008/21026 (90%)]\tLoss: 0.133720\n",
      "Train Epoch: 3 [19072/21026 (91%)]\tLoss: 0.148846\n",
      "Train Epoch: 3 [19136/21026 (91%)]\tLoss: 0.139313\n",
      "Train Epoch: 3 [19200/21026 (91%)]\tLoss: 0.130447\n",
      "Train Epoch: 3 [19264/21026 (91%)]\tLoss: 0.147466\n",
      "Train Epoch: 3 [19328/21026 (92%)]\tLoss: 0.155371\n",
      "Train Epoch: 3 [19392/21026 (92%)]\tLoss: 0.153055\n",
      "Train Epoch: 3 [19456/21026 (92%)]\tLoss: 0.153917\n",
      "Train Epoch: 3 [19520/21026 (93%)]\tLoss: 0.157394\n",
      "Train Epoch: 3 [19584/21026 (93%)]\tLoss: 0.134910\n",
      "Train Epoch: 3 [19648/21026 (93%)]\tLoss: 0.117153\n",
      "Train Epoch: 3 [19712/21026 (94%)]\tLoss: 0.139479\n",
      "Train Epoch: 3 [19776/21026 (94%)]\tLoss: 0.160730\n",
      "Train Epoch: 3 [19840/21026 (94%)]\tLoss: 0.146599\n",
      "Train Epoch: 3 [19904/21026 (95%)]\tLoss: 0.145884\n",
      "Train Epoch: 3 [19968/21026 (95%)]\tLoss: 0.148995\n",
      "Train Epoch: 3 [20032/21026 (95%)]\tLoss: 0.150758\n",
      "Train Epoch: 3 [20096/21026 (95%)]\tLoss: 0.155262\n",
      "Train Epoch: 3 [20160/21026 (96%)]\tLoss: 0.156924\n",
      "Train Epoch: 3 [20224/21026 (96%)]\tLoss: 0.132259\n",
      "Train Epoch: 3 [20288/21026 (96%)]\tLoss: 0.147913\n",
      "Train Epoch: 3 [20352/21026 (97%)]\tLoss: 0.135964\n",
      "Train Epoch: 3 [20416/21026 (97%)]\tLoss: 0.133162\n",
      "Train Epoch: 3 [20480/21026 (97%)]\tLoss: 0.151900\n",
      "Train Epoch: 3 [20544/21026 (98%)]\tLoss: 0.154726\n",
      "Train Epoch: 3 [20608/21026 (98%)]\tLoss: 0.132744\n",
      "Train Epoch: 3 [20672/21026 (98%)]\tLoss: 0.150484\n",
      "Train Epoch: 3 [20736/21026 (98%)]\tLoss: 0.134500\n",
      "Train Epoch: 3 [20800/21026 (99%)]\tLoss: 0.146498\n",
      "Train Epoch: 3 [20864/21026 (99%)]\tLoss: 0.168562\n",
      "Train Epoch: 3 [20928/21026 (99%)]\tLoss: 0.139772\n",
      "Train Epoch: 3 [11152/21026 (100%)]\tLoss: 0.148643\n",
      "====> Epoch: 3 Average loss: 0.00445716\n",
      "Train Epoch: 4 [0/21026 (0%)]\tLoss: 0.118596\n",
      "Train Epoch: 4 [64/21026 (0%)]\tLoss: 0.115164\n",
      "Train Epoch: 4 [128/21026 (1%)]\tLoss: 0.106191\n",
      "Train Epoch: 4 [192/21026 (1%)]\tLoss: 0.129235\n",
      "Train Epoch: 4 [256/21026 (1%)]\tLoss: 0.120722\n",
      "Train Epoch: 4 [320/21026 (2%)]\tLoss: 0.118991\n",
      "Train Epoch: 4 [384/21026 (2%)]\tLoss: 0.120737\n",
      "Train Epoch: 4 [448/21026 (2%)]\tLoss: 0.118354\n",
      "Train Epoch: 4 [512/21026 (2%)]\tLoss: 0.106656\n",
      "Train Epoch: 4 [576/21026 (3%)]\tLoss: 0.120766\n",
      "Train Epoch: 4 [640/21026 (3%)]\tLoss: 0.129802\n",
      "Train Epoch: 4 [704/21026 (3%)]\tLoss: 0.110998\n",
      "Train Epoch: 4 [768/21026 (4%)]\tLoss: 0.117192\n",
      "Train Epoch: 4 [832/21026 (4%)]\tLoss: 0.119504\n",
      "Train Epoch: 4 [896/21026 (4%)]\tLoss: 0.114131\n",
      "Train Epoch: 4 [960/21026 (5%)]\tLoss: 0.108755\n",
      "Train Epoch: 4 [1024/21026 (5%)]\tLoss: 0.106374\n",
      "Train Epoch: 4 [1088/21026 (5%)]\tLoss: 0.124016\n",
      "Train Epoch: 4 [1152/21026 (5%)]\tLoss: 0.105450\n",
      "Train Epoch: 4 [1216/21026 (6%)]\tLoss: 0.111380\n",
      "Train Epoch: 4 [1280/21026 (6%)]\tLoss: 0.114594\n",
      "Train Epoch: 4 [1344/21026 (6%)]\tLoss: 0.108840\n",
      "Train Epoch: 4 [1408/21026 (7%)]\tLoss: 0.129094\n",
      "Train Epoch: 4 [1472/21026 (7%)]\tLoss: 0.107993\n",
      "Train Epoch: 4 [1536/21026 (7%)]\tLoss: 0.113258\n",
      "Train Epoch: 4 [1600/21026 (8%)]\tLoss: 0.104605\n",
      "Train Epoch: 4 [1664/21026 (8%)]\tLoss: 0.109877\n",
      "Train Epoch: 4 [1728/21026 (8%)]\tLoss: 0.139965\n",
      "Train Epoch: 4 [1792/21026 (9%)]\tLoss: 0.108402\n",
      "Train Epoch: 4 [1856/21026 (9%)]\tLoss: 0.098692\n",
      "Train Epoch: 4 [1920/21026 (9%)]\tLoss: 0.114792\n",
      "Train Epoch: 4 [1984/21026 (9%)]\tLoss: 0.115959\n",
      "Train Epoch: 4 [2048/21026 (10%)]\tLoss: 0.109239\n",
      "Train Epoch: 4 [2112/21026 (10%)]\tLoss: 0.113499\n",
      "Train Epoch: 4 [2176/21026 (10%)]\tLoss: 0.126574\n",
      "Train Epoch: 4 [2240/21026 (11%)]\tLoss: 0.102901\n",
      "Train Epoch: 4 [2304/21026 (11%)]\tLoss: 0.116106\n",
      "Train Epoch: 4 [2368/21026 (11%)]\tLoss: 0.109544\n",
      "Train Epoch: 4 [2432/21026 (12%)]\tLoss: 0.105407\n",
      "Train Epoch: 4 [2496/21026 (12%)]\tLoss: 0.111616\n",
      "Train Epoch: 4 [2560/21026 (12%)]\tLoss: 0.113540\n",
      "Train Epoch: 4 [2624/21026 (12%)]\tLoss: 0.102962\n",
      "Train Epoch: 4 [2688/21026 (13%)]\tLoss: 0.104920\n",
      "Train Epoch: 4 [2752/21026 (13%)]\tLoss: 0.112466\n",
      "Train Epoch: 4 [2816/21026 (13%)]\tLoss: 0.110195\n",
      "Train Epoch: 4 [2880/21026 (14%)]\tLoss: 0.111773\n",
      "Train Epoch: 4 [2944/21026 (14%)]\tLoss: 0.108491\n",
      "Train Epoch: 4 [3008/21026 (14%)]\tLoss: 0.123673\n",
      "Train Epoch: 4 [3072/21026 (15%)]\tLoss: 0.130846\n",
      "Train Epoch: 4 [3136/21026 (15%)]\tLoss: 0.113279\n",
      "Train Epoch: 4 [3200/21026 (15%)]\tLoss: 0.109527\n",
      "Train Epoch: 4 [3264/21026 (16%)]\tLoss: 0.115249\n",
      "Train Epoch: 4 [3328/21026 (16%)]\tLoss: 0.115686\n",
      "Train Epoch: 4 [3392/21026 (16%)]\tLoss: 0.114669\n",
      "Train Epoch: 4 [3456/21026 (16%)]\tLoss: 0.113769\n",
      "Train Epoch: 4 [3520/21026 (17%)]\tLoss: 0.117243\n",
      "Train Epoch: 4 [3584/21026 (17%)]\tLoss: 0.109588\n",
      "Train Epoch: 4 [3648/21026 (17%)]\tLoss: 0.107095\n",
      "Train Epoch: 4 [3712/21026 (18%)]\tLoss: 0.113929\n",
      "Train Epoch: 4 [3776/21026 (18%)]\tLoss: 0.109748\n",
      "Train Epoch: 4 [3840/21026 (18%)]\tLoss: 0.108849\n",
      "Train Epoch: 4 [3904/21026 (19%)]\tLoss: 0.112524\n",
      "Train Epoch: 4 [3968/21026 (19%)]\tLoss: 0.108087\n",
      "Train Epoch: 4 [4032/21026 (19%)]\tLoss: 0.125289\n",
      "Train Epoch: 4 [4096/21026 (19%)]\tLoss: 0.112188\n",
      "Train Epoch: 4 [4160/21026 (20%)]\tLoss: 0.107414\n",
      "Train Epoch: 4 [4224/21026 (20%)]\tLoss: 0.143817\n",
      "Train Epoch: 4 [4288/21026 (20%)]\tLoss: 0.114931\n",
      "Train Epoch: 4 [4352/21026 (21%)]\tLoss: 0.119437\n",
      "Train Epoch: 4 [4416/21026 (21%)]\tLoss: 0.121849\n",
      "Train Epoch: 4 [4480/21026 (21%)]\tLoss: 0.107538\n",
      "Train Epoch: 4 [4544/21026 (22%)]\tLoss: 0.119702\n",
      "Train Epoch: 4 [4608/21026 (22%)]\tLoss: 0.121887\n",
      "Train Epoch: 4 [4672/21026 (22%)]\tLoss: 0.125837\n",
      "Train Epoch: 4 [4736/21026 (22%)]\tLoss: 0.104808\n",
      "Train Epoch: 4 [4800/21026 (23%)]\tLoss: 0.121610\n",
      "Train Epoch: 4 [4864/21026 (23%)]\tLoss: 0.124259\n",
      "Train Epoch: 4 [4928/21026 (23%)]\tLoss: 0.106376\n",
      "Train Epoch: 4 [4992/21026 (24%)]\tLoss: 0.116474\n",
      "Train Epoch: 4 [5056/21026 (24%)]\tLoss: 0.115446\n",
      "Train Epoch: 4 [5120/21026 (24%)]\tLoss: 0.104947\n",
      "Train Epoch: 4 [5184/21026 (25%)]\tLoss: 0.106550\n",
      "Train Epoch: 4 [5248/21026 (25%)]\tLoss: 0.108645\n",
      "Train Epoch: 4 [5312/21026 (25%)]\tLoss: 0.115920\n",
      "Train Epoch: 4 [5376/21026 (26%)]\tLoss: 0.125361\n",
      "Train Epoch: 4 [5440/21026 (26%)]\tLoss: 0.108884\n",
      "Train Epoch: 4 [5504/21026 (26%)]\tLoss: 0.122927\n",
      "Train Epoch: 4 [5568/21026 (26%)]\tLoss: 0.109264\n",
      "Train Epoch: 4 [5632/21026 (27%)]\tLoss: 0.111825\n",
      "Train Epoch: 4 [5696/21026 (27%)]\tLoss: 0.110592\n",
      "Train Epoch: 4 [5760/21026 (27%)]\tLoss: 0.109240\n",
      "Train Epoch: 4 [5824/21026 (28%)]\tLoss: 0.108784\n",
      "Train Epoch: 4 [5888/21026 (28%)]\tLoss: 0.116971\n",
      "Train Epoch: 4 [5952/21026 (28%)]\tLoss: 0.121880\n",
      "Train Epoch: 4 [6016/21026 (29%)]\tLoss: 0.111424\n",
      "Train Epoch: 4 [6080/21026 (29%)]\tLoss: 0.105650\n",
      "Train Epoch: 4 [6144/21026 (29%)]\tLoss: 0.119737\n",
      "Train Epoch: 4 [6208/21026 (29%)]\tLoss: 0.115837\n",
      "Train Epoch: 4 [6272/21026 (30%)]\tLoss: 0.112130\n",
      "Train Epoch: 4 [6336/21026 (30%)]\tLoss: 0.113731\n",
      "Train Epoch: 4 [6400/21026 (30%)]\tLoss: 0.119736\n",
      "Train Epoch: 4 [6464/21026 (31%)]\tLoss: 0.110826\n",
      "Train Epoch: 4 [6528/21026 (31%)]\tLoss: 0.113223\n",
      "Train Epoch: 4 [6592/21026 (31%)]\tLoss: 0.116051\n",
      "Train Epoch: 4 [6656/21026 (32%)]\tLoss: 0.105954\n",
      "Train Epoch: 4 [6720/21026 (32%)]\tLoss: 0.114332\n",
      "Train Epoch: 4 [6784/21026 (32%)]\tLoss: 0.119717\n",
      "Train Epoch: 4 [6848/21026 (33%)]\tLoss: 0.092791\n",
      "Train Epoch: 4 [6912/21026 (33%)]\tLoss: 0.106143\n",
      "Train Epoch: 4 [6976/21026 (33%)]\tLoss: 0.113402\n",
      "Train Epoch: 4 [7040/21026 (33%)]\tLoss: 0.130634\n",
      "Train Epoch: 4 [7104/21026 (34%)]\tLoss: 0.120353\n",
      "Train Epoch: 4 [7168/21026 (34%)]\tLoss: 0.118086\n",
      "Train Epoch: 4 [7232/21026 (34%)]\tLoss: 0.129360\n",
      "Train Epoch: 4 [7296/21026 (35%)]\tLoss: 0.115820\n",
      "Train Epoch: 4 [7360/21026 (35%)]\tLoss: 0.111892\n",
      "Train Epoch: 4 [7424/21026 (35%)]\tLoss: 0.125116\n",
      "Train Epoch: 4 [7488/21026 (36%)]\tLoss: 0.121780\n",
      "Train Epoch: 4 [7552/21026 (36%)]\tLoss: 0.121014\n",
      "Train Epoch: 4 [7616/21026 (36%)]\tLoss: 0.097044\n",
      "Train Epoch: 4 [7680/21026 (36%)]\tLoss: 0.122089\n",
      "Train Epoch: 4 [7744/21026 (37%)]\tLoss: 0.134046\n",
      "Train Epoch: 4 [7808/21026 (37%)]\tLoss: 0.124004\n",
      "Train Epoch: 4 [7872/21026 (37%)]\tLoss: 0.104690\n",
      "Train Epoch: 4 [7936/21026 (38%)]\tLoss: 0.131914\n",
      "Train Epoch: 4 [8000/21026 (38%)]\tLoss: 0.128392\n",
      "Train Epoch: 4 [8064/21026 (38%)]\tLoss: 0.126946\n",
      "Train Epoch: 4 [8128/21026 (39%)]\tLoss: 0.109501\n",
      "Train Epoch: 4 [8192/21026 (39%)]\tLoss: 0.124743\n",
      "Train Epoch: 4 [8256/21026 (39%)]\tLoss: 0.112091\n",
      "Train Epoch: 4 [8320/21026 (40%)]\tLoss: 0.106030\n",
      "Train Epoch: 4 [8384/21026 (40%)]\tLoss: 0.118170\n",
      "Train Epoch: 4 [8448/21026 (40%)]\tLoss: 0.116398\n",
      "Train Epoch: 4 [8512/21026 (40%)]\tLoss: 0.134738\n",
      "Train Epoch: 4 [8576/21026 (41%)]\tLoss: 0.117144\n",
      "Train Epoch: 4 [8640/21026 (41%)]\tLoss: 0.120630\n",
      "Train Epoch: 4 [8704/21026 (41%)]\tLoss: 0.116364\n",
      "Train Epoch: 4 [8768/21026 (42%)]\tLoss: 0.115209\n",
      "Train Epoch: 4 [8832/21026 (42%)]\tLoss: 0.111641\n",
      "Train Epoch: 4 [8896/21026 (42%)]\tLoss: 0.115340\n",
      "Train Epoch: 4 [8960/21026 (43%)]\tLoss: 0.122181\n",
      "Train Epoch: 4 [9024/21026 (43%)]\tLoss: 0.119172\n",
      "Train Epoch: 4 [9088/21026 (43%)]\tLoss: 0.115453\n",
      "Train Epoch: 4 [9152/21026 (43%)]\tLoss: 0.112295\n",
      "Train Epoch: 4 [9216/21026 (44%)]\tLoss: 0.116402\n",
      "Train Epoch: 4 [9280/21026 (44%)]\tLoss: 0.121091\n",
      "Train Epoch: 4 [9344/21026 (44%)]\tLoss: 0.110229\n",
      "Train Epoch: 4 [9408/21026 (45%)]\tLoss: 0.111430\n",
      "Train Epoch: 4 [9472/21026 (45%)]\tLoss: 0.104390\n",
      "Train Epoch: 4 [9536/21026 (45%)]\tLoss: 0.122462\n",
      "Train Epoch: 4 [9600/21026 (46%)]\tLoss: 0.125978\n",
      "Train Epoch: 4 [9664/21026 (46%)]\tLoss: 0.101772\n",
      "Train Epoch: 4 [9728/21026 (46%)]\tLoss: 0.107907\n",
      "Train Epoch: 4 [9792/21026 (47%)]\tLoss: 0.121855\n",
      "Train Epoch: 4 [9856/21026 (47%)]\tLoss: 0.111248\n",
      "Train Epoch: 4 [9920/21026 (47%)]\tLoss: 0.108581\n",
      "Train Epoch: 4 [9984/21026 (47%)]\tLoss: 0.128278\n",
      "Train Epoch: 4 [10048/21026 (48%)]\tLoss: 0.123685\n",
      "Train Epoch: 4 [10112/21026 (48%)]\tLoss: 0.108488\n",
      "Train Epoch: 4 [10176/21026 (48%)]\tLoss: 0.115125\n",
      "Train Epoch: 4 [10240/21026 (49%)]\tLoss: 0.119817\n",
      "Train Epoch: 4 [10304/21026 (49%)]\tLoss: 0.118462\n",
      "Train Epoch: 4 [10368/21026 (49%)]\tLoss: 0.120579\n",
      "Train Epoch: 4 [10432/21026 (50%)]\tLoss: 0.104340\n",
      "Train Epoch: 4 [10496/21026 (50%)]\tLoss: 0.126620\n",
      "Train Epoch: 4 [10560/21026 (50%)]\tLoss: 0.110846\n",
      "Train Epoch: 4 [10624/21026 (50%)]\tLoss: 0.116165\n",
      "Train Epoch: 4 [10688/21026 (51%)]\tLoss: 0.127204\n",
      "Train Epoch: 4 [10752/21026 (51%)]\tLoss: 0.108093\n",
      "Train Epoch: 4 [10816/21026 (51%)]\tLoss: 0.118074\n",
      "Train Epoch: 4 [10880/21026 (52%)]\tLoss: 0.117825\n",
      "Train Epoch: 4 [10944/21026 (52%)]\tLoss: 0.108888\n",
      "Train Epoch: 4 [11008/21026 (52%)]\tLoss: 0.124689\n",
      "Train Epoch: 4 [11072/21026 (53%)]\tLoss: 0.115576\n",
      "Train Epoch: 4 [11136/21026 (53%)]\tLoss: 0.126730\n",
      "Train Epoch: 4 [11200/21026 (53%)]\tLoss: 0.105537\n",
      "Train Epoch: 4 [11264/21026 (53%)]\tLoss: 0.120638\n",
      "Train Epoch: 4 [11328/21026 (54%)]\tLoss: 0.111588\n",
      "Train Epoch: 4 [11392/21026 (54%)]\tLoss: 0.109414\n",
      "Train Epoch: 4 [11456/21026 (54%)]\tLoss: 0.116797\n",
      "Train Epoch: 4 [11520/21026 (55%)]\tLoss: 0.105584\n",
      "Train Epoch: 4 [11584/21026 (55%)]\tLoss: 0.122045\n",
      "Train Epoch: 4 [11648/21026 (55%)]\tLoss: 0.125889\n",
      "Train Epoch: 4 [11712/21026 (56%)]\tLoss: 0.119677\n",
      "Train Epoch: 4 [11776/21026 (56%)]\tLoss: 0.119822\n",
      "Train Epoch: 4 [11840/21026 (56%)]\tLoss: 0.112420\n",
      "Train Epoch: 4 [11904/21026 (57%)]\tLoss: 0.128526\n",
      "Train Epoch: 4 [11968/21026 (57%)]\tLoss: 0.114147\n",
      "Train Epoch: 4 [12032/21026 (57%)]\tLoss: 0.112530\n",
      "Train Epoch: 4 [12096/21026 (57%)]\tLoss: 0.121131\n",
      "Train Epoch: 4 [12160/21026 (58%)]\tLoss: 0.129643\n",
      "Train Epoch: 4 [12224/21026 (58%)]\tLoss: 0.120944\n",
      "Train Epoch: 4 [12288/21026 (58%)]\tLoss: 0.111392\n",
      "Train Epoch: 4 [12352/21026 (59%)]\tLoss: 0.124224\n",
      "Train Epoch: 4 [12416/21026 (59%)]\tLoss: 0.114956\n",
      "Train Epoch: 4 [12480/21026 (59%)]\tLoss: 0.115026\n",
      "Train Epoch: 4 [12544/21026 (60%)]\tLoss: 0.105236\n",
      "Train Epoch: 4 [12608/21026 (60%)]\tLoss: 0.123520\n",
      "Train Epoch: 4 [12672/21026 (60%)]\tLoss: 0.105704\n",
      "Train Epoch: 4 [12736/21026 (60%)]\tLoss: 0.109013\n",
      "Train Epoch: 4 [12800/21026 (61%)]\tLoss: 0.118346\n",
      "Train Epoch: 4 [12864/21026 (61%)]\tLoss: 0.108995\n",
      "Train Epoch: 4 [12928/21026 (61%)]\tLoss: 0.110806\n",
      "Train Epoch: 4 [12992/21026 (62%)]\tLoss: 0.114066\n",
      "Train Epoch: 4 [13056/21026 (62%)]\tLoss: 0.112807\n",
      "Train Epoch: 4 [13120/21026 (62%)]\tLoss: 0.111061\n",
      "Train Epoch: 4 [13184/21026 (63%)]\tLoss: 0.115795\n",
      "Train Epoch: 4 [13248/21026 (63%)]\tLoss: 0.121590\n",
      "Train Epoch: 4 [13312/21026 (63%)]\tLoss: 0.120761\n",
      "Train Epoch: 4 [13376/21026 (64%)]\tLoss: 0.131369\n",
      "Train Epoch: 4 [13440/21026 (64%)]\tLoss: 0.134810\n",
      "Train Epoch: 4 [13504/21026 (64%)]\tLoss: 0.107496\n",
      "Train Epoch: 4 [13568/21026 (64%)]\tLoss: 0.121267\n",
      "Train Epoch: 4 [13632/21026 (65%)]\tLoss: 0.126994\n",
      "Train Epoch: 4 [13696/21026 (65%)]\tLoss: 0.124892\n",
      "Train Epoch: 4 [13760/21026 (65%)]\tLoss: 0.117179\n",
      "Train Epoch: 4 [13824/21026 (66%)]\tLoss: 0.127651\n",
      "Train Epoch: 4 [13888/21026 (66%)]\tLoss: 0.120633\n",
      "Train Epoch: 4 [13952/21026 (66%)]\tLoss: 0.127922\n",
      "Train Epoch: 4 [14016/21026 (67%)]\tLoss: 0.121101\n",
      "Train Epoch: 4 [14080/21026 (67%)]\tLoss: 0.108410\n",
      "Train Epoch: 4 [14144/21026 (67%)]\tLoss: 0.114200\n",
      "Train Epoch: 4 [14208/21026 (67%)]\tLoss: 0.115812\n",
      "Train Epoch: 4 [14272/21026 (68%)]\tLoss: 0.123962\n",
      "Train Epoch: 4 [14336/21026 (68%)]\tLoss: 0.121751\n",
      "Train Epoch: 4 [14400/21026 (68%)]\tLoss: 0.122013\n",
      "Train Epoch: 4 [14464/21026 (69%)]\tLoss: 0.128084\n",
      "Train Epoch: 4 [14528/21026 (69%)]\tLoss: 0.116939\n",
      "Train Epoch: 4 [14592/21026 (69%)]\tLoss: 0.131767\n",
      "Train Epoch: 4 [14656/21026 (70%)]\tLoss: 0.129096\n",
      "Train Epoch: 4 [14720/21026 (70%)]\tLoss: 0.128435\n",
      "Train Epoch: 4 [14784/21026 (70%)]\tLoss: 0.146911\n",
      "Train Epoch: 4 [14848/21026 (71%)]\tLoss: 0.121010\n",
      "Train Epoch: 4 [14912/21026 (71%)]\tLoss: 0.116723\n",
      "Train Epoch: 4 [14976/21026 (71%)]\tLoss: 0.127802\n",
      "Train Epoch: 4 [15040/21026 (71%)]\tLoss: 0.118915\n",
      "Train Epoch: 4 [15104/21026 (72%)]\tLoss: 0.130598\n",
      "Train Epoch: 4 [15168/21026 (72%)]\tLoss: 0.112833\n",
      "Train Epoch: 4 [15232/21026 (72%)]\tLoss: 0.125811\n",
      "Train Epoch: 4 [15296/21026 (73%)]\tLoss: 0.121340\n",
      "Train Epoch: 4 [15360/21026 (73%)]\tLoss: 0.128002\n",
      "Train Epoch: 4 [15424/21026 (73%)]\tLoss: 0.149006\n",
      "Train Epoch: 4 [15488/21026 (74%)]\tLoss: 0.142436\n",
      "Train Epoch: 4 [15552/21026 (74%)]\tLoss: 0.116660\n",
      "Train Epoch: 4 [15616/21026 (74%)]\tLoss: 0.120631\n",
      "Train Epoch: 4 [15680/21026 (74%)]\tLoss: 0.138680\n",
      "Train Epoch: 4 [15744/21026 (75%)]\tLoss: 0.121653\n",
      "Train Epoch: 4 [15808/21026 (75%)]\tLoss: 0.132959\n",
      "Train Epoch: 4 [15872/21026 (75%)]\tLoss: 0.123989\n",
      "Train Epoch: 4 [15936/21026 (76%)]\tLoss: 0.114902\n",
      "Train Epoch: 4 [16000/21026 (76%)]\tLoss: 0.121238\n",
      "Train Epoch: 4 [16064/21026 (76%)]\tLoss: 0.121612\n",
      "Train Epoch: 4 [16128/21026 (77%)]\tLoss: 0.113983\n",
      "Train Epoch: 4 [16192/21026 (77%)]\tLoss: 0.129372\n",
      "Train Epoch: 4 [16256/21026 (77%)]\tLoss: 0.111468\n",
      "Train Epoch: 4 [16320/21026 (78%)]\tLoss: 0.121319\n",
      "Train Epoch: 4 [16384/21026 (78%)]\tLoss: 0.147098\n",
      "Train Epoch: 4 [16448/21026 (78%)]\tLoss: 0.130039\n",
      "Train Epoch: 4 [16512/21026 (78%)]\tLoss: 0.133369\n",
      "Train Epoch: 4 [16576/21026 (79%)]\tLoss: 0.136315\n",
      "Train Epoch: 4 [16640/21026 (79%)]\tLoss: 0.118167\n",
      "Train Epoch: 4 [16704/21026 (79%)]\tLoss: 0.129633\n",
      "Train Epoch: 4 [16768/21026 (80%)]\tLoss: 0.114448\n",
      "Train Epoch: 4 [16832/21026 (80%)]\tLoss: 0.114360\n",
      "Train Epoch: 4 [16896/21026 (80%)]\tLoss: 0.123390\n",
      "Train Epoch: 4 [16960/21026 (81%)]\tLoss: 0.118946\n",
      "Train Epoch: 4 [17024/21026 (81%)]\tLoss: 0.132964\n",
      "Train Epoch: 4 [17088/21026 (81%)]\tLoss: 0.123087\n",
      "Train Epoch: 4 [17152/21026 (81%)]\tLoss: 0.114960\n",
      "Train Epoch: 4 [17216/21026 (82%)]\tLoss: 0.122289\n",
      "Train Epoch: 4 [17280/21026 (82%)]\tLoss: 0.123318\n",
      "Train Epoch: 4 [17344/21026 (82%)]\tLoss: 0.122625\n",
      "Train Epoch: 4 [17408/21026 (83%)]\tLoss: 0.124653\n",
      "Train Epoch: 4 [17472/21026 (83%)]\tLoss: 0.114832\n",
      "Train Epoch: 4 [17536/21026 (83%)]\tLoss: 0.128276\n",
      "Train Epoch: 4 [17600/21026 (84%)]\tLoss: 0.124177\n",
      "Train Epoch: 4 [17664/21026 (84%)]\tLoss: 0.117797\n",
      "Train Epoch: 4 [17728/21026 (84%)]\tLoss: 0.127050\n",
      "Train Epoch: 4 [17792/21026 (84%)]\tLoss: 0.125348\n",
      "Train Epoch: 4 [17856/21026 (85%)]\tLoss: 0.133547\n",
      "Train Epoch: 4 [17920/21026 (85%)]\tLoss: 0.126099\n",
      "Train Epoch: 4 [17984/21026 (85%)]\tLoss: 0.120949\n",
      "Train Epoch: 4 [18048/21026 (86%)]\tLoss: 0.124784\n",
      "Train Epoch: 4 [18112/21026 (86%)]\tLoss: 0.127389\n",
      "Train Epoch: 4 [18176/21026 (86%)]\tLoss: 0.111910\n",
      "Train Epoch: 4 [18240/21026 (87%)]\tLoss: 0.110477\n",
      "Train Epoch: 4 [18304/21026 (87%)]\tLoss: 0.120023\n",
      "Train Epoch: 4 [18368/21026 (87%)]\tLoss: 0.111127\n",
      "Train Epoch: 4 [18432/21026 (88%)]\tLoss: 0.123124\n",
      "Train Epoch: 4 [18496/21026 (88%)]\tLoss: 0.116331\n",
      "Train Epoch: 4 [18560/21026 (88%)]\tLoss: 0.126080\n",
      "Train Epoch: 4 [18624/21026 (88%)]\tLoss: 0.123386\n",
      "Train Epoch: 4 [18688/21026 (89%)]\tLoss: 0.151964\n",
      "Train Epoch: 4 [18752/21026 (89%)]\tLoss: 0.119168\n",
      "Train Epoch: 4 [18816/21026 (89%)]\tLoss: 0.115778\n",
      "Train Epoch: 4 [18880/21026 (90%)]\tLoss: 0.135850\n",
      "Train Epoch: 4 [18944/21026 (90%)]\tLoss: 0.118933\n",
      "Train Epoch: 4 [19008/21026 (90%)]\tLoss: 0.121294\n",
      "Train Epoch: 4 [19072/21026 (91%)]\tLoss: 0.128380\n",
      "Train Epoch: 4 [19136/21026 (91%)]\tLoss: 0.112464\n",
      "Train Epoch: 4 [19200/21026 (91%)]\tLoss: 0.109469\n",
      "Train Epoch: 4 [19264/21026 (91%)]\tLoss: 0.108314\n",
      "Train Epoch: 4 [19328/21026 (92%)]\tLoss: 0.124303\n",
      "Train Epoch: 4 [19392/21026 (92%)]\tLoss: 0.132781\n",
      "Train Epoch: 4 [19456/21026 (92%)]\tLoss: 0.124896\n",
      "Train Epoch: 4 [19520/21026 (93%)]\tLoss: 0.115984\n",
      "Train Epoch: 4 [19584/21026 (93%)]\tLoss: 0.103976\n",
      "Train Epoch: 4 [19648/21026 (93%)]\tLoss: 0.117000\n",
      "Train Epoch: 4 [19712/21026 (94%)]\tLoss: 0.126926\n",
      "Train Epoch: 4 [19776/21026 (94%)]\tLoss: 0.119450\n",
      "Train Epoch: 4 [19840/21026 (94%)]\tLoss: 0.108662\n",
      "Train Epoch: 4 [19904/21026 (95%)]\tLoss: 0.122164\n",
      "Train Epoch: 4 [19968/21026 (95%)]\tLoss: 0.115493\n",
      "Train Epoch: 4 [20032/21026 (95%)]\tLoss: 0.135195\n",
      "Train Epoch: 4 [20096/21026 (95%)]\tLoss: 0.109805\n",
      "Train Epoch: 4 [20160/21026 (96%)]\tLoss: 0.117079\n",
      "Train Epoch: 4 [20224/21026 (96%)]\tLoss: 0.134602\n",
      "Train Epoch: 4 [20288/21026 (96%)]\tLoss: 0.123124\n",
      "Train Epoch: 4 [20352/21026 (97%)]\tLoss: 0.120545\n",
      "Train Epoch: 4 [20416/21026 (97%)]\tLoss: 0.117259\n",
      "Train Epoch: 4 [20480/21026 (97%)]\tLoss: 0.133474\n",
      "Train Epoch: 4 [20544/21026 (98%)]\tLoss: 0.115155\n",
      "Train Epoch: 4 [20608/21026 (98%)]\tLoss: 0.114318\n",
      "Train Epoch: 4 [20672/21026 (98%)]\tLoss: 0.120350\n",
      "Train Epoch: 4 [20736/21026 (98%)]\tLoss: 0.116806\n",
      "Train Epoch: 4 [20800/21026 (99%)]\tLoss: 0.124419\n",
      "Train Epoch: 4 [20864/21026 (99%)]\tLoss: 0.141230\n",
      "Train Epoch: 4 [20928/21026 (99%)]\tLoss: 0.120100\n",
      "Train Epoch: 4 [11152/21026 (100%)]\tLoss: 0.131250\n",
      "====> Epoch: 4 Average loss: 0.00370286\n",
      "Train Epoch: 5 [0/21026 (0%)]\tLoss: 0.106537\n",
      "Train Epoch: 5 [64/21026 (0%)]\tLoss: 0.097653\n",
      "Train Epoch: 5 [128/21026 (1%)]\tLoss: 0.093667\n",
      "Train Epoch: 5 [192/21026 (1%)]\tLoss: 0.096648\n",
      "Train Epoch: 5 [256/21026 (1%)]\tLoss: 0.088934\n",
      "Train Epoch: 5 [320/21026 (2%)]\tLoss: 0.094671\n",
      "Train Epoch: 5 [384/21026 (2%)]\tLoss: 0.098054\n",
      "Train Epoch: 5 [448/21026 (2%)]\tLoss: 0.103127\n",
      "Train Epoch: 5 [512/21026 (2%)]\tLoss: 0.100966\n",
      "Train Epoch: 5 [576/21026 (3%)]\tLoss: 0.092470\n",
      "Train Epoch: 5 [640/21026 (3%)]\tLoss: 0.085878\n",
      "Train Epoch: 5 [704/21026 (3%)]\tLoss: 0.086049\n",
      "Train Epoch: 5 [768/21026 (4%)]\tLoss: 0.090364\n",
      "Train Epoch: 5 [832/21026 (4%)]\tLoss: 0.091417\n",
      "Train Epoch: 5 [896/21026 (4%)]\tLoss: 0.092612\n",
      "Train Epoch: 5 [960/21026 (5%)]\tLoss: 0.091389\n",
      "Train Epoch: 5 [1024/21026 (5%)]\tLoss: 0.092898\n",
      "Train Epoch: 5 [1088/21026 (5%)]\tLoss: 0.097450\n",
      "Train Epoch: 5 [1152/21026 (5%)]\tLoss: 0.092013\n",
      "Train Epoch: 5 [1216/21026 (6%)]\tLoss: 0.099208\n",
      "Train Epoch: 5 [1280/21026 (6%)]\tLoss: 0.084833\n",
      "Train Epoch: 5 [1344/21026 (6%)]\tLoss: 0.084395\n",
      "Train Epoch: 5 [1408/21026 (7%)]\tLoss: 0.098061\n",
      "Train Epoch: 5 [1472/21026 (7%)]\tLoss: 0.092646\n",
      "Train Epoch: 5 [1536/21026 (7%)]\tLoss: 0.089096\n",
      "Train Epoch: 5 [1600/21026 (8%)]\tLoss: 0.095675\n",
      "Train Epoch: 5 [1664/21026 (8%)]\tLoss: 0.092802\n",
      "Train Epoch: 5 [1728/21026 (8%)]\tLoss: 0.097703\n",
      "Train Epoch: 5 [1792/21026 (9%)]\tLoss: 0.088630\n",
      "Train Epoch: 5 [1856/21026 (9%)]\tLoss: 0.091358\n",
      "Train Epoch: 5 [1920/21026 (9%)]\tLoss: 0.104288\n",
      "Train Epoch: 5 [1984/21026 (9%)]\tLoss: 0.095783\n",
      "Train Epoch: 5 [2048/21026 (10%)]\tLoss: 0.095498\n",
      "Train Epoch: 5 [2112/21026 (10%)]\tLoss: 0.084243\n",
      "Train Epoch: 5 [2176/21026 (10%)]\tLoss: 0.082992\n",
      "Train Epoch: 5 [2240/21026 (11%)]\tLoss: 0.089156\n",
      "Train Epoch: 5 [2304/21026 (11%)]\tLoss: 0.093814\n",
      "Train Epoch: 5 [2368/21026 (11%)]\tLoss: 0.089284\n",
      "Train Epoch: 5 [2432/21026 (12%)]\tLoss: 0.091078\n",
      "Train Epoch: 5 [2496/21026 (12%)]\tLoss: 0.093614\n",
      "Train Epoch: 5 [2560/21026 (12%)]\tLoss: 0.084723\n",
      "Train Epoch: 5 [2624/21026 (12%)]\tLoss: 0.091601\n",
      "Train Epoch: 5 [2688/21026 (13%)]\tLoss: 0.094041\n",
      "Train Epoch: 5 [2752/21026 (13%)]\tLoss: 0.099880\n",
      "Train Epoch: 5 [2816/21026 (13%)]\tLoss: 0.093934\n",
      "Train Epoch: 5 [2880/21026 (14%)]\tLoss: 0.097313\n",
      "Train Epoch: 5 [2944/21026 (14%)]\tLoss: 0.089332\n",
      "Train Epoch: 5 [3008/21026 (14%)]\tLoss: 0.099236\n",
      "Train Epoch: 5 [3072/21026 (15%)]\tLoss: 0.097069\n",
      "Train Epoch: 5 [3136/21026 (15%)]\tLoss: 0.094809\n",
      "Train Epoch: 5 [3200/21026 (15%)]\tLoss: 0.105575\n",
      "Train Epoch: 5 [3264/21026 (16%)]\tLoss: 0.099158\n",
      "Train Epoch: 5 [3328/21026 (16%)]\tLoss: 0.096028\n",
      "Train Epoch: 5 [3392/21026 (16%)]\tLoss: 0.096805\n",
      "Train Epoch: 5 [3456/21026 (16%)]\tLoss: 0.099937\n",
      "Train Epoch: 5 [3520/21026 (17%)]\tLoss: 0.098130\n",
      "Train Epoch: 5 [3584/21026 (17%)]\tLoss: 0.109293\n",
      "Train Epoch: 5 [3648/21026 (17%)]\tLoss: 0.100683\n",
      "Train Epoch: 5 [3712/21026 (18%)]\tLoss: 0.101326\n",
      "Train Epoch: 5 [3776/21026 (18%)]\tLoss: 0.103790\n",
      "Train Epoch: 5 [3840/21026 (18%)]\tLoss: 0.103558\n",
      "Train Epoch: 5 [3904/21026 (19%)]\tLoss: 0.098984\n",
      "Train Epoch: 5 [3968/21026 (19%)]\tLoss: 0.109656\n",
      "Train Epoch: 5 [4032/21026 (19%)]\tLoss: 0.091859\n",
      "Train Epoch: 5 [4096/21026 (19%)]\tLoss: 0.090900\n",
      "Train Epoch: 5 [4160/21026 (20%)]\tLoss: 0.100647\n",
      "Train Epoch: 5 [4224/21026 (20%)]\tLoss: 0.099837\n",
      "Train Epoch: 5 [4288/21026 (20%)]\tLoss: 0.094981\n",
      "Train Epoch: 5 [4352/21026 (21%)]\tLoss: 0.108222\n",
      "Train Epoch: 5 [4416/21026 (21%)]\tLoss: 0.100726\n",
      "Train Epoch: 5 [4480/21026 (21%)]\tLoss: 0.098626\n",
      "Train Epoch: 5 [4544/21026 (22%)]\tLoss: 0.096325\n",
      "Train Epoch: 5 [4608/21026 (22%)]\tLoss: 0.104822\n",
      "Train Epoch: 5 [4672/21026 (22%)]\tLoss: 0.111070\n",
      "Train Epoch: 5 [4736/21026 (22%)]\tLoss: 0.100706\n",
      "Train Epoch: 5 [4800/21026 (23%)]\tLoss: 0.098785\n",
      "Train Epoch: 5 [4864/21026 (23%)]\tLoss: 0.094716\n",
      "Train Epoch: 5 [4928/21026 (23%)]\tLoss: 0.091968\n",
      "Train Epoch: 5 [4992/21026 (24%)]\tLoss: 0.103149\n",
      "Train Epoch: 5 [5056/21026 (24%)]\tLoss: 0.094008\n",
      "Train Epoch: 5 [5120/21026 (24%)]\tLoss: 0.092726\n",
      "Train Epoch: 5 [5184/21026 (25%)]\tLoss: 0.094458\n",
      "Train Epoch: 5 [5248/21026 (25%)]\tLoss: 0.111141\n",
      "Train Epoch: 5 [5312/21026 (25%)]\tLoss: 0.098985\n",
      "Train Epoch: 5 [5376/21026 (26%)]\tLoss: 0.097275\n",
      "Train Epoch: 5 [5440/21026 (26%)]\tLoss: 0.110317\n",
      "Train Epoch: 5 [5504/21026 (26%)]\tLoss: 0.093734\n",
      "Train Epoch: 5 [5568/21026 (26%)]\tLoss: 0.089747\n",
      "Train Epoch: 5 [5632/21026 (27%)]\tLoss: 0.107262\n",
      "Train Epoch: 5 [5696/21026 (27%)]\tLoss: 0.101918\n",
      "Train Epoch: 5 [5760/21026 (27%)]\tLoss: 0.094491\n",
      "Train Epoch: 5 [5824/21026 (28%)]\tLoss: 0.103197\n",
      "Train Epoch: 5 [5888/21026 (28%)]\tLoss: 0.098140\n",
      "Train Epoch: 5 [5952/21026 (28%)]\tLoss: 0.109897\n",
      "Train Epoch: 5 [6016/21026 (29%)]\tLoss: 0.110623\n",
      "Train Epoch: 5 [6080/21026 (29%)]\tLoss: 0.107212\n",
      "Train Epoch: 5 [6144/21026 (29%)]\tLoss: 0.086743\n",
      "Train Epoch: 5 [6208/21026 (29%)]\tLoss: 0.089161\n",
      "Train Epoch: 5 [6272/21026 (30%)]\tLoss: 0.109773\n",
      "Train Epoch: 5 [6336/21026 (30%)]\tLoss: 0.096255\n",
      "Train Epoch: 5 [6400/21026 (30%)]\tLoss: 0.099226\n",
      "Train Epoch: 5 [6464/21026 (31%)]\tLoss: 0.097797\n",
      "Train Epoch: 5 [6528/21026 (31%)]\tLoss: 0.108814\n",
      "Train Epoch: 5 [6592/21026 (31%)]\tLoss: 0.097749\n",
      "Train Epoch: 5 [6656/21026 (32%)]\tLoss: 0.099688\n",
      "Train Epoch: 5 [6720/21026 (32%)]\tLoss: 0.101688\n",
      "Train Epoch: 5 [6784/21026 (32%)]\tLoss: 0.093566\n",
      "Train Epoch: 5 [6848/21026 (33%)]\tLoss: 0.101978\n",
      "Train Epoch: 5 [6912/21026 (33%)]\tLoss: 0.101827\n",
      "Train Epoch: 5 [6976/21026 (33%)]\tLoss: 0.097551\n",
      "Train Epoch: 5 [7040/21026 (33%)]\tLoss: 0.106579\n",
      "Train Epoch: 5 [7104/21026 (34%)]\tLoss: 0.085462\n",
      "Train Epoch: 5 [7168/21026 (34%)]\tLoss: 0.096157\n",
      "Train Epoch: 5 [7232/21026 (34%)]\tLoss: 0.094890\n",
      "Train Epoch: 5 [7296/21026 (35%)]\tLoss: 0.099807\n",
      "Train Epoch: 5 [7360/21026 (35%)]\tLoss: 0.085520\n",
      "Train Epoch: 5 [7424/21026 (35%)]\tLoss: 0.105038\n",
      "Train Epoch: 5 [7488/21026 (36%)]\tLoss: 0.104317\n",
      "Train Epoch: 5 [7552/21026 (36%)]\tLoss: 0.101271\n",
      "Train Epoch: 5 [7616/21026 (36%)]\tLoss: 0.099725\n",
      "Train Epoch: 5 [7680/21026 (36%)]\tLoss: 0.108919\n",
      "Train Epoch: 5 [7744/21026 (37%)]\tLoss: 0.102889\n",
      "Train Epoch: 5 [7808/21026 (37%)]\tLoss: 0.098803\n",
      "Train Epoch: 5 [7872/21026 (37%)]\tLoss: 0.093227\n",
      "Train Epoch: 5 [7936/21026 (38%)]\tLoss: 0.097287\n",
      "Train Epoch: 5 [8000/21026 (38%)]\tLoss: 0.090537\n",
      "Train Epoch: 5 [8064/21026 (38%)]\tLoss: 0.100645\n",
      "Train Epoch: 5 [8128/21026 (39%)]\tLoss: 0.093250\n",
      "Train Epoch: 5 [8192/21026 (39%)]\tLoss: 0.094036\n",
      "Train Epoch: 5 [8256/21026 (39%)]\tLoss: 0.097945\n",
      "Train Epoch: 5 [8320/21026 (40%)]\tLoss: 0.097130\n",
      "Train Epoch: 5 [8384/21026 (40%)]\tLoss: 0.095297\n",
      "Train Epoch: 5 [8448/21026 (40%)]\tLoss: 0.086847\n",
      "Train Epoch: 5 [8512/21026 (40%)]\tLoss: 0.089856\n",
      "Train Epoch: 5 [8576/21026 (41%)]\tLoss: 0.102811\n",
      "Train Epoch: 5 [8640/21026 (41%)]\tLoss: 0.082233\n",
      "Train Epoch: 5 [8704/21026 (41%)]\tLoss: 0.099724\n",
      "Train Epoch: 5 [8768/21026 (42%)]\tLoss: 0.102529\n",
      "Train Epoch: 5 [8832/21026 (42%)]\tLoss: 0.086106\n",
      "Train Epoch: 5 [8896/21026 (42%)]\tLoss: 0.091992\n",
      "Train Epoch: 5 [8960/21026 (43%)]\tLoss: 0.093044\n",
      "Train Epoch: 5 [9024/21026 (43%)]\tLoss: 0.109954\n",
      "Train Epoch: 5 [9088/21026 (43%)]\tLoss: 0.091774\n",
      "Train Epoch: 5 [9152/21026 (43%)]\tLoss: 0.093166\n",
      "Train Epoch: 5 [9216/21026 (44%)]\tLoss: 0.097221\n",
      "Train Epoch: 5 [9280/21026 (44%)]\tLoss: 0.092161\n",
      "Train Epoch: 5 [9344/21026 (44%)]\tLoss: 0.094644\n",
      "Train Epoch: 5 [9408/21026 (45%)]\tLoss: 0.091005\n",
      "Train Epoch: 5 [9472/21026 (45%)]\tLoss: 0.093260\n",
      "Train Epoch: 5 [9536/21026 (45%)]\tLoss: 0.099944\n",
      "Train Epoch: 5 [9600/21026 (46%)]\tLoss: 0.107247\n",
      "Train Epoch: 5 [9664/21026 (46%)]\tLoss: 0.096306\n",
      "Train Epoch: 5 [9728/21026 (46%)]\tLoss: 0.095676\n",
      "Train Epoch: 5 [9792/21026 (47%)]\tLoss: 0.100719\n",
      "Train Epoch: 5 [9856/21026 (47%)]\tLoss: 0.105631\n",
      "Train Epoch: 5 [9920/21026 (47%)]\tLoss: 0.100649\n",
      "Train Epoch: 5 [9984/21026 (47%)]\tLoss: 0.091440\n",
      "Train Epoch: 5 [10048/21026 (48%)]\tLoss: 0.107021\n",
      "Train Epoch: 5 [10112/21026 (48%)]\tLoss: 0.092500\n",
      "Train Epoch: 5 [10176/21026 (48%)]\tLoss: 0.109233\n",
      "Train Epoch: 5 [10240/21026 (49%)]\tLoss: 0.099262\n",
      "Train Epoch: 5 [10304/21026 (49%)]\tLoss: 0.092686\n",
      "Train Epoch: 5 [10368/21026 (49%)]\tLoss: 0.098460\n",
      "Train Epoch: 5 [10432/21026 (50%)]\tLoss: 0.117362\n",
      "Train Epoch: 5 [10496/21026 (50%)]\tLoss: 0.102861\n",
      "Train Epoch: 5 [10560/21026 (50%)]\tLoss: 0.089715\n",
      "Train Epoch: 5 [10624/21026 (50%)]\tLoss: 0.101200\n",
      "Train Epoch: 5 [10688/21026 (51%)]\tLoss: 0.098311\n",
      "Train Epoch: 5 [10752/21026 (51%)]\tLoss: 0.093568\n",
      "Train Epoch: 5 [10816/21026 (51%)]\tLoss: 0.099017\n",
      "Train Epoch: 5 [10880/21026 (52%)]\tLoss: 0.095493\n",
      "Train Epoch: 5 [10944/21026 (52%)]\tLoss: 0.106618\n",
      "Train Epoch: 5 [11008/21026 (52%)]\tLoss: 0.104293\n",
      "Train Epoch: 5 [11072/21026 (53%)]\tLoss: 0.092239\n",
      "Train Epoch: 5 [11136/21026 (53%)]\tLoss: 0.110891\n",
      "Train Epoch: 5 [11200/21026 (53%)]\tLoss: 0.104102\n",
      "Train Epoch: 5 [11264/21026 (53%)]\tLoss: 0.093043\n",
      "Train Epoch: 5 [11328/21026 (54%)]\tLoss: 0.097843\n",
      "Train Epoch: 5 [11392/21026 (54%)]\tLoss: 0.104707\n",
      "Train Epoch: 5 [11456/21026 (54%)]\tLoss: 0.109759\n",
      "Train Epoch: 5 [11520/21026 (55%)]\tLoss: 0.095387\n",
      "Train Epoch: 5 [11584/21026 (55%)]\tLoss: 0.104211\n",
      "Train Epoch: 5 [11648/21026 (55%)]\tLoss: 0.096940\n",
      "Train Epoch: 5 [11712/21026 (56%)]\tLoss: 0.105836\n",
      "Train Epoch: 5 [11776/21026 (56%)]\tLoss: 0.104087\n",
      "Train Epoch: 5 [11840/21026 (56%)]\tLoss: 0.085551\n",
      "Train Epoch: 5 [11904/21026 (57%)]\tLoss: 0.100757\n",
      "Train Epoch: 5 [11968/21026 (57%)]\tLoss: 0.100619\n",
      "Train Epoch: 5 [12032/21026 (57%)]\tLoss: 0.098066\n",
      "Train Epoch: 5 [12096/21026 (57%)]\tLoss: 0.095151\n",
      "Train Epoch: 5 [12160/21026 (58%)]\tLoss: 0.093924\n",
      "Train Epoch: 5 [12224/21026 (58%)]\tLoss: 0.093523\n",
      "Train Epoch: 5 [12288/21026 (58%)]\tLoss: 0.091509\n",
      "Train Epoch: 5 [12352/21026 (59%)]\tLoss: 0.094216\n",
      "Train Epoch: 5 [12416/21026 (59%)]\tLoss: 0.108036\n",
      "Train Epoch: 5 [12480/21026 (59%)]\tLoss: 0.118731\n",
      "Train Epoch: 5 [12544/21026 (60%)]\tLoss: 0.104813\n",
      "Train Epoch: 5 [12608/21026 (60%)]\tLoss: 0.096108\n",
      "Train Epoch: 5 [12672/21026 (60%)]\tLoss: 0.108780\n",
      "Train Epoch: 5 [12736/21026 (60%)]\tLoss: 0.102299\n",
      "Train Epoch: 5 [12800/21026 (61%)]\tLoss: 0.107499\n",
      "Train Epoch: 5 [12864/21026 (61%)]\tLoss: 0.108163\n",
      "Train Epoch: 5 [12928/21026 (61%)]\tLoss: 0.096043\n",
      "Train Epoch: 5 [12992/21026 (62%)]\tLoss: 0.104060\n",
      "Train Epoch: 5 [13056/21026 (62%)]\tLoss: 0.102434\n",
      "Train Epoch: 5 [13120/21026 (62%)]\tLoss: 0.107438\n",
      "Train Epoch: 5 [13184/21026 (63%)]\tLoss: 0.090409\n",
      "Train Epoch: 5 [13248/21026 (63%)]\tLoss: 0.100542\n",
      "Train Epoch: 5 [13312/21026 (63%)]\tLoss: 0.087348\n",
      "Train Epoch: 5 [13376/21026 (64%)]\tLoss: 0.095219\n",
      "Train Epoch: 5 [13440/21026 (64%)]\tLoss: 0.106707\n",
      "Train Epoch: 5 [13504/21026 (64%)]\tLoss: 0.112072\n",
      "Train Epoch: 5 [13568/21026 (64%)]\tLoss: 0.096802\n",
      "Train Epoch: 5 [13632/21026 (65%)]\tLoss: 0.101003\n",
      "Train Epoch: 5 [13696/21026 (65%)]\tLoss: 0.093150\n",
      "Train Epoch: 5 [13760/21026 (65%)]\tLoss: 0.100861\n",
      "Train Epoch: 5 [13824/21026 (66%)]\tLoss: 0.100310\n",
      "Train Epoch: 5 [13888/21026 (66%)]\tLoss: 0.108025\n",
      "Train Epoch: 5 [13952/21026 (66%)]\tLoss: 0.096397\n",
      "Train Epoch: 5 [14016/21026 (67%)]\tLoss: 0.095093\n",
      "Train Epoch: 5 [14080/21026 (67%)]\tLoss: 0.101303\n",
      "Train Epoch: 5 [14144/21026 (67%)]\tLoss: 0.115349\n",
      "Train Epoch: 5 [14208/21026 (67%)]\tLoss: 0.107423\n",
      "Train Epoch: 5 [14272/21026 (68%)]\tLoss: 0.085279\n",
      "Train Epoch: 5 [14336/21026 (68%)]\tLoss: 0.098911\n",
      "Train Epoch: 5 [14400/21026 (68%)]\tLoss: 0.094675\n",
      "Train Epoch: 5 [14464/21026 (69%)]\tLoss: 0.091531\n",
      "Train Epoch: 5 [14528/21026 (69%)]\tLoss: 0.098081\n",
      "Train Epoch: 5 [14592/21026 (69%)]\tLoss: 0.100389\n",
      "Train Epoch: 5 [14656/21026 (70%)]\tLoss: 0.092583\n",
      "Train Epoch: 5 [14720/21026 (70%)]\tLoss: 0.095944\n",
      "Train Epoch: 5 [14784/21026 (70%)]\tLoss: 0.091923\n",
      "Train Epoch: 5 [14848/21026 (71%)]\tLoss: 0.091203\n",
      "Train Epoch: 5 [14912/21026 (71%)]\tLoss: 0.095947\n",
      "Train Epoch: 5 [14976/21026 (71%)]\tLoss: 0.099397\n",
      "Train Epoch: 5 [15040/21026 (71%)]\tLoss: 0.100823\n",
      "Train Epoch: 5 [15104/21026 (72%)]\tLoss: 0.115415\n",
      "Train Epoch: 5 [15168/21026 (72%)]\tLoss: 0.103797\n",
      "Train Epoch: 5 [15232/21026 (72%)]\tLoss: 0.087768\n",
      "Train Epoch: 5 [15296/21026 (73%)]\tLoss: 0.090774\n",
      "Train Epoch: 5 [15360/21026 (73%)]\tLoss: 0.093612\n",
      "Train Epoch: 5 [15424/21026 (73%)]\tLoss: 0.097936\n",
      "Train Epoch: 5 [15488/21026 (74%)]\tLoss: 0.106422\n",
      "Train Epoch: 5 [15552/21026 (74%)]\tLoss: 0.107502\n",
      "Train Epoch: 5 [15616/21026 (74%)]\tLoss: 0.095786\n",
      "Train Epoch: 5 [15680/21026 (74%)]\tLoss: 0.097785\n",
      "Train Epoch: 5 [15744/21026 (75%)]\tLoss: 0.099038\n",
      "Train Epoch: 5 [15808/21026 (75%)]\tLoss: 0.098351\n",
      "Train Epoch: 5 [15872/21026 (75%)]\tLoss: 0.108685\n",
      "Train Epoch: 5 [15936/21026 (76%)]\tLoss: 0.093372\n",
      "Train Epoch: 5 [16000/21026 (76%)]\tLoss: 0.103138\n",
      "Train Epoch: 5 [16064/21026 (76%)]\tLoss: 0.111823\n",
      "Train Epoch: 5 [16128/21026 (77%)]\tLoss: 0.102337\n",
      "Train Epoch: 5 [16192/21026 (77%)]\tLoss: 0.114737\n",
      "Train Epoch: 5 [16256/21026 (77%)]\tLoss: 0.105436\n",
      "Train Epoch: 5 [16320/21026 (78%)]\tLoss: 0.096296\n",
      "Train Epoch: 5 [16384/21026 (78%)]\tLoss: 0.098158\n",
      "Train Epoch: 5 [16448/21026 (78%)]\tLoss: 0.110158\n",
      "Train Epoch: 5 [16512/21026 (78%)]\tLoss: 0.104879\n",
      "Train Epoch: 5 [16576/21026 (79%)]\tLoss: 0.108074\n",
      "Train Epoch: 5 [16640/21026 (79%)]\tLoss: 0.100759\n",
      "Train Epoch: 5 [16704/21026 (79%)]\tLoss: 0.109423\n",
      "Train Epoch: 5 [16768/21026 (80%)]\tLoss: 0.098885\n",
      "Train Epoch: 5 [16832/21026 (80%)]\tLoss: 0.111766\n",
      "Train Epoch: 5 [16896/21026 (80%)]\tLoss: 0.101108\n",
      "Train Epoch: 5 [16960/21026 (81%)]\tLoss: 0.095900\n",
      "Train Epoch: 5 [17024/21026 (81%)]\tLoss: 0.103912\n",
      "Train Epoch: 5 [17088/21026 (81%)]\tLoss: 0.101213\n",
      "Train Epoch: 5 [17152/21026 (81%)]\tLoss: 0.116365\n",
      "Train Epoch: 5 [17216/21026 (82%)]\tLoss: 0.117477\n",
      "Train Epoch: 5 [17280/21026 (82%)]\tLoss: 0.108962\n",
      "Train Epoch: 5 [17344/21026 (82%)]\tLoss: 0.110042\n",
      "Train Epoch: 5 [17408/21026 (83%)]\tLoss: 0.121902\n",
      "Train Epoch: 5 [17472/21026 (83%)]\tLoss: 0.102113\n",
      "Train Epoch: 5 [17536/21026 (83%)]\tLoss: 0.109351\n",
      "Train Epoch: 5 [17600/21026 (84%)]\tLoss: 0.110331\n",
      "Train Epoch: 5 [17664/21026 (84%)]\tLoss: 0.120009\n",
      "Train Epoch: 5 [17728/21026 (84%)]\tLoss: 0.114087\n",
      "Train Epoch: 5 [17792/21026 (84%)]\tLoss: 0.105017\n",
      "Train Epoch: 5 [17856/21026 (85%)]\tLoss: 0.107927\n",
      "Train Epoch: 5 [17920/21026 (85%)]\tLoss: 0.103676\n",
      "Train Epoch: 5 [17984/21026 (85%)]\tLoss: 0.104268\n",
      "Train Epoch: 5 [18048/21026 (86%)]\tLoss: 0.107640\n",
      "Train Epoch: 5 [18112/21026 (86%)]\tLoss: 0.115514\n",
      "Train Epoch: 5 [18176/21026 (86%)]\tLoss: 0.107309\n",
      "Train Epoch: 5 [18240/21026 (87%)]\tLoss: 0.114703\n",
      "Train Epoch: 5 [18304/21026 (87%)]\tLoss: 0.107324\n",
      "Train Epoch: 5 [18368/21026 (87%)]\tLoss: 0.105832\n",
      "Train Epoch: 5 [18432/21026 (88%)]\tLoss: 0.110739\n",
      "Train Epoch: 5 [18496/21026 (88%)]\tLoss: 0.097642\n",
      "Train Epoch: 5 [18560/21026 (88%)]\tLoss: 0.102686\n",
      "Train Epoch: 5 [18624/21026 (88%)]\tLoss: 0.105437\n",
      "Train Epoch: 5 [18688/21026 (89%)]\tLoss: 0.114704\n",
      "Train Epoch: 5 [18752/21026 (89%)]\tLoss: 0.092194\n",
      "Train Epoch: 5 [18816/21026 (89%)]\tLoss: 0.096457\n",
      "Train Epoch: 5 [18880/21026 (90%)]\tLoss: 0.101202\n",
      "Train Epoch: 5 [18944/21026 (90%)]\tLoss: 0.110986\n",
      "Train Epoch: 5 [19008/21026 (90%)]\tLoss: 0.101173\n",
      "Train Epoch: 5 [19072/21026 (91%)]\tLoss: 0.099212\n",
      "Train Epoch: 5 [19136/21026 (91%)]\tLoss: 0.104848\n",
      "Train Epoch: 5 [19200/21026 (91%)]\tLoss: 0.098817\n",
      "Train Epoch: 5 [19264/21026 (91%)]\tLoss: 0.100033\n",
      "Train Epoch: 5 [19328/21026 (92%)]\tLoss: 0.107675\n",
      "Train Epoch: 5 [19392/21026 (92%)]\tLoss: 0.112514\n",
      "Train Epoch: 5 [19456/21026 (92%)]\tLoss: 0.100856\n",
      "Train Epoch: 5 [19520/21026 (93%)]\tLoss: 0.112821\n",
      "Train Epoch: 5 [19584/21026 (93%)]\tLoss: 0.099770\n",
      "Train Epoch: 5 [19648/21026 (93%)]\tLoss: 0.112378\n",
      "Train Epoch: 5 [19712/21026 (94%)]\tLoss: 0.101553\n",
      "Train Epoch: 5 [19776/21026 (94%)]\tLoss: 0.116934\n",
      "Train Epoch: 5 [19840/21026 (94%)]\tLoss: 0.110502\n",
      "Train Epoch: 5 [19904/21026 (95%)]\tLoss: 0.105803\n",
      "Train Epoch: 5 [19968/21026 (95%)]\tLoss: 0.094516\n",
      "Train Epoch: 5 [20032/21026 (95%)]\tLoss: 0.112172\n",
      "Train Epoch: 5 [20096/21026 (95%)]\tLoss: 0.101376\n",
      "Train Epoch: 5 [20160/21026 (96%)]\tLoss: 0.120615\n",
      "Train Epoch: 5 [20224/21026 (96%)]\tLoss: 0.120201\n",
      "Train Epoch: 5 [20288/21026 (96%)]\tLoss: 0.106962\n",
      "Train Epoch: 5 [20352/21026 (97%)]\tLoss: 0.100873\n",
      "Train Epoch: 5 [20416/21026 (97%)]\tLoss: 0.104125\n",
      "Train Epoch: 5 [20480/21026 (97%)]\tLoss: 0.104368\n",
      "Train Epoch: 5 [20544/21026 (98%)]\tLoss: 0.103052\n",
      "Train Epoch: 5 [20608/21026 (98%)]\tLoss: 0.103738\n",
      "Train Epoch: 5 [20672/21026 (98%)]\tLoss: 0.103472\n",
      "Train Epoch: 5 [20736/21026 (98%)]\tLoss: 0.096886\n",
      "Train Epoch: 5 [20800/21026 (99%)]\tLoss: 0.114035\n",
      "Train Epoch: 5 [20864/21026 (99%)]\tLoss: 0.108158\n",
      "Train Epoch: 5 [20928/21026 (99%)]\tLoss: 0.103876\n",
      "Train Epoch: 5 [11152/21026 (100%)]\tLoss: 0.115206\n",
      "====> Epoch: 5 Average loss: 0.00313179\n",
      "Train Epoch: 6 [0/21026 (0%)]\tLoss: 0.088408\n",
      "Train Epoch: 6 [64/21026 (0%)]\tLoss: 0.094608\n",
      "Train Epoch: 6 [128/21026 (1%)]\tLoss: 0.096733\n",
      "Train Epoch: 6 [192/21026 (1%)]\tLoss: 0.088014\n",
      "Train Epoch: 6 [256/21026 (1%)]\tLoss: 0.086442\n",
      "Train Epoch: 6 [320/21026 (2%)]\tLoss: 0.096804\n",
      "Train Epoch: 6 [384/21026 (2%)]\tLoss: 0.084926\n",
      "Train Epoch: 6 [448/21026 (2%)]\tLoss: 0.081727\n",
      "Train Epoch: 6 [512/21026 (2%)]\tLoss: 0.090233\n",
      "Train Epoch: 6 [576/21026 (3%)]\tLoss: 0.076704\n",
      "Train Epoch: 6 [640/21026 (3%)]\tLoss: 0.079813\n",
      "Train Epoch: 6 [704/21026 (3%)]\tLoss: 0.100038\n",
      "Train Epoch: 6 [768/21026 (4%)]\tLoss: 0.098546\n",
      "Train Epoch: 6 [832/21026 (4%)]\tLoss: 0.090182\n",
      "Train Epoch: 6 [896/21026 (4%)]\tLoss: 0.103155\n",
      "Train Epoch: 6 [960/21026 (5%)]\tLoss: 0.086186\n",
      "Train Epoch: 6 [1024/21026 (5%)]\tLoss: 0.088278\n",
      "Train Epoch: 6 [1088/21026 (5%)]\tLoss: 0.096040\n",
      "Train Epoch: 6 [1152/21026 (5%)]\tLoss: 0.107079\n",
      "Train Epoch: 6 [1216/21026 (6%)]\tLoss: 0.090456\n",
      "Train Epoch: 6 [1280/21026 (6%)]\tLoss: 0.094194\n",
      "Train Epoch: 6 [1344/21026 (6%)]\tLoss: 0.095370\n",
      "Train Epoch: 6 [1408/21026 (7%)]\tLoss: 0.083136\n",
      "Train Epoch: 6 [1472/21026 (7%)]\tLoss: 0.083329\n",
      "Train Epoch: 6 [1536/21026 (7%)]\tLoss: 0.091361\n",
      "Train Epoch: 6 [1600/21026 (8%)]\tLoss: 0.095137\n",
      "Train Epoch: 6 [1664/21026 (8%)]\tLoss: 0.084622\n",
      "Train Epoch: 6 [1728/21026 (8%)]\tLoss: 0.097880\n",
      "Train Epoch: 6 [1792/21026 (9%)]\tLoss: 0.089659\n",
      "Train Epoch: 6 [1856/21026 (9%)]\tLoss: 0.085626\n",
      "Train Epoch: 6 [1920/21026 (9%)]\tLoss: 0.091574\n",
      "Train Epoch: 6 [1984/21026 (9%)]\tLoss: 0.084146\n",
      "Train Epoch: 6 [2048/21026 (10%)]\tLoss: 0.078323\n",
      "Train Epoch: 6 [2112/21026 (10%)]\tLoss: 0.088130\n",
      "Train Epoch: 6 [2176/21026 (10%)]\tLoss: 0.082481\n",
      "Train Epoch: 6 [2240/21026 (11%)]\tLoss: 0.076562\n",
      "Train Epoch: 6 [2304/21026 (11%)]\tLoss: 0.084890\n",
      "Train Epoch: 6 [2368/21026 (11%)]\tLoss: 0.087638\n",
      "Train Epoch: 6 [2432/21026 (12%)]\tLoss: 0.084942\n",
      "Train Epoch: 6 [2496/21026 (12%)]\tLoss: 0.084554\n",
      "Train Epoch: 6 [2560/21026 (12%)]\tLoss: 0.081518\n",
      "Train Epoch: 6 [2624/21026 (12%)]\tLoss: 0.077445\n",
      "Train Epoch: 6 [2688/21026 (13%)]\tLoss: 0.088957\n",
      "Train Epoch: 6 [2752/21026 (13%)]\tLoss: 0.085923\n",
      "Train Epoch: 6 [2816/21026 (13%)]\tLoss: 0.080606\n",
      "Train Epoch: 6 [2880/21026 (14%)]\tLoss: 0.075523\n",
      "Train Epoch: 6 [2944/21026 (14%)]\tLoss: 0.088529\n",
      "Train Epoch: 6 [3008/21026 (14%)]\tLoss: 0.095087\n",
      "Train Epoch: 6 [3072/21026 (15%)]\tLoss: 0.085280\n",
      "Train Epoch: 6 [3136/21026 (15%)]\tLoss: 0.080979\n",
      "Train Epoch: 6 [3200/21026 (15%)]\tLoss: 0.090042\n",
      "Train Epoch: 6 [3264/21026 (16%)]\tLoss: 0.086941\n",
      "Train Epoch: 6 [3328/21026 (16%)]\tLoss: 0.084710\n",
      "Train Epoch: 6 [3392/21026 (16%)]\tLoss: 0.086325\n",
      "Train Epoch: 6 [3456/21026 (16%)]\tLoss: 0.083322\n",
      "Train Epoch: 6 [3520/21026 (17%)]\tLoss: 0.070157\n",
      "Train Epoch: 6 [3584/21026 (17%)]\tLoss: 0.083409\n",
      "Train Epoch: 6 [3648/21026 (17%)]\tLoss: 0.084635\n",
      "Train Epoch: 6 [3712/21026 (18%)]\tLoss: 0.089276\n",
      "Train Epoch: 6 [3776/21026 (18%)]\tLoss: 0.088362\n",
      "Train Epoch: 6 [3840/21026 (18%)]\tLoss: 0.082183\n",
      "Train Epoch: 6 [3904/21026 (19%)]\tLoss: 0.092175\n",
      "Train Epoch: 6 [3968/21026 (19%)]\tLoss: 0.085784\n",
      "Train Epoch: 6 [4032/21026 (19%)]\tLoss: 0.086344\n",
      "Train Epoch: 6 [4096/21026 (19%)]\tLoss: 0.082946\n",
      "Train Epoch: 6 [4160/21026 (20%)]\tLoss: 0.093923\n",
      "Train Epoch: 6 [4224/21026 (20%)]\tLoss: 0.089016\n",
      "Train Epoch: 6 [4288/21026 (20%)]\tLoss: 0.081361\n",
      "Train Epoch: 6 [4352/21026 (21%)]\tLoss: 0.080029\n",
      "Train Epoch: 6 [4416/21026 (21%)]\tLoss: 0.089845\n",
      "Train Epoch: 6 [4480/21026 (21%)]\tLoss: 0.087669\n",
      "Train Epoch: 6 [4544/21026 (22%)]\tLoss: 0.088107\n",
      "Train Epoch: 6 [4608/21026 (22%)]\tLoss: 0.089354\n",
      "Train Epoch: 6 [4672/21026 (22%)]\tLoss: 0.079828\n",
      "Train Epoch: 6 [4736/21026 (22%)]\tLoss: 0.089566\n",
      "Train Epoch: 6 [4800/21026 (23%)]\tLoss: 0.086823\n",
      "Train Epoch: 6 [4864/21026 (23%)]\tLoss: 0.093586\n",
      "Train Epoch: 6 [4928/21026 (23%)]\tLoss: 0.085066\n",
      "Train Epoch: 6 [4992/21026 (24%)]\tLoss: 0.091234\n",
      "Train Epoch: 6 [5056/21026 (24%)]\tLoss: 0.085248\n",
      "Train Epoch: 6 [5120/21026 (24%)]\tLoss: 0.090774\n",
      "Train Epoch: 6 [5184/21026 (25%)]\tLoss: 0.089677\n",
      "Train Epoch: 6 [5248/21026 (25%)]\tLoss: 0.079843\n",
      "Train Epoch: 6 [5312/21026 (25%)]\tLoss: 0.097915\n",
      "Train Epoch: 6 [5376/21026 (26%)]\tLoss: 0.081470\n",
      "Train Epoch: 6 [5440/21026 (26%)]\tLoss: 0.088341\n",
      "Train Epoch: 6 [5504/21026 (26%)]\tLoss: 0.084322\n",
      "Train Epoch: 6 [5568/21026 (26%)]\tLoss: 0.090805\n",
      "Train Epoch: 6 [5632/21026 (27%)]\tLoss: 0.078498\n",
      "Train Epoch: 6 [5696/21026 (27%)]\tLoss: 0.079077\n",
      "Train Epoch: 6 [5760/21026 (27%)]\tLoss: 0.089788\n",
      "Train Epoch: 6 [5824/21026 (28%)]\tLoss: 0.087194\n",
      "Train Epoch: 6 [5888/21026 (28%)]\tLoss: 0.082188\n",
      "Train Epoch: 6 [5952/21026 (28%)]\tLoss: 0.082075\n",
      "Train Epoch: 6 [6016/21026 (29%)]\tLoss: 0.083758\n",
      "Train Epoch: 6 [6080/21026 (29%)]\tLoss: 0.083405\n",
      "Train Epoch: 6 [6144/21026 (29%)]\tLoss: 0.088948\n",
      "Train Epoch: 6 [6208/21026 (29%)]\tLoss: 0.082904\n",
      "Train Epoch: 6 [6272/21026 (30%)]\tLoss: 0.099091\n",
      "Train Epoch: 6 [6336/21026 (30%)]\tLoss: 0.078895\n",
      "Train Epoch: 6 [6400/21026 (30%)]\tLoss: 0.092002\n",
      "Train Epoch: 6 [6464/21026 (31%)]\tLoss: 0.091312\n",
      "Train Epoch: 6 [6528/21026 (31%)]\tLoss: 0.087120\n",
      "Train Epoch: 6 [6592/21026 (31%)]\tLoss: 0.094686\n",
      "Train Epoch: 6 [6656/21026 (32%)]\tLoss: 0.080786\n",
      "Train Epoch: 6 [6720/21026 (32%)]\tLoss: 0.085859\n",
      "Train Epoch: 6 [6784/21026 (32%)]\tLoss: 0.089704\n",
      "Train Epoch: 6 [6848/21026 (33%)]\tLoss: 0.086061\n",
      "Train Epoch: 6 [6912/21026 (33%)]\tLoss: 0.091487\n",
      "Train Epoch: 6 [6976/21026 (33%)]\tLoss: 0.090950\n",
      "Train Epoch: 6 [7040/21026 (33%)]\tLoss: 0.092760\n",
      "Train Epoch: 6 [7104/21026 (34%)]\tLoss: 0.091598\n",
      "Train Epoch: 6 [7168/21026 (34%)]\tLoss: 0.082674\n",
      "Train Epoch: 6 [7232/21026 (34%)]\tLoss: 0.084069\n",
      "Train Epoch: 6 [7296/21026 (35%)]\tLoss: 0.083572\n",
      "Train Epoch: 6 [7360/21026 (35%)]\tLoss: 0.084130\n",
      "Train Epoch: 6 [7424/21026 (35%)]\tLoss: 0.092692\n",
      "Train Epoch: 6 [7488/21026 (36%)]\tLoss: 0.089224\n",
      "Train Epoch: 6 [7552/21026 (36%)]\tLoss: 0.086932\n",
      "Train Epoch: 6 [7616/21026 (36%)]\tLoss: 0.094745\n",
      "Train Epoch: 6 [7680/21026 (36%)]\tLoss: 0.095743\n",
      "Train Epoch: 6 [7744/21026 (37%)]\tLoss: 0.086516\n",
      "Train Epoch: 6 [7808/21026 (37%)]\tLoss: 0.081305\n",
      "Train Epoch: 6 [7872/21026 (37%)]\tLoss: 0.077387\n",
      "Train Epoch: 6 [7936/21026 (38%)]\tLoss: 0.084710\n",
      "Train Epoch: 6 [8000/21026 (38%)]\tLoss: 0.082171\n",
      "Train Epoch: 6 [8064/21026 (38%)]\tLoss: 0.086873\n",
      "Train Epoch: 6 [8128/21026 (39%)]\tLoss: 0.078160\n",
      "Train Epoch: 6 [8192/21026 (39%)]\tLoss: 0.080556\n",
      "Train Epoch: 6 [8256/21026 (39%)]\tLoss: 0.076332\n",
      "Train Epoch: 6 [8320/21026 (40%)]\tLoss: 0.079218\n",
      "Train Epoch: 6 [8384/21026 (40%)]\tLoss: 0.075693\n",
      "Train Epoch: 6 [8448/21026 (40%)]\tLoss: 0.079989\n",
      "Train Epoch: 6 [8512/21026 (40%)]\tLoss: 0.081909\n",
      "Train Epoch: 6 [8576/21026 (41%)]\tLoss: 0.085351\n",
      "Train Epoch: 6 [8640/21026 (41%)]\tLoss: 0.091228\n",
      "Train Epoch: 6 [8704/21026 (41%)]\tLoss: 0.086070\n",
      "Train Epoch: 6 [8768/21026 (42%)]\tLoss: 0.085400\n",
      "Train Epoch: 6 [8832/21026 (42%)]\tLoss: 0.090667\n",
      "Train Epoch: 6 [8896/21026 (42%)]\tLoss: 0.090756\n",
      "Train Epoch: 6 [8960/21026 (43%)]\tLoss: 0.087722\n",
      "Train Epoch: 6 [9024/21026 (43%)]\tLoss: 0.103332\n",
      "Train Epoch: 6 [9088/21026 (43%)]\tLoss: 0.075763\n",
      "Train Epoch: 6 [9152/21026 (43%)]\tLoss: 0.092413\n",
      "Train Epoch: 6 [9216/21026 (44%)]\tLoss: 0.081064\n",
      "Train Epoch: 6 [9280/21026 (44%)]\tLoss: 0.093448\n",
      "Train Epoch: 6 [9344/21026 (44%)]\tLoss: 0.086605\n",
      "Train Epoch: 6 [9408/21026 (45%)]\tLoss: 0.089555\n",
      "Train Epoch: 6 [9472/21026 (45%)]\tLoss: 0.084101\n",
      "Train Epoch: 6 [9536/21026 (45%)]\tLoss: 0.091439\n",
      "Train Epoch: 6 [9600/21026 (46%)]\tLoss: 0.094927\n",
      "Train Epoch: 6 [9664/21026 (46%)]\tLoss: 0.090110\n",
      "Train Epoch: 6 [9728/21026 (46%)]\tLoss: 0.086630\n",
      "Train Epoch: 6 [9792/21026 (47%)]\tLoss: 0.086593\n",
      "Train Epoch: 6 [9856/21026 (47%)]\tLoss: 0.085174\n",
      "Train Epoch: 6 [9920/21026 (47%)]\tLoss: 0.087107\n",
      "Train Epoch: 6 [9984/21026 (47%)]\tLoss: 0.092665\n",
      "Train Epoch: 6 [10048/21026 (48%)]\tLoss: 0.087038\n",
      "Train Epoch: 6 [10112/21026 (48%)]\tLoss: 0.089186\n",
      "Train Epoch: 6 [10176/21026 (48%)]\tLoss: 0.089610\n",
      "Train Epoch: 6 [10240/21026 (49%)]\tLoss: 0.087643\n",
      "Train Epoch: 6 [10304/21026 (49%)]\tLoss: 0.085431\n",
      "Train Epoch: 6 [10368/21026 (49%)]\tLoss: 0.088382\n",
      "Train Epoch: 6 [10432/21026 (50%)]\tLoss: 0.090424\n",
      "Train Epoch: 6 [10496/21026 (50%)]\tLoss: 0.085323\n",
      "Train Epoch: 6 [10560/21026 (50%)]\tLoss: 0.081559\n",
      "Train Epoch: 6 [10624/21026 (50%)]\tLoss: 0.079285\n",
      "Train Epoch: 6 [10688/21026 (51%)]\tLoss: 0.084775\n",
      "Train Epoch: 6 [10752/21026 (51%)]\tLoss: 0.084496\n",
      "Train Epoch: 6 [10816/21026 (51%)]\tLoss: 0.090760\n",
      "Train Epoch: 6 [10880/21026 (52%)]\tLoss: 0.091953\n",
      "Train Epoch: 6 [10944/21026 (52%)]\tLoss: 0.087014\n",
      "Train Epoch: 6 [11008/21026 (52%)]\tLoss: 0.087929\n",
      "Train Epoch: 6 [11072/21026 (53%)]\tLoss: 0.093030\n",
      "Train Epoch: 6 [11136/21026 (53%)]\tLoss: 0.091002\n",
      "Train Epoch: 6 [11200/21026 (53%)]\tLoss: 0.082603\n",
      "Train Epoch: 6 [11264/21026 (53%)]\tLoss: 0.085888\n",
      "Train Epoch: 6 [11328/21026 (54%)]\tLoss: 0.094842\n",
      "Train Epoch: 6 [11392/21026 (54%)]\tLoss: 0.078672\n",
      "Train Epoch: 6 [11456/21026 (54%)]\tLoss: 0.084776\n",
      "Train Epoch: 6 [11520/21026 (55%)]\tLoss: 0.094977\n",
      "Train Epoch: 6 [11584/21026 (55%)]\tLoss: 0.091865\n",
      "Train Epoch: 6 [11648/21026 (55%)]\tLoss: 0.088278\n",
      "Train Epoch: 6 [11712/21026 (56%)]\tLoss: 0.084993\n",
      "Train Epoch: 6 [11776/21026 (56%)]\tLoss: 0.089107\n",
      "Train Epoch: 6 [11840/21026 (56%)]\tLoss: 0.094723\n",
      "Train Epoch: 6 [11904/21026 (57%)]\tLoss: 0.085767\n",
      "Train Epoch: 6 [11968/21026 (57%)]\tLoss: 0.089378\n",
      "Train Epoch: 6 [12032/21026 (57%)]\tLoss: 0.090576\n",
      "Train Epoch: 6 [12096/21026 (57%)]\tLoss: 0.090824\n",
      "Train Epoch: 6 [12160/21026 (58%)]\tLoss: 0.089294\n",
      "Train Epoch: 6 [12224/21026 (58%)]\tLoss: 0.081927\n",
      "Train Epoch: 6 [12288/21026 (58%)]\tLoss: 0.097600\n",
      "Train Epoch: 6 [12352/21026 (59%)]\tLoss: 0.091636\n",
      "Train Epoch: 6 [12416/21026 (59%)]\tLoss: 0.103342\n",
      "Train Epoch: 6 [12480/21026 (59%)]\tLoss: 0.095616\n",
      "Train Epoch: 6 [12544/21026 (60%)]\tLoss: 0.090490\n",
      "Train Epoch: 6 [12608/21026 (60%)]\tLoss: 0.097621\n",
      "Train Epoch: 6 [12672/21026 (60%)]\tLoss: 0.085531\n",
      "Train Epoch: 6 [12736/21026 (60%)]\tLoss: 0.107869\n",
      "Train Epoch: 6 [12800/21026 (61%)]\tLoss: 0.097688\n",
      "Train Epoch: 6 [12864/21026 (61%)]\tLoss: 0.085189\n",
      "Train Epoch: 6 [12928/21026 (61%)]\tLoss: 0.097007\n",
      "Train Epoch: 6 [12992/21026 (62%)]\tLoss: 0.090312\n",
      "Train Epoch: 6 [13056/21026 (62%)]\tLoss: 0.098848\n",
      "Train Epoch: 6 [13120/21026 (62%)]\tLoss: 0.093731\n",
      "Train Epoch: 6 [13184/21026 (63%)]\tLoss: 0.088456\n",
      "Train Epoch: 6 [13248/21026 (63%)]\tLoss: 0.087271\n",
      "Train Epoch: 6 [13312/21026 (63%)]\tLoss: 0.094314\n",
      "Train Epoch: 6 [13376/21026 (64%)]\tLoss: 0.093656\n",
      "Train Epoch: 6 [13440/21026 (64%)]\tLoss: 0.089918\n",
      "Train Epoch: 6 [13504/21026 (64%)]\tLoss: 0.085512\n",
      "Train Epoch: 6 [13568/21026 (64%)]\tLoss: 0.092898\n",
      "Train Epoch: 6 [13632/21026 (65%)]\tLoss: 0.091163\n",
      "Train Epoch: 6 [13696/21026 (65%)]\tLoss: 0.095246\n",
      "Train Epoch: 6 [13760/21026 (65%)]\tLoss: 0.094204\n",
      "Train Epoch: 6 [13824/21026 (66%)]\tLoss: 0.091096\n",
      "Train Epoch: 6 [13888/21026 (66%)]\tLoss: 0.090696\n",
      "Train Epoch: 6 [13952/21026 (66%)]\tLoss: 0.087197\n",
      "Train Epoch: 6 [14016/21026 (67%)]\tLoss: 0.089907\n",
      "Train Epoch: 6 [14080/21026 (67%)]\tLoss: 0.100170\n",
      "Train Epoch: 6 [14144/21026 (67%)]\tLoss: 0.085936\n",
      "Train Epoch: 6 [14208/21026 (67%)]\tLoss: 0.085293\n",
      "Train Epoch: 6 [14272/21026 (68%)]\tLoss: 0.091324\n",
      "Train Epoch: 6 [14336/21026 (68%)]\tLoss: 0.085484\n",
      "Train Epoch: 6 [14400/21026 (68%)]\tLoss: 0.096857\n",
      "Train Epoch: 6 [14464/21026 (69%)]\tLoss: 0.091361\n",
      "Train Epoch: 6 [14528/21026 (69%)]\tLoss: 0.088471\n",
      "Train Epoch: 6 [14592/21026 (69%)]\tLoss: 0.092556\n",
      "Train Epoch: 6 [14656/21026 (70%)]\tLoss: 0.084893\n",
      "Train Epoch: 6 [14720/21026 (70%)]\tLoss: 0.090728\n",
      "Train Epoch: 6 [14784/21026 (70%)]\tLoss: 0.097735\n",
      "Train Epoch: 6 [14848/21026 (71%)]\tLoss: 0.089299\n",
      "Train Epoch: 6 [14912/21026 (71%)]\tLoss: 0.085880\n",
      "Train Epoch: 6 [14976/21026 (71%)]\tLoss: 0.086290\n",
      "Train Epoch: 6 [15040/21026 (71%)]\tLoss: 0.088319\n",
      "Train Epoch: 6 [15104/21026 (72%)]\tLoss: 0.089601\n",
      "Train Epoch: 6 [15168/21026 (72%)]\tLoss: 0.078029\n",
      "Train Epoch: 6 [15232/21026 (72%)]\tLoss: 0.082401\n",
      "Train Epoch: 6 [15296/21026 (73%)]\tLoss: 0.090427\n",
      "Train Epoch: 6 [15360/21026 (73%)]\tLoss: 0.095214\n",
      "Train Epoch: 6 [15424/21026 (73%)]\tLoss: 0.092401\n",
      "Train Epoch: 6 [15488/21026 (74%)]\tLoss: 0.085182\n",
      "Train Epoch: 6 [15552/21026 (74%)]\tLoss: 0.091550\n",
      "Train Epoch: 6 [15616/21026 (74%)]\tLoss: 0.105037\n",
      "Train Epoch: 6 [15680/21026 (74%)]\tLoss: 0.096709\n",
      "Train Epoch: 6 [15744/21026 (75%)]\tLoss: 0.098128\n",
      "Train Epoch: 6 [15808/21026 (75%)]\tLoss: 0.089505\n",
      "Train Epoch: 6 [15872/21026 (75%)]\tLoss: 0.099900\n",
      "Train Epoch: 6 [15936/21026 (76%)]\tLoss: 0.089482\n",
      "Train Epoch: 6 [16000/21026 (76%)]\tLoss: 0.093094\n",
      "Train Epoch: 6 [16064/21026 (76%)]\tLoss: 0.098381\n",
      "Train Epoch: 6 [16128/21026 (77%)]\tLoss: 0.099787\n",
      "Train Epoch: 6 [16192/21026 (77%)]\tLoss: 0.091720\n",
      "Train Epoch: 6 [16256/21026 (77%)]\tLoss: 0.096637\n",
      "Train Epoch: 6 [16320/21026 (78%)]\tLoss: 0.088776\n",
      "Train Epoch: 6 [16384/21026 (78%)]\tLoss: 0.086754\n",
      "Train Epoch: 6 [16448/21026 (78%)]\tLoss: 0.100010\n",
      "Train Epoch: 6 [16512/21026 (78%)]\tLoss: 0.099006\n",
      "Train Epoch: 6 [16576/21026 (79%)]\tLoss: 0.095063\n",
      "Train Epoch: 6 [16640/21026 (79%)]\tLoss: 0.094598\n",
      "Train Epoch: 6 [16704/21026 (79%)]\tLoss: 0.091076\n",
      "Train Epoch: 6 [16768/21026 (80%)]\tLoss: 0.086471\n",
      "Train Epoch: 6 [16832/21026 (80%)]\tLoss: 0.099865\n",
      "Train Epoch: 6 [16896/21026 (80%)]\tLoss: 0.084920\n",
      "Train Epoch: 6 [16960/21026 (81%)]\tLoss: 0.088946\n",
      "Train Epoch: 6 [17024/21026 (81%)]\tLoss: 0.094678\n",
      "Train Epoch: 6 [17088/21026 (81%)]\tLoss: 0.086704\n",
      "Train Epoch: 6 [17152/21026 (81%)]\tLoss: 0.096737\n",
      "Train Epoch: 6 [17216/21026 (82%)]\tLoss: 0.099292\n",
      "Train Epoch: 6 [17280/21026 (82%)]\tLoss: 0.092358\n",
      "Train Epoch: 6 [17344/21026 (82%)]\tLoss: 0.092417\n",
      "Train Epoch: 6 [17408/21026 (83%)]\tLoss: 0.100217\n",
      "Train Epoch: 6 [17472/21026 (83%)]\tLoss: 0.101166\n",
      "Train Epoch: 6 [17536/21026 (83%)]\tLoss: 0.089210\n",
      "Train Epoch: 6 [17600/21026 (84%)]\tLoss: 0.101673\n",
      "Train Epoch: 6 [17664/21026 (84%)]\tLoss: 0.093493\n",
      "Train Epoch: 6 [17728/21026 (84%)]\tLoss: 0.100520\n",
      "Train Epoch: 6 [17792/21026 (84%)]\tLoss: 0.093237\n",
      "Train Epoch: 6 [17856/21026 (85%)]\tLoss: 0.098091\n",
      "Train Epoch: 6 [17920/21026 (85%)]\tLoss: 0.091614\n",
      "Train Epoch: 6 [17984/21026 (85%)]\tLoss: 0.091676\n",
      "Train Epoch: 6 [18048/21026 (86%)]\tLoss: 0.091017\n",
      "Train Epoch: 6 [18112/21026 (86%)]\tLoss: 0.088021\n",
      "Train Epoch: 6 [18176/21026 (86%)]\tLoss: 0.086123\n",
      "Train Epoch: 6 [18240/21026 (87%)]\tLoss: 0.092721\n",
      "Train Epoch: 6 [18304/21026 (87%)]\tLoss: 0.089518\n",
      "Train Epoch: 6 [18368/21026 (87%)]\tLoss: 0.095451\n",
      "Train Epoch: 6 [18432/21026 (88%)]\tLoss: 0.092282\n",
      "Train Epoch: 6 [18496/21026 (88%)]\tLoss: 0.096591\n",
      "Train Epoch: 6 [18560/21026 (88%)]\tLoss: 0.092075\n",
      "Train Epoch: 6 [18624/21026 (88%)]\tLoss: 0.096378\n",
      "Train Epoch: 6 [18688/21026 (89%)]\tLoss: 0.084797\n",
      "Train Epoch: 6 [18752/21026 (89%)]\tLoss: 0.097356\n",
      "Train Epoch: 6 [18816/21026 (89%)]\tLoss: 0.101265\n",
      "Train Epoch: 6 [18880/21026 (90%)]\tLoss: 0.099397\n",
      "Train Epoch: 6 [18944/21026 (90%)]\tLoss: 0.103724\n",
      "Train Epoch: 6 [19008/21026 (90%)]\tLoss: 0.088001\n",
      "Train Epoch: 6 [19072/21026 (91%)]\tLoss: 0.094319\n",
      "Train Epoch: 6 [19136/21026 (91%)]\tLoss: 0.088455\n",
      "Train Epoch: 6 [19200/21026 (91%)]\tLoss: 0.090262\n",
      "Train Epoch: 6 [19264/21026 (91%)]\tLoss: 0.095799\n",
      "Train Epoch: 6 [19328/21026 (92%)]\tLoss: 0.092864\n",
      "Train Epoch: 6 [19392/21026 (92%)]\tLoss: 0.085236\n",
      "Train Epoch: 6 [19456/21026 (92%)]\tLoss: 0.097093\n",
      "Train Epoch: 6 [19520/21026 (93%)]\tLoss: 0.099458\n",
      "Train Epoch: 6 [19584/21026 (93%)]\tLoss: 0.096059\n",
      "Train Epoch: 6 [19648/21026 (93%)]\tLoss: 0.093095\n",
      "Train Epoch: 6 [19712/21026 (94%)]\tLoss: 0.083363\n",
      "Train Epoch: 6 [19776/21026 (94%)]\tLoss: 0.104563\n",
      "Train Epoch: 6 [19840/21026 (94%)]\tLoss: 0.087361\n",
      "Train Epoch: 6 [19904/21026 (95%)]\tLoss: 0.088148\n",
      "Train Epoch: 6 [19968/21026 (95%)]\tLoss: 0.089617\n",
      "Train Epoch: 6 [20032/21026 (95%)]\tLoss: 0.093931\n",
      "Train Epoch: 6 [20096/21026 (95%)]\tLoss: 0.091261\n",
      "Train Epoch: 6 [20160/21026 (96%)]\tLoss: 0.090498\n",
      "Train Epoch: 6 [20224/21026 (96%)]\tLoss: 0.088588\n",
      "Train Epoch: 6 [20288/21026 (96%)]\tLoss: 0.092218\n",
      "Train Epoch: 6 [20352/21026 (97%)]\tLoss: 0.091811\n",
      "Train Epoch: 6 [20416/21026 (97%)]\tLoss: 0.089448\n",
      "Train Epoch: 6 [20480/21026 (97%)]\tLoss: 0.087012\n",
      "Train Epoch: 6 [20544/21026 (98%)]\tLoss: 0.095351\n",
      "Train Epoch: 6 [20608/21026 (98%)]\tLoss: 0.087138\n",
      "Train Epoch: 6 [20672/21026 (98%)]\tLoss: 0.092402\n",
      "Train Epoch: 6 [20736/21026 (98%)]\tLoss: 0.090342\n",
      "Train Epoch: 6 [20800/21026 (99%)]\tLoss: 0.090790\n",
      "Train Epoch: 6 [20864/21026 (99%)]\tLoss: 0.097902\n",
      "Train Epoch: 6 [20928/21026 (99%)]\tLoss: 0.102302\n",
      "Train Epoch: 6 [11152/21026 (100%)]\tLoss: 0.095630\n",
      "====> Epoch: 6 Average loss: 0.00280108\n",
      "Train Epoch: 7 [0/21026 (0%)]\tLoss: 0.082167\n",
      "Train Epoch: 7 [64/21026 (0%)]\tLoss: 0.079633\n",
      "Train Epoch: 7 [128/21026 (1%)]\tLoss: 0.076883\n",
      "Train Epoch: 7 [192/21026 (1%)]\tLoss: 0.080349\n",
      "Train Epoch: 7 [256/21026 (1%)]\tLoss: 0.072108\n",
      "Train Epoch: 7 [320/21026 (2%)]\tLoss: 0.077375\n",
      "Train Epoch: 7 [384/21026 (2%)]\tLoss: 0.081454\n",
      "Train Epoch: 7 [448/21026 (2%)]\tLoss: 0.080866\n",
      "Train Epoch: 7 [512/21026 (2%)]\tLoss: 0.077294\n",
      "Train Epoch: 7 [576/21026 (3%)]\tLoss: 0.072976\n",
      "Train Epoch: 7 [640/21026 (3%)]\tLoss: 0.079444\n",
      "Train Epoch: 7 [704/21026 (3%)]\tLoss: 0.076534\n",
      "Train Epoch: 7 [768/21026 (4%)]\tLoss: 0.077407\n",
      "Train Epoch: 7 [832/21026 (4%)]\tLoss: 0.077085\n",
      "Train Epoch: 7 [896/21026 (4%)]\tLoss: 0.082892\n",
      "Train Epoch: 7 [960/21026 (5%)]\tLoss: 0.075056\n",
      "Train Epoch: 7 [1024/21026 (5%)]\tLoss: 0.071278\n",
      "Train Epoch: 7 [1088/21026 (5%)]\tLoss: 0.072181\n",
      "Train Epoch: 7 [1152/21026 (5%)]\tLoss: 0.075712\n",
      "Train Epoch: 7 [1216/21026 (6%)]\tLoss: 0.078325\n",
      "Train Epoch: 7 [1280/21026 (6%)]\tLoss: 0.071163\n",
      "Train Epoch: 7 [1344/21026 (6%)]\tLoss: 0.067429\n",
      "Train Epoch: 7 [1408/21026 (7%)]\tLoss: 0.074869\n",
      "Train Epoch: 7 [1472/21026 (7%)]\tLoss: 0.079074\n",
      "Train Epoch: 7 [1536/21026 (7%)]\tLoss: 0.071051\n",
      "Train Epoch: 7 [1600/21026 (8%)]\tLoss: 0.071682\n",
      "Train Epoch: 7 [1664/21026 (8%)]\tLoss: 0.063478\n",
      "Train Epoch: 7 [1728/21026 (8%)]\tLoss: 0.070394\n",
      "Train Epoch: 7 [1792/21026 (9%)]\tLoss: 0.070907\n",
      "Train Epoch: 7 [1856/21026 (9%)]\tLoss: 0.072945\n",
      "Train Epoch: 7 [1920/21026 (9%)]\tLoss: 0.075567\n",
      "Train Epoch: 7 [1984/21026 (9%)]\tLoss: 0.077245\n",
      "Train Epoch: 7 [2048/21026 (10%)]\tLoss: 0.072917\n",
      "Train Epoch: 7 [2112/21026 (10%)]\tLoss: 0.068569\n",
      "Train Epoch: 7 [2176/21026 (10%)]\tLoss: 0.077427\n",
      "Train Epoch: 7 [2240/21026 (11%)]\tLoss: 0.074841\n",
      "Train Epoch: 7 [2304/21026 (11%)]\tLoss: 0.068301\n",
      "Train Epoch: 7 [2368/21026 (11%)]\tLoss: 0.074676\n",
      "Train Epoch: 7 [2432/21026 (12%)]\tLoss: 0.073539\n",
      "Train Epoch: 7 [2496/21026 (12%)]\tLoss: 0.072723\n",
      "Train Epoch: 7 [2560/21026 (12%)]\tLoss: 0.075383\n",
      "Train Epoch: 7 [2624/21026 (12%)]\tLoss: 0.075596\n",
      "Train Epoch: 7 [2688/21026 (13%)]\tLoss: 0.071292\n",
      "Train Epoch: 7 [2752/21026 (13%)]\tLoss: 0.071957\n",
      "Train Epoch: 7 [2816/21026 (13%)]\tLoss: 0.071863\n",
      "Train Epoch: 7 [2880/21026 (14%)]\tLoss: 0.073903\n",
      "Train Epoch: 7 [2944/21026 (14%)]\tLoss: 0.074286\n",
      "Train Epoch: 7 [3008/21026 (14%)]\tLoss: 0.078599\n",
      "Train Epoch: 7 [3072/21026 (15%)]\tLoss: 0.072871\n",
      "Train Epoch: 7 [3136/21026 (15%)]\tLoss: 0.068355\n",
      "Train Epoch: 7 [3200/21026 (15%)]\tLoss: 0.075256\n",
      "Train Epoch: 7 [3264/21026 (16%)]\tLoss: 0.068881\n",
      "Train Epoch: 7 [3328/21026 (16%)]\tLoss: 0.071504\n",
      "Train Epoch: 7 [3392/21026 (16%)]\tLoss: 0.077142\n",
      "Train Epoch: 7 [3456/21026 (16%)]\tLoss: 0.071961\n",
      "Train Epoch: 7 [3520/21026 (17%)]\tLoss: 0.071672\n",
      "Train Epoch: 7 [3584/21026 (17%)]\tLoss: 0.076801\n",
      "Train Epoch: 7 [3648/21026 (17%)]\tLoss: 0.072376\n",
      "Train Epoch: 7 [3712/21026 (18%)]\tLoss: 0.069596\n",
      "Train Epoch: 7 [3776/21026 (18%)]\tLoss: 0.071365\n",
      "Train Epoch: 7 [3840/21026 (18%)]\tLoss: 0.075274\n",
      "Train Epoch: 7 [3904/21026 (19%)]\tLoss: 0.079735\n",
      "Train Epoch: 7 [3968/21026 (19%)]\tLoss: 0.069263\n",
      "Train Epoch: 7 [4032/21026 (19%)]\tLoss: 0.078836\n",
      "Train Epoch: 7 [4096/21026 (19%)]\tLoss: 0.068839\n",
      "Train Epoch: 7 [4160/21026 (20%)]\tLoss: 0.069781\n",
      "Train Epoch: 7 [4224/21026 (20%)]\tLoss: 0.072153\n",
      "Train Epoch: 7 [4288/21026 (20%)]\tLoss: 0.076831\n",
      "Train Epoch: 7 [4352/21026 (21%)]\tLoss: 0.070393\n",
      "Train Epoch: 7 [4416/21026 (21%)]\tLoss: 0.072850\n",
      "Train Epoch: 7 [4480/21026 (21%)]\tLoss: 0.074826\n",
      "Train Epoch: 7 [4544/21026 (22%)]\tLoss: 0.068853\n",
      "Train Epoch: 7 [4608/21026 (22%)]\tLoss: 0.070365\n",
      "Train Epoch: 7 [4672/21026 (22%)]\tLoss: 0.070584\n",
      "Train Epoch: 7 [4736/21026 (22%)]\tLoss: 0.072502\n",
      "Train Epoch: 7 [4800/21026 (23%)]\tLoss: 0.068227\n",
      "Train Epoch: 7 [4864/21026 (23%)]\tLoss: 0.069860\n",
      "Train Epoch: 7 [4928/21026 (23%)]\tLoss: 0.071476\n",
      "Train Epoch: 7 [4992/21026 (24%)]\tLoss: 0.074517\n",
      "Train Epoch: 7 [5056/21026 (24%)]\tLoss: 0.071482\n",
      "Train Epoch: 7 [5120/21026 (24%)]\tLoss: 0.073643\n",
      "Train Epoch: 7 [5184/21026 (25%)]\tLoss: 0.072734\n",
      "Train Epoch: 7 [5248/21026 (25%)]\tLoss: 0.067746\n",
      "Train Epoch: 7 [5312/21026 (25%)]\tLoss: 0.075986\n",
      "Train Epoch: 7 [5376/21026 (26%)]\tLoss: 0.075431\n",
      "Train Epoch: 7 [5440/21026 (26%)]\tLoss: 0.079620\n",
      "Train Epoch: 7 [5504/21026 (26%)]\tLoss: 0.076836\n",
      "Train Epoch: 7 [5568/21026 (26%)]\tLoss: 0.070365\n",
      "Train Epoch: 7 [5632/21026 (27%)]\tLoss: 0.074111\n",
      "Train Epoch: 7 [5696/21026 (27%)]\tLoss: 0.069340\n",
      "Train Epoch: 7 [5760/21026 (27%)]\tLoss: 0.069976\n",
      "Train Epoch: 7 [5824/21026 (28%)]\tLoss: 0.071393\n",
      "Train Epoch: 7 [5888/21026 (28%)]\tLoss: 0.071950\n",
      "Train Epoch: 7 [5952/21026 (28%)]\tLoss: 0.073125\n",
      "Train Epoch: 7 [6016/21026 (29%)]\tLoss: 0.079773\n",
      "Train Epoch: 7 [6080/21026 (29%)]\tLoss: 0.069704\n",
      "Train Epoch: 7 [6144/21026 (29%)]\tLoss: 0.069011\n",
      "Train Epoch: 7 [6208/21026 (29%)]\tLoss: 0.075009\n",
      "Train Epoch: 7 [6272/21026 (30%)]\tLoss: 0.071904\n",
      "Train Epoch: 7 [6336/21026 (30%)]\tLoss: 0.071931\n",
      "Train Epoch: 7 [6400/21026 (30%)]\tLoss: 0.082518\n",
      "Train Epoch: 7 [6464/21026 (31%)]\tLoss: 0.079844\n",
      "Train Epoch: 7 [6528/21026 (31%)]\tLoss: 0.073729\n",
      "Train Epoch: 7 [6592/21026 (31%)]\tLoss: 0.076694\n",
      "Train Epoch: 7 [6656/21026 (32%)]\tLoss: 0.076420\n",
      "Train Epoch: 7 [6720/21026 (32%)]\tLoss: 0.075893\n",
      "Train Epoch: 7 [6784/21026 (32%)]\tLoss: 0.076405\n",
      "Train Epoch: 7 [6848/21026 (33%)]\tLoss: 0.079557\n",
      "Train Epoch: 7 [6912/21026 (33%)]\tLoss: 0.071436\n",
      "Train Epoch: 7 [6976/21026 (33%)]\tLoss: 0.071235\n",
      "Train Epoch: 7 [7040/21026 (33%)]\tLoss: 0.077297\n",
      "Train Epoch: 7 [7104/21026 (34%)]\tLoss: 0.068303\n",
      "Train Epoch: 7 [7168/21026 (34%)]\tLoss: 0.077949\n",
      "Train Epoch: 7 [7232/21026 (34%)]\tLoss: 0.080203\n",
      "Train Epoch: 7 [7296/21026 (35%)]\tLoss: 0.082511\n",
      "Train Epoch: 7 [7360/21026 (35%)]\tLoss: 0.077230\n",
      "Train Epoch: 7 [7424/21026 (35%)]\tLoss: 0.073134\n",
      "Train Epoch: 7 [7488/21026 (36%)]\tLoss: 0.076047\n",
      "Train Epoch: 7 [7552/21026 (36%)]\tLoss: 0.077334\n",
      "Train Epoch: 7 [7616/21026 (36%)]\tLoss: 0.071516\n",
      "Train Epoch: 7 [7680/21026 (36%)]\tLoss: 0.087269\n",
      "Train Epoch: 7 [7744/21026 (37%)]\tLoss: 0.076524\n",
      "Train Epoch: 7 [7808/21026 (37%)]\tLoss: 0.076674\n",
      "Train Epoch: 7 [7872/21026 (37%)]\tLoss: 0.072475\n",
      "Train Epoch: 7 [7936/21026 (38%)]\tLoss: 0.077823\n",
      "Train Epoch: 7 [8000/21026 (38%)]\tLoss: 0.074748\n",
      "Train Epoch: 7 [8064/21026 (38%)]\tLoss: 0.084919\n",
      "Train Epoch: 7 [8128/21026 (39%)]\tLoss: 0.075542\n",
      "Train Epoch: 7 [8192/21026 (39%)]\tLoss: 0.073879\n",
      "Train Epoch: 7 [8256/21026 (39%)]\tLoss: 0.073502\n",
      "Train Epoch: 7 [8320/21026 (40%)]\tLoss: 0.069501\n",
      "Train Epoch: 7 [8384/21026 (40%)]\tLoss: 0.077706\n",
      "Train Epoch: 7 [8448/21026 (40%)]\tLoss: 0.082151\n",
      "Train Epoch: 7 [8512/21026 (40%)]\tLoss: 0.079762\n",
      "Train Epoch: 7 [8576/21026 (41%)]\tLoss: 0.078895\n",
      "Train Epoch: 7 [8640/21026 (41%)]\tLoss: 0.083395\n",
      "Train Epoch: 7 [8704/21026 (41%)]\tLoss: 0.072919\n",
      "Train Epoch: 7 [8768/21026 (42%)]\tLoss: 0.070397\n",
      "Train Epoch: 7 [8832/21026 (42%)]\tLoss: 0.073758\n",
      "Train Epoch: 7 [8896/21026 (42%)]\tLoss: 0.074639\n",
      "Train Epoch: 7 [8960/21026 (43%)]\tLoss: 0.069983\n",
      "Train Epoch: 7 [9024/21026 (43%)]\tLoss: 0.073637\n",
      "Train Epoch: 7 [9088/21026 (43%)]\tLoss: 0.077410\n",
      "Train Epoch: 7 [9152/21026 (43%)]\tLoss: 0.079547\n",
      "Train Epoch: 7 [9216/21026 (44%)]\tLoss: 0.070567\n",
      "Train Epoch: 7 [9280/21026 (44%)]\tLoss: 0.068602\n",
      "Train Epoch: 7 [9344/21026 (44%)]\tLoss: 0.080559\n",
      "Train Epoch: 7 [9408/21026 (45%)]\tLoss: 0.075577\n",
      "Train Epoch: 7 [9472/21026 (45%)]\tLoss: 0.072931\n",
      "Train Epoch: 7 [9536/21026 (45%)]\tLoss: 0.081956\n",
      "Train Epoch: 7 [9600/21026 (46%)]\tLoss: 0.073589\n",
      "Train Epoch: 7 [9664/21026 (46%)]\tLoss: 0.071031\n",
      "Train Epoch: 7 [9728/21026 (46%)]\tLoss: 0.076990\n",
      "Train Epoch: 7 [9792/21026 (47%)]\tLoss: 0.080607\n",
      "Train Epoch: 7 [9856/21026 (47%)]\tLoss: 0.083594\n",
      "Train Epoch: 7 [9920/21026 (47%)]\tLoss: 0.084012\n",
      "Train Epoch: 7 [9984/21026 (47%)]\tLoss: 0.078414\n",
      "Train Epoch: 7 [10048/21026 (48%)]\tLoss: 0.071184\n",
      "Train Epoch: 7 [10112/21026 (48%)]\tLoss: 0.074129\n",
      "Train Epoch: 7 [10176/21026 (48%)]\tLoss: 0.071687\n",
      "Train Epoch: 7 [10240/21026 (49%)]\tLoss: 0.078606\n",
      "Train Epoch: 7 [10304/21026 (49%)]\tLoss: 0.072133\n",
      "Train Epoch: 7 [10368/21026 (49%)]\tLoss: 0.085967\n",
      "Train Epoch: 7 [10432/21026 (50%)]\tLoss: 0.073392\n",
      "Train Epoch: 7 [10496/21026 (50%)]\tLoss: 0.095260\n",
      "Train Epoch: 7 [10560/21026 (50%)]\tLoss: 0.077065\n",
      "Train Epoch: 7 [10624/21026 (50%)]\tLoss: 0.075136\n",
      "Train Epoch: 7 [10688/21026 (51%)]\tLoss: 0.077140\n",
      "Train Epoch: 7 [10752/21026 (51%)]\tLoss: 0.078061\n",
      "Train Epoch: 7 [10816/21026 (51%)]\tLoss: 0.071306\n",
      "Train Epoch: 7 [10880/21026 (52%)]\tLoss: 0.072958\n",
      "Train Epoch: 7 [10944/21026 (52%)]\tLoss: 0.072223\n",
      "Train Epoch: 7 [11008/21026 (52%)]\tLoss: 0.077265\n",
      "Train Epoch: 7 [11072/21026 (53%)]\tLoss: 0.077758\n",
      "Train Epoch: 7 [11136/21026 (53%)]\tLoss: 0.078637\n",
      "Train Epoch: 7 [11200/21026 (53%)]\tLoss: 0.068775\n",
      "Train Epoch: 7 [11264/21026 (53%)]\tLoss: 0.075307\n",
      "Train Epoch: 7 [11328/21026 (54%)]\tLoss: 0.082293\n",
      "Train Epoch: 7 [11392/21026 (54%)]\tLoss: 0.074845\n",
      "Train Epoch: 7 [11456/21026 (54%)]\tLoss: 0.079771\n",
      "Train Epoch: 7 [11520/21026 (55%)]\tLoss: 0.082117\n",
      "Train Epoch: 7 [11584/21026 (55%)]\tLoss: 0.075165\n",
      "Train Epoch: 7 [11648/21026 (55%)]\tLoss: 0.081362\n",
      "Train Epoch: 7 [11712/21026 (56%)]\tLoss: 0.076239\n",
      "Train Epoch: 7 [11776/21026 (56%)]\tLoss: 0.077447\n",
      "Train Epoch: 7 [11840/21026 (56%)]\tLoss: 0.072069\n",
      "Train Epoch: 7 [11904/21026 (57%)]\tLoss: 0.073912\n",
      "Train Epoch: 7 [11968/21026 (57%)]\tLoss: 0.081484\n",
      "Train Epoch: 7 [12032/21026 (57%)]\tLoss: 0.080770\n",
      "Train Epoch: 7 [12096/21026 (57%)]\tLoss: 0.073239\n",
      "Train Epoch: 7 [12160/21026 (58%)]\tLoss: 0.081624\n",
      "Train Epoch: 7 [12224/21026 (58%)]\tLoss: 0.080138\n",
      "Train Epoch: 7 [12288/21026 (58%)]\tLoss: 0.082133\n",
      "Train Epoch: 7 [12352/21026 (59%)]\tLoss: 0.069259\n",
      "Train Epoch: 7 [12416/21026 (59%)]\tLoss: 0.078549\n",
      "Train Epoch: 7 [12480/21026 (59%)]\tLoss: 0.082987\n",
      "Train Epoch: 7 [12544/21026 (60%)]\tLoss: 0.074461\n",
      "Train Epoch: 7 [12608/21026 (60%)]\tLoss: 0.076929\n",
      "Train Epoch: 7 [12672/21026 (60%)]\tLoss: 0.078537\n",
      "Train Epoch: 7 [12736/21026 (60%)]\tLoss: 0.095425\n",
      "Train Epoch: 7 [12800/21026 (61%)]\tLoss: 0.075338\n",
      "Train Epoch: 7 [12864/21026 (61%)]\tLoss: 0.081670\n",
      "Train Epoch: 7 [12928/21026 (61%)]\tLoss: 0.089729\n",
      "Train Epoch: 7 [12992/21026 (62%)]\tLoss: 0.089685\n",
      "Train Epoch: 7 [13056/21026 (62%)]\tLoss: 0.083322\n",
      "Train Epoch: 7 [13120/21026 (62%)]\tLoss: 0.079378\n",
      "Train Epoch: 7 [13184/21026 (63%)]\tLoss: 0.077432\n",
      "Train Epoch: 7 [13248/21026 (63%)]\tLoss: 0.085504\n",
      "Train Epoch: 7 [13312/21026 (63%)]\tLoss: 0.076330\n",
      "Train Epoch: 7 [13376/21026 (64%)]\tLoss: 0.083156\n",
      "Train Epoch: 7 [13440/21026 (64%)]\tLoss: 0.078067\n",
      "Train Epoch: 7 [13504/21026 (64%)]\tLoss: 0.076710\n",
      "Train Epoch: 7 [13568/21026 (64%)]\tLoss: 0.089225\n",
      "Train Epoch: 7 [13632/21026 (65%)]\tLoss: 0.081360\n",
      "Train Epoch: 7 [13696/21026 (65%)]\tLoss: 0.075473\n",
      "Train Epoch: 7 [13760/21026 (65%)]\tLoss: 0.074793\n",
      "Train Epoch: 7 [13824/21026 (66%)]\tLoss: 0.078471\n",
      "Train Epoch: 7 [13888/21026 (66%)]\tLoss: 0.074694\n",
      "Train Epoch: 7 [13952/21026 (66%)]\tLoss: 0.079065\n",
      "Train Epoch: 7 [14016/21026 (67%)]\tLoss: 0.076539\n",
      "Train Epoch: 7 [14080/21026 (67%)]\tLoss: 0.086767\n",
      "Train Epoch: 7 [14144/21026 (67%)]\tLoss: 0.077717\n",
      "Train Epoch: 7 [14208/21026 (67%)]\tLoss: 0.073895\n",
      "Train Epoch: 7 [14272/21026 (68%)]\tLoss: 0.072322\n",
      "Train Epoch: 7 [14336/21026 (68%)]\tLoss: 0.075298\n",
      "Train Epoch: 7 [14400/21026 (68%)]\tLoss: 0.085075\n",
      "Train Epoch: 7 [14464/21026 (69%)]\tLoss: 0.085290\n",
      "Train Epoch: 7 [14528/21026 (69%)]\tLoss: 0.078009\n",
      "Train Epoch: 7 [14592/21026 (69%)]\tLoss: 0.086605\n",
      "Train Epoch: 7 [14656/21026 (70%)]\tLoss: 0.089235\n",
      "Train Epoch: 7 [14720/21026 (70%)]\tLoss: 0.082732\n",
      "Train Epoch: 7 [14784/21026 (70%)]\tLoss: 0.086917\n",
      "Train Epoch: 7 [14848/21026 (71%)]\tLoss: 0.078388\n",
      "Train Epoch: 7 [14912/21026 (71%)]\tLoss: 0.091241\n",
      "Train Epoch: 7 [14976/21026 (71%)]\tLoss: 0.079214\n",
      "Train Epoch: 7 [15040/21026 (71%)]\tLoss: 0.077703\n",
      "Train Epoch: 7 [15104/21026 (72%)]\tLoss: 0.085708\n",
      "Train Epoch: 7 [15168/21026 (72%)]\tLoss: 0.094724\n",
      "Train Epoch: 7 [15232/21026 (72%)]\tLoss: 0.076069\n",
      "Train Epoch: 7 [15296/21026 (73%)]\tLoss: 0.082445\n",
      "Train Epoch: 7 [15360/21026 (73%)]\tLoss: 0.084600\n",
      "Train Epoch: 7 [15424/21026 (73%)]\tLoss: 0.079004\n",
      "Train Epoch: 7 [15488/21026 (74%)]\tLoss: 0.082434\n",
      "Train Epoch: 7 [15552/21026 (74%)]\tLoss: 0.082892\n",
      "Train Epoch: 7 [15616/21026 (74%)]\tLoss: 0.078133\n",
      "Train Epoch: 7 [15680/21026 (74%)]\tLoss: 0.084035\n",
      "Train Epoch: 7 [15744/21026 (75%)]\tLoss: 0.076494\n",
      "Train Epoch: 7 [15808/21026 (75%)]\tLoss: 0.082861\n",
      "Train Epoch: 7 [15872/21026 (75%)]\tLoss: 0.086686\n",
      "Train Epoch: 7 [15936/21026 (76%)]\tLoss: 0.082288\n",
      "Train Epoch: 7 [16000/21026 (76%)]\tLoss: 0.098188\n",
      "Train Epoch: 7 [16064/21026 (76%)]\tLoss: 0.088006\n",
      "Train Epoch: 7 [16128/21026 (77%)]\tLoss: 0.088863\n",
      "Train Epoch: 7 [16192/21026 (77%)]\tLoss: 0.089681\n",
      "Train Epoch: 7 [16256/21026 (77%)]\tLoss: 0.092190\n",
      "Train Epoch: 7 [16320/21026 (78%)]\tLoss: 0.084463\n",
      "Train Epoch: 7 [16384/21026 (78%)]\tLoss: 0.085990\n",
      "Train Epoch: 7 [16448/21026 (78%)]\tLoss: 0.084154\n",
      "Train Epoch: 7 [16512/21026 (78%)]\tLoss: 0.080973\n",
      "Train Epoch: 7 [16576/21026 (79%)]\tLoss: 0.084007\n",
      "Train Epoch: 7 [16640/21026 (79%)]\tLoss: 0.086600\n",
      "Train Epoch: 7 [16704/21026 (79%)]\tLoss: 0.089872\n",
      "Train Epoch: 7 [16768/21026 (80%)]\tLoss: 0.081760\n",
      "Train Epoch: 7 [16832/21026 (80%)]\tLoss: 0.094813\n",
      "Train Epoch: 7 [16896/21026 (80%)]\tLoss: 0.087485\n",
      "Train Epoch: 7 [16960/21026 (81%)]\tLoss: 0.083934\n",
      "Train Epoch: 7 [17024/21026 (81%)]\tLoss: 0.080221\n",
      "Train Epoch: 7 [17088/21026 (81%)]\tLoss: 0.082084\n",
      "Train Epoch: 7 [17152/21026 (81%)]\tLoss: 0.081613\n",
      "Train Epoch: 7 [17216/21026 (82%)]\tLoss: 0.086963\n",
      "Train Epoch: 7 [17280/21026 (82%)]\tLoss: 0.086068\n",
      "Train Epoch: 7 [17344/21026 (82%)]\tLoss: 0.085101\n",
      "Train Epoch: 7 [17408/21026 (83%)]\tLoss: 0.081936\n",
      "Train Epoch: 7 [17472/21026 (83%)]\tLoss: 0.087375\n",
      "Train Epoch: 7 [17536/21026 (83%)]\tLoss: 0.075008\n",
      "Train Epoch: 7 [17600/21026 (84%)]\tLoss: 0.091890\n",
      "Train Epoch: 7 [17664/21026 (84%)]\tLoss: 0.082503\n",
      "Train Epoch: 7 [17728/21026 (84%)]\tLoss: 0.074552\n",
      "Train Epoch: 7 [17792/21026 (84%)]\tLoss: 0.087311\n",
      "Train Epoch: 7 [17856/21026 (85%)]\tLoss: 0.086188\n",
      "Train Epoch: 7 [17920/21026 (85%)]\tLoss: 0.080635\n",
      "Train Epoch: 7 [17984/21026 (85%)]\tLoss: 0.091732\n",
      "Train Epoch: 7 [18048/21026 (86%)]\tLoss: 0.089428\n",
      "Train Epoch: 7 [18112/21026 (86%)]\tLoss: 0.092047\n",
      "Train Epoch: 7 [18176/21026 (86%)]\tLoss: 0.087707\n",
      "Train Epoch: 7 [18240/21026 (87%)]\tLoss: 0.078105\n",
      "Train Epoch: 7 [18304/21026 (87%)]\tLoss: 0.081124\n",
      "Train Epoch: 7 [18368/21026 (87%)]\tLoss: 0.091346\n",
      "Train Epoch: 7 [18432/21026 (88%)]\tLoss: 0.081187\n",
      "Train Epoch: 7 [18496/21026 (88%)]\tLoss: 0.088670\n",
      "Train Epoch: 7 [18560/21026 (88%)]\tLoss: 0.088234\n",
      "Train Epoch: 7 [18624/21026 (88%)]\tLoss: 0.089062\n",
      "Train Epoch: 7 [18688/21026 (89%)]\tLoss: 0.080893\n",
      "Train Epoch: 7 [18752/21026 (89%)]\tLoss: 0.080263\n",
      "Train Epoch: 7 [18816/21026 (89%)]\tLoss: 0.082500\n",
      "Train Epoch: 7 [18880/21026 (90%)]\tLoss: 0.084651\n",
      "Train Epoch: 7 [18944/21026 (90%)]\tLoss: 0.084545\n",
      "Train Epoch: 7 [19008/21026 (90%)]\tLoss: 0.087880\n",
      "Train Epoch: 7 [19072/21026 (91%)]\tLoss: 0.095262\n",
      "Train Epoch: 7 [19136/21026 (91%)]\tLoss: 0.087907\n",
      "Train Epoch: 7 [19200/21026 (91%)]\tLoss: 0.083552\n",
      "Train Epoch: 7 [19264/21026 (91%)]\tLoss: 0.082173\n",
      "Train Epoch: 7 [19328/21026 (92%)]\tLoss: 0.082814\n",
      "Train Epoch: 7 [19392/21026 (92%)]\tLoss: 0.080650\n",
      "Train Epoch: 7 [19456/21026 (92%)]\tLoss: 0.077626\n",
      "Train Epoch: 7 [19520/21026 (93%)]\tLoss: 0.080329\n",
      "Train Epoch: 7 [19584/21026 (93%)]\tLoss: 0.084894\n",
      "Train Epoch: 7 [19648/21026 (93%)]\tLoss: 0.086634\n",
      "Train Epoch: 7 [19712/21026 (94%)]\tLoss: 0.089176\n",
      "Train Epoch: 7 [19776/21026 (94%)]\tLoss: 0.080100\n",
      "Train Epoch: 7 [19840/21026 (94%)]\tLoss: 0.079302\n",
      "Train Epoch: 7 [19904/21026 (95%)]\tLoss: 0.082402\n",
      "Train Epoch: 7 [19968/21026 (95%)]\tLoss: 0.077641\n",
      "Train Epoch: 7 [20032/21026 (95%)]\tLoss: 0.080684\n",
      "Train Epoch: 7 [20096/21026 (95%)]\tLoss: 0.081536\n",
      "Train Epoch: 7 [20160/21026 (96%)]\tLoss: 0.086130\n",
      "Train Epoch: 7 [20224/21026 (96%)]\tLoss: 0.082861\n",
      "Train Epoch: 7 [20288/21026 (96%)]\tLoss: 0.079183\n",
      "Train Epoch: 7 [20352/21026 (97%)]\tLoss: 0.083580\n",
      "Train Epoch: 7 [20416/21026 (97%)]\tLoss: 0.080925\n",
      "Train Epoch: 7 [20480/21026 (97%)]\tLoss: 0.080456\n",
      "Train Epoch: 7 [20544/21026 (98%)]\tLoss: 0.087737\n",
      "Train Epoch: 7 [20608/21026 (98%)]\tLoss: 0.083513\n",
      "Train Epoch: 7 [20672/21026 (98%)]\tLoss: 0.079497\n",
      "Train Epoch: 7 [20736/21026 (98%)]\tLoss: 0.076220\n",
      "Train Epoch: 7 [20800/21026 (99%)]\tLoss: 0.091380\n",
      "Train Epoch: 7 [20864/21026 (99%)]\tLoss: 0.088682\n",
      "Train Epoch: 7 [20928/21026 (99%)]\tLoss: 0.089271\n",
      "Train Epoch: 7 [11152/21026 (100%)]\tLoss: 0.087580\n",
      "====> Epoch: 7 Average loss: 0.00245681\n",
      "Train Epoch: 8 [0/21026 (0%)]\tLoss: 0.074959\n",
      "Train Epoch: 8 [64/21026 (0%)]\tLoss: 0.070355\n",
      "Train Epoch: 8 [128/21026 (1%)]\tLoss: 0.076160\n",
      "Train Epoch: 8 [192/21026 (1%)]\tLoss: 0.067859\n",
      "Train Epoch: 8 [256/21026 (1%)]\tLoss: 0.067198\n",
      "Train Epoch: 8 [320/21026 (2%)]\tLoss: 0.066426\n",
      "Train Epoch: 8 [384/21026 (2%)]\tLoss: 0.072968\n",
      "Train Epoch: 8 [448/21026 (2%)]\tLoss: 0.070011\n",
      "Train Epoch: 8 [512/21026 (2%)]\tLoss: 0.072255\n",
      "Train Epoch: 8 [576/21026 (3%)]\tLoss: 0.073191\n",
      "Train Epoch: 8 [640/21026 (3%)]\tLoss: 0.073823\n",
      "Train Epoch: 8 [704/21026 (3%)]\tLoss: 0.074285\n",
      "Train Epoch: 8 [768/21026 (4%)]\tLoss: 0.070648\n",
      "Train Epoch: 8 [832/21026 (4%)]\tLoss: 0.064726\n",
      "Train Epoch: 8 [896/21026 (4%)]\tLoss: 0.070156\n",
      "Train Epoch: 8 [960/21026 (5%)]\tLoss: 0.067006\n",
      "Train Epoch: 8 [1024/21026 (5%)]\tLoss: 0.069807\n",
      "Train Epoch: 8 [1088/21026 (5%)]\tLoss: 0.058942\n",
      "Train Epoch: 8 [1152/21026 (5%)]\tLoss: 0.063435\n",
      "Train Epoch: 8 [1216/21026 (6%)]\tLoss: 0.065061\n",
      "Train Epoch: 8 [1280/21026 (6%)]\tLoss: 0.063535\n",
      "Train Epoch: 8 [1344/21026 (6%)]\tLoss: 0.064441\n",
      "Train Epoch: 8 [1408/21026 (7%)]\tLoss: 0.063682\n",
      "Train Epoch: 8 [1472/21026 (7%)]\tLoss: 0.066671\n",
      "Train Epoch: 8 [1536/21026 (7%)]\tLoss: 0.066516\n",
      "Train Epoch: 8 [1600/21026 (8%)]\tLoss: 0.071991\n",
      "Train Epoch: 8 [1664/21026 (8%)]\tLoss: 0.064505\n",
      "Train Epoch: 8 [1728/21026 (8%)]\tLoss: 0.064402\n",
      "Train Epoch: 8 [1792/21026 (9%)]\tLoss: 0.073245\n",
      "Train Epoch: 8 [1856/21026 (9%)]\tLoss: 0.067831\n",
      "Train Epoch: 8 [1920/21026 (9%)]\tLoss: 0.066116\n",
      "Train Epoch: 8 [1984/21026 (9%)]\tLoss: 0.069308\n",
      "Train Epoch: 8 [2048/21026 (10%)]\tLoss: 0.066270\n",
      "Train Epoch: 8 [2112/21026 (10%)]\tLoss: 0.068490\n",
      "Train Epoch: 8 [2176/21026 (10%)]\tLoss: 0.068270\n",
      "Train Epoch: 8 [2240/21026 (11%)]\tLoss: 0.066946\n",
      "Train Epoch: 8 [2304/21026 (11%)]\tLoss: 0.070779\n",
      "Train Epoch: 8 [2368/21026 (11%)]\tLoss: 0.068211\n",
      "Train Epoch: 8 [2432/21026 (12%)]\tLoss: 0.066623\n",
      "Train Epoch: 8 [2496/21026 (12%)]\tLoss: 0.066230\n",
      "Train Epoch: 8 [2560/21026 (12%)]\tLoss: 0.063580\n",
      "Train Epoch: 8 [2624/21026 (12%)]\tLoss: 0.062801\n",
      "Train Epoch: 8 [2688/21026 (13%)]\tLoss: 0.072015\n",
      "Train Epoch: 8 [2752/21026 (13%)]\tLoss: 0.064663\n",
      "Train Epoch: 8 [2816/21026 (13%)]\tLoss: 0.069946\n",
      "Train Epoch: 8 [2880/21026 (14%)]\tLoss: 0.066598\n",
      "Train Epoch: 8 [2944/21026 (14%)]\tLoss: 0.069280\n",
      "Train Epoch: 8 [3008/21026 (14%)]\tLoss: 0.063986\n",
      "Train Epoch: 8 [3072/21026 (15%)]\tLoss: 0.072394\n",
      "Train Epoch: 8 [3136/21026 (15%)]\tLoss: 0.066061\n",
      "Train Epoch: 8 [3200/21026 (15%)]\tLoss: 0.061979\n",
      "Train Epoch: 8 [3264/21026 (16%)]\tLoss: 0.071226\n",
      "Train Epoch: 8 [3328/21026 (16%)]\tLoss: 0.066043\n",
      "Train Epoch: 8 [3392/21026 (16%)]\tLoss: 0.062740\n",
      "Train Epoch: 8 [3456/21026 (16%)]\tLoss: 0.061129\n",
      "Train Epoch: 8 [3520/21026 (17%)]\tLoss: 0.067010\n",
      "Train Epoch: 8 [3584/21026 (17%)]\tLoss: 0.069118\n",
      "Train Epoch: 8 [3648/21026 (17%)]\tLoss: 0.064964\n",
      "Train Epoch: 8 [3712/21026 (18%)]\tLoss: 0.059202\n",
      "Train Epoch: 8 [3776/21026 (18%)]\tLoss: 0.072072\n",
      "Train Epoch: 8 [3840/21026 (18%)]\tLoss: 0.062800\n",
      "Train Epoch: 8 [3904/21026 (19%)]\tLoss: 0.068462\n",
      "Train Epoch: 8 [3968/21026 (19%)]\tLoss: 0.065292\n",
      "Train Epoch: 8 [4032/21026 (19%)]\tLoss: 0.064953\n",
      "Train Epoch: 8 [4096/21026 (19%)]\tLoss: 0.065371\n",
      "Train Epoch: 8 [4160/21026 (20%)]\tLoss: 0.064283\n",
      "Train Epoch: 8 [4224/21026 (20%)]\tLoss: 0.057003\n",
      "Train Epoch: 8 [4288/21026 (20%)]\tLoss: 0.074346\n",
      "Train Epoch: 8 [4352/21026 (21%)]\tLoss: 0.070705\n",
      "Train Epoch: 8 [4416/21026 (21%)]\tLoss: 0.067978\n",
      "Train Epoch: 8 [4480/21026 (21%)]\tLoss: 0.074175\n",
      "Train Epoch: 8 [4544/21026 (22%)]\tLoss: 0.066605\n",
      "Train Epoch: 8 [4608/21026 (22%)]\tLoss: 0.071257\n",
      "Train Epoch: 8 [4672/21026 (22%)]\tLoss: 0.068075\n",
      "Train Epoch: 8 [4736/21026 (22%)]\tLoss: 0.067726\n",
      "Train Epoch: 8 [4800/21026 (23%)]\tLoss: 0.069167\n",
      "Train Epoch: 8 [4864/21026 (23%)]\tLoss: 0.062588\n",
      "Train Epoch: 8 [4928/21026 (23%)]\tLoss: 0.069369\n",
      "Train Epoch: 8 [4992/21026 (24%)]\tLoss: 0.072228\n",
      "Train Epoch: 8 [5056/21026 (24%)]\tLoss: 0.068652\n",
      "Train Epoch: 8 [5120/21026 (24%)]\tLoss: 0.063002\n",
      "Train Epoch: 8 [5184/21026 (25%)]\tLoss: 0.068066\n",
      "Train Epoch: 8 [5248/21026 (25%)]\tLoss: 0.071164\n",
      "Train Epoch: 8 [5312/21026 (25%)]\tLoss: 0.062697\n",
      "Train Epoch: 8 [5376/21026 (26%)]\tLoss: 0.064874\n",
      "Train Epoch: 8 [5440/21026 (26%)]\tLoss: 0.065140\n",
      "Train Epoch: 8 [5504/21026 (26%)]\tLoss: 0.070448\n",
      "Train Epoch: 8 [5568/21026 (26%)]\tLoss: 0.067130\n",
      "Train Epoch: 8 [5632/21026 (27%)]\tLoss: 0.063189\n",
      "Train Epoch: 8 [5696/21026 (27%)]\tLoss: 0.074219\n",
      "Train Epoch: 8 [5760/21026 (27%)]\tLoss: 0.063950\n",
      "Train Epoch: 8 [5824/21026 (28%)]\tLoss: 0.067509\n",
      "Train Epoch: 8 [5888/21026 (28%)]\tLoss: 0.069466\n",
      "Train Epoch: 8 [5952/21026 (28%)]\tLoss: 0.066158\n",
      "Train Epoch: 8 [6016/21026 (29%)]\tLoss: 0.067420\n",
      "Train Epoch: 8 [6080/21026 (29%)]\tLoss: 0.068563\n",
      "Train Epoch: 8 [6144/21026 (29%)]\tLoss: 0.067507\n",
      "Train Epoch: 8 [6208/21026 (29%)]\tLoss: 0.064201\n",
      "Train Epoch: 8 [6272/21026 (30%)]\tLoss: 0.068548\n",
      "Train Epoch: 8 [6336/21026 (30%)]\tLoss: 0.068588\n",
      "Train Epoch: 8 [6400/21026 (30%)]\tLoss: 0.064790\n",
      "Train Epoch: 8 [6464/21026 (31%)]\tLoss: 0.063309\n",
      "Train Epoch: 8 [6528/21026 (31%)]\tLoss: 0.079815\n",
      "Train Epoch: 8 [6592/21026 (31%)]\tLoss: 0.073520\n",
      "Train Epoch: 8 [6656/21026 (32%)]\tLoss: 0.065152\n",
      "Train Epoch: 8 [6720/21026 (32%)]\tLoss: 0.074572\n",
      "Train Epoch: 8 [6784/21026 (32%)]\tLoss: 0.072933\n",
      "Train Epoch: 8 [6848/21026 (33%)]\tLoss: 0.073726\n",
      "Train Epoch: 8 [6912/21026 (33%)]\tLoss: 0.070770\n",
      "Train Epoch: 8 [6976/21026 (33%)]\tLoss: 0.068424\n",
      "Train Epoch: 8 [7040/21026 (33%)]\tLoss: 0.070438\n",
      "Train Epoch: 8 [7104/21026 (34%)]\tLoss: 0.074372\n",
      "Train Epoch: 8 [7168/21026 (34%)]\tLoss: 0.069117\n",
      "Train Epoch: 8 [7232/21026 (34%)]\tLoss: 0.074066\n",
      "Train Epoch: 8 [7296/21026 (35%)]\tLoss: 0.074492\n",
      "Train Epoch: 8 [7360/21026 (35%)]\tLoss: 0.067235\n",
      "Train Epoch: 8 [7424/21026 (35%)]\tLoss: 0.068664\n",
      "Train Epoch: 8 [7488/21026 (36%)]\tLoss: 0.070413\n",
      "Train Epoch: 8 [7552/21026 (36%)]\tLoss: 0.072662\n",
      "Train Epoch: 8 [7616/21026 (36%)]\tLoss: 0.066955\n",
      "Train Epoch: 8 [7680/21026 (36%)]\tLoss: 0.068932\n",
      "Train Epoch: 8 [7744/21026 (37%)]\tLoss: 0.064169\n",
      "Train Epoch: 8 [7808/21026 (37%)]\tLoss: 0.070564\n",
      "Train Epoch: 8 [7872/21026 (37%)]\tLoss: 0.070655\n",
      "Train Epoch: 8 [7936/21026 (38%)]\tLoss: 0.067686\n",
      "Train Epoch: 8 [8000/21026 (38%)]\tLoss: 0.066706\n",
      "Train Epoch: 8 [8064/21026 (38%)]\tLoss: 0.073186\n",
      "Train Epoch: 8 [8128/21026 (39%)]\tLoss: 0.062055\n",
      "Train Epoch: 8 [8192/21026 (39%)]\tLoss: 0.071558\n",
      "Train Epoch: 8 [8256/21026 (39%)]\tLoss: 0.070166\n",
      "Train Epoch: 8 [8320/21026 (40%)]\tLoss: 0.071659\n",
      "Train Epoch: 8 [8384/21026 (40%)]\tLoss: 0.079232\n",
      "Train Epoch: 8 [8448/21026 (40%)]\tLoss: 0.070426\n",
      "Train Epoch: 8 [8512/21026 (40%)]\tLoss: 0.070639\n",
      "Train Epoch: 8 [8576/21026 (41%)]\tLoss: 0.071604\n",
      "Train Epoch: 8 [8640/21026 (41%)]\tLoss: 0.077941\n",
      "Train Epoch: 8 [8704/21026 (41%)]\tLoss: 0.067722\n",
      "Train Epoch: 8 [8768/21026 (42%)]\tLoss: 0.074342\n",
      "Train Epoch: 8 [8832/21026 (42%)]\tLoss: 0.069912\n",
      "Train Epoch: 8 [8896/21026 (42%)]\tLoss: 0.072857\n",
      "Train Epoch: 8 [8960/21026 (43%)]\tLoss: 0.065922\n",
      "Train Epoch: 8 [9024/21026 (43%)]\tLoss: 0.071898\n",
      "Train Epoch: 8 [9088/21026 (43%)]\tLoss: 0.075726\n",
      "Train Epoch: 8 [9152/21026 (43%)]\tLoss: 0.067244\n",
      "Train Epoch: 8 [9216/21026 (44%)]\tLoss: 0.080036\n",
      "Train Epoch: 8 [9280/21026 (44%)]\tLoss: 0.069667\n",
      "Train Epoch: 8 [9344/21026 (44%)]\tLoss: 0.074093\n",
      "Train Epoch: 8 [9408/21026 (45%)]\tLoss: 0.067517\n",
      "Train Epoch: 8 [9472/21026 (45%)]\tLoss: 0.071212\n",
      "Train Epoch: 8 [9536/21026 (45%)]\tLoss: 0.078435\n",
      "Train Epoch: 8 [9600/21026 (46%)]\tLoss: 0.068643\n",
      "Train Epoch: 8 [9664/21026 (46%)]\tLoss: 0.070340\n",
      "Train Epoch: 8 [9728/21026 (46%)]\tLoss: 0.068628\n",
      "Train Epoch: 8 [9792/21026 (47%)]\tLoss: 0.078649\n",
      "Train Epoch: 8 [9856/21026 (47%)]\tLoss: 0.078305\n",
      "Train Epoch: 8 [9920/21026 (47%)]\tLoss: 0.074530\n",
      "Train Epoch: 8 [9984/21026 (47%)]\tLoss: 0.066146\n",
      "Train Epoch: 8 [10048/21026 (48%)]\tLoss: 0.077030\n",
      "Train Epoch: 8 [10112/21026 (48%)]\tLoss: 0.073688\n",
      "Train Epoch: 8 [10176/21026 (48%)]\tLoss: 0.069240\n",
      "Train Epoch: 8 [10240/21026 (49%)]\tLoss: 0.069802\n",
      "Train Epoch: 8 [10304/21026 (49%)]\tLoss: 0.070169\n",
      "Train Epoch: 8 [10368/21026 (49%)]\tLoss: 0.074480\n",
      "Train Epoch: 8 [10432/21026 (50%)]\tLoss: 0.077428\n",
      "Train Epoch: 8 [10496/21026 (50%)]\tLoss: 0.069991\n",
      "Train Epoch: 8 [10560/21026 (50%)]\tLoss: 0.064604\n",
      "Train Epoch: 8 [10624/21026 (50%)]\tLoss: 0.067085\n",
      "Train Epoch: 8 [10688/21026 (51%)]\tLoss: 0.076464\n",
      "Train Epoch: 8 [10752/21026 (51%)]\tLoss: 0.073296\n",
      "Train Epoch: 8 [10816/21026 (51%)]\tLoss: 0.073529\n",
      "Train Epoch: 8 [10880/21026 (52%)]\tLoss: 0.073990\n",
      "Train Epoch: 8 [10944/21026 (52%)]\tLoss: 0.078811\n",
      "Train Epoch: 8 [11008/21026 (52%)]\tLoss: 0.071199\n",
      "Train Epoch: 8 [11072/21026 (53%)]\tLoss: 0.078403\n",
      "Train Epoch: 8 [11136/21026 (53%)]\tLoss: 0.071433\n",
      "Train Epoch: 8 [11200/21026 (53%)]\tLoss: 0.070858\n",
      "Train Epoch: 8 [11264/21026 (53%)]\tLoss: 0.065897\n",
      "Train Epoch: 8 [11328/21026 (54%)]\tLoss: 0.073664\n",
      "Train Epoch: 8 [11392/21026 (54%)]\tLoss: 0.078152\n",
      "Train Epoch: 8 [11456/21026 (54%)]\tLoss: 0.073771\n",
      "Train Epoch: 8 [11520/21026 (55%)]\tLoss: 0.071296\n",
      "Train Epoch: 8 [11584/21026 (55%)]\tLoss: 0.069709\n",
      "Train Epoch: 8 [11648/21026 (55%)]\tLoss: 0.077503\n",
      "Train Epoch: 8 [11712/21026 (56%)]\tLoss: 0.068458\n",
      "Train Epoch: 8 [11776/21026 (56%)]\tLoss: 0.070445\n",
      "Train Epoch: 8 [11840/21026 (56%)]\tLoss: 0.068484\n",
      "Train Epoch: 8 [11904/21026 (57%)]\tLoss: 0.077644\n",
      "Train Epoch: 8 [11968/21026 (57%)]\tLoss: 0.069811\n",
      "Train Epoch: 8 [12032/21026 (57%)]\tLoss: 0.075579\n",
      "Train Epoch: 8 [12096/21026 (57%)]\tLoss: 0.074894\n",
      "Train Epoch: 8 [12160/21026 (58%)]\tLoss: 0.070425\n",
      "Train Epoch: 8 [12224/21026 (58%)]\tLoss: 0.071767\n",
      "Train Epoch: 8 [12288/21026 (58%)]\tLoss: 0.070541\n",
      "Train Epoch: 8 [12352/21026 (59%)]\tLoss: 0.075900\n",
      "Train Epoch: 8 [12416/21026 (59%)]\tLoss: 0.077376\n",
      "Train Epoch: 8 [12480/21026 (59%)]\tLoss: 0.076763\n",
      "Train Epoch: 8 [12544/21026 (60%)]\tLoss: 0.070509\n",
      "Train Epoch: 8 [12608/21026 (60%)]\tLoss: 0.072169\n",
      "Train Epoch: 8 [12672/21026 (60%)]\tLoss: 0.084005\n",
      "Train Epoch: 8 [12736/21026 (60%)]\tLoss: 0.072756\n",
      "Train Epoch: 8 [12800/21026 (61%)]\tLoss: 0.074929\n",
      "Train Epoch: 8 [12864/21026 (61%)]\tLoss: 0.069493\n",
      "Train Epoch: 8 [12928/21026 (61%)]\tLoss: 0.068525\n",
      "Train Epoch: 8 [12992/21026 (62%)]\tLoss: 0.076883\n",
      "Train Epoch: 8 [13056/21026 (62%)]\tLoss: 0.070731\n",
      "Train Epoch: 8 [13120/21026 (62%)]\tLoss: 0.068000\n",
      "Train Epoch: 8 [13184/21026 (63%)]\tLoss: 0.073996\n",
      "Train Epoch: 8 [13248/21026 (63%)]\tLoss: 0.076845\n",
      "Train Epoch: 8 [13312/21026 (63%)]\tLoss: 0.069665\n",
      "Train Epoch: 8 [13376/21026 (64%)]\tLoss: 0.070982\n",
      "Train Epoch: 8 [13440/21026 (64%)]\tLoss: 0.075811\n",
      "Train Epoch: 8 [13504/21026 (64%)]\tLoss: 0.071016\n",
      "Train Epoch: 8 [13568/21026 (64%)]\tLoss: 0.069650\n",
      "Train Epoch: 8 [13632/21026 (65%)]\tLoss: 0.082912\n",
      "Train Epoch: 8 [13696/21026 (65%)]\tLoss: 0.085897\n",
      "Train Epoch: 8 [13760/21026 (65%)]\tLoss: 0.075350\n",
      "Train Epoch: 8 [13824/21026 (66%)]\tLoss: 0.081547\n",
      "Train Epoch: 8 [13888/21026 (66%)]\tLoss: 0.071244\n",
      "Train Epoch: 8 [13952/21026 (66%)]\tLoss: 0.077687\n",
      "Train Epoch: 8 [14016/21026 (67%)]\tLoss: 0.080870\n",
      "Train Epoch: 8 [14080/21026 (67%)]\tLoss: 0.072645\n",
      "Train Epoch: 8 [14144/21026 (67%)]\tLoss: 0.068208\n",
      "Train Epoch: 8 [14208/21026 (67%)]\tLoss: 0.068515\n",
      "Train Epoch: 8 [14272/21026 (68%)]\tLoss: 0.072370\n",
      "Train Epoch: 8 [14336/21026 (68%)]\tLoss: 0.066906\n",
      "Train Epoch: 8 [14400/21026 (68%)]\tLoss: 0.074439\n",
      "Train Epoch: 8 [14464/21026 (69%)]\tLoss: 0.089923\n",
      "Train Epoch: 8 [14528/21026 (69%)]\tLoss: 0.075901\n",
      "Train Epoch: 8 [14592/21026 (69%)]\tLoss: 0.073558\n",
      "Train Epoch: 8 [14656/21026 (70%)]\tLoss: 0.077974\n",
      "Train Epoch: 8 [14720/21026 (70%)]\tLoss: 0.075989\n",
      "Train Epoch: 8 [14784/21026 (70%)]\tLoss: 0.077631\n",
      "Train Epoch: 8 [14848/21026 (71%)]\tLoss: 0.074941\n",
      "Train Epoch: 8 [14912/21026 (71%)]\tLoss: 0.077102\n",
      "Train Epoch: 8 [14976/21026 (71%)]\tLoss: 0.074291\n",
      "Train Epoch: 8 [15040/21026 (71%)]\tLoss: 0.073091\n",
      "Train Epoch: 8 [15104/21026 (72%)]\tLoss: 0.077254\n",
      "Train Epoch: 8 [15168/21026 (72%)]\tLoss: 0.068607\n",
      "Train Epoch: 8 [15232/21026 (72%)]\tLoss: 0.073177\n",
      "Train Epoch: 8 [15296/21026 (73%)]\tLoss: 0.071365\n",
      "Train Epoch: 8 [15360/21026 (73%)]\tLoss: 0.081424\n",
      "Train Epoch: 8 [15424/21026 (73%)]\tLoss: 0.071424\n",
      "Train Epoch: 8 [15488/21026 (74%)]\tLoss: 0.077316\n",
      "Train Epoch: 8 [15552/21026 (74%)]\tLoss: 0.073697\n",
      "Train Epoch: 8 [15616/21026 (74%)]\tLoss: 0.075256\n",
      "Train Epoch: 8 [15680/21026 (74%)]\tLoss: 0.074714\n",
      "Train Epoch: 8 [15744/21026 (75%)]\tLoss: 0.072954\n",
      "Train Epoch: 8 [15808/21026 (75%)]\tLoss: 0.071774\n",
      "Train Epoch: 8 [15872/21026 (75%)]\tLoss: 0.078209\n",
      "Train Epoch: 8 [15936/21026 (76%)]\tLoss: 0.069613\n",
      "Train Epoch: 8 [16000/21026 (76%)]\tLoss: 0.069117\n",
      "Train Epoch: 8 [16064/21026 (76%)]\tLoss: 0.079878\n",
      "Train Epoch: 8 [16128/21026 (77%)]\tLoss: 0.072420\n",
      "Train Epoch: 8 [16192/21026 (77%)]\tLoss: 0.078027\n",
      "Train Epoch: 8 [16256/21026 (77%)]\tLoss: 0.073379\n",
      "Train Epoch: 8 [16320/21026 (78%)]\tLoss: 0.074726\n",
      "Train Epoch: 8 [16384/21026 (78%)]\tLoss: 0.074594\n",
      "Train Epoch: 8 [16448/21026 (78%)]\tLoss: 0.077519\n",
      "Train Epoch: 8 [16512/21026 (78%)]\tLoss: 0.075730\n",
      "Train Epoch: 8 [16576/21026 (79%)]\tLoss: 0.075662\n",
      "Train Epoch: 8 [16640/21026 (79%)]\tLoss: 0.077144\n",
      "Train Epoch: 8 [16704/21026 (79%)]\tLoss: 0.074473\n",
      "Train Epoch: 8 [16768/21026 (80%)]\tLoss: 0.073249\n",
      "Train Epoch: 8 [16832/21026 (80%)]\tLoss: 0.071578\n",
      "Train Epoch: 8 [16896/21026 (80%)]\tLoss: 0.077376\n",
      "Train Epoch: 8 [16960/21026 (81%)]\tLoss: 0.068692\n",
      "Train Epoch: 8 [17024/21026 (81%)]\tLoss: 0.069665\n",
      "Train Epoch: 8 [17088/21026 (81%)]\tLoss: 0.075152\n",
      "Train Epoch: 8 [17152/21026 (81%)]\tLoss: 0.069711\n",
      "Train Epoch: 8 [17216/21026 (82%)]\tLoss: 0.071044\n",
      "Train Epoch: 8 [17280/21026 (82%)]\tLoss: 0.074826\n",
      "Train Epoch: 8 [17344/21026 (82%)]\tLoss: 0.072036\n",
      "Train Epoch: 8 [17408/21026 (83%)]\tLoss: 0.075938\n",
      "Train Epoch: 8 [17472/21026 (83%)]\tLoss: 0.068959\n",
      "Train Epoch: 8 [17536/21026 (83%)]\tLoss: 0.069574\n",
      "Train Epoch: 8 [17600/21026 (84%)]\tLoss: 0.076309\n",
      "Train Epoch: 8 [17664/21026 (84%)]\tLoss: 0.071584\n",
      "Train Epoch: 8 [17728/21026 (84%)]\tLoss: 0.074505\n",
      "Train Epoch: 8 [17792/21026 (84%)]\tLoss: 0.073401\n",
      "Train Epoch: 8 [17856/21026 (85%)]\tLoss: 0.069250\n",
      "Train Epoch: 8 [17920/21026 (85%)]\tLoss: 0.073863\n",
      "Train Epoch: 8 [17984/21026 (85%)]\tLoss: 0.071790\n",
      "Train Epoch: 8 [18048/21026 (86%)]\tLoss: 0.083232\n",
      "Train Epoch: 8 [18112/21026 (86%)]\tLoss: 0.072584\n",
      "Train Epoch: 8 [18176/21026 (86%)]\tLoss: 0.072317\n",
      "Train Epoch: 8 [18240/21026 (87%)]\tLoss: 0.074564\n",
      "Train Epoch: 8 [18304/21026 (87%)]\tLoss: 0.075651\n",
      "Train Epoch: 8 [18368/21026 (87%)]\tLoss: 0.072189\n",
      "Train Epoch: 8 [18432/21026 (88%)]\tLoss: 0.066776\n",
      "Train Epoch: 8 [18496/21026 (88%)]\tLoss: 0.076729\n",
      "Train Epoch: 8 [18560/21026 (88%)]\tLoss: 0.075138\n",
      "Train Epoch: 8 [18624/21026 (88%)]\tLoss: 0.077726\n",
      "Train Epoch: 8 [18688/21026 (89%)]\tLoss: 0.074768\n",
      "Train Epoch: 8 [18752/21026 (89%)]\tLoss: 0.072721\n",
      "Train Epoch: 8 [18816/21026 (89%)]\tLoss: 0.077164\n",
      "Train Epoch: 8 [18880/21026 (90%)]\tLoss: 0.077121\n",
      "Train Epoch: 8 [18944/21026 (90%)]\tLoss: 0.074076\n",
      "Train Epoch: 8 [19008/21026 (90%)]\tLoss: 0.072553\n",
      "Train Epoch: 8 [19072/21026 (91%)]\tLoss: 0.071584\n",
      "Train Epoch: 8 [19136/21026 (91%)]\tLoss: 0.079888\n",
      "Train Epoch: 8 [19200/21026 (91%)]\tLoss: 0.070851\n",
      "Train Epoch: 8 [19264/21026 (91%)]\tLoss: 0.069907\n",
      "Train Epoch: 8 [19328/21026 (92%)]\tLoss: 0.077801\n",
      "Train Epoch: 8 [19392/21026 (92%)]\tLoss: 0.076590\n",
      "Train Epoch: 8 [19456/21026 (92%)]\tLoss: 0.076748\n",
      "Train Epoch: 8 [19520/21026 (93%)]\tLoss: 0.069882\n",
      "Train Epoch: 8 [19584/21026 (93%)]\tLoss: 0.076776\n",
      "Train Epoch: 8 [19648/21026 (93%)]\tLoss: 0.085464\n",
      "Train Epoch: 8 [19712/21026 (94%)]\tLoss: 0.073515\n",
      "Train Epoch: 8 [19776/21026 (94%)]\tLoss: 0.077387\n",
      "Train Epoch: 8 [19840/21026 (94%)]\tLoss: 0.074849\n",
      "Train Epoch: 8 [19904/21026 (95%)]\tLoss: 0.068259\n",
      "Train Epoch: 8 [19968/21026 (95%)]\tLoss: 0.076265\n",
      "Train Epoch: 8 [20032/21026 (95%)]\tLoss: 0.078793\n",
      "Train Epoch: 8 [20096/21026 (95%)]\tLoss: 0.078604\n",
      "Train Epoch: 8 [20160/21026 (96%)]\tLoss: 0.069415\n",
      "Train Epoch: 8 [20224/21026 (96%)]\tLoss: 0.075043\n",
      "Train Epoch: 8 [20288/21026 (96%)]\tLoss: 0.071977\n",
      "Train Epoch: 8 [20352/21026 (97%)]\tLoss: 0.076397\n",
      "Train Epoch: 8 [20416/21026 (97%)]\tLoss: 0.076175\n",
      "Train Epoch: 8 [20480/21026 (97%)]\tLoss: 0.077379\n",
      "Train Epoch: 8 [20544/21026 (98%)]\tLoss: 0.068084\n",
      "Train Epoch: 8 [20608/21026 (98%)]\tLoss: 0.076431\n",
      "Train Epoch: 8 [20672/21026 (98%)]\tLoss: 0.085490\n",
      "Train Epoch: 8 [20736/21026 (98%)]\tLoss: 0.071546\n",
      "Train Epoch: 8 [20800/21026 (99%)]\tLoss: 0.075455\n",
      "Train Epoch: 8 [20864/21026 (99%)]\tLoss: 0.068587\n",
      "Train Epoch: 8 [20928/21026 (99%)]\tLoss: 0.076681\n",
      "Train Epoch: 8 [11152/21026 (100%)]\tLoss: 0.073460\n",
      "====> Epoch: 8 Average loss: 0.00223918\n",
      "Train Epoch: 9 [0/21026 (0%)]\tLoss: 0.059473\n",
      "Train Epoch: 9 [64/21026 (0%)]\tLoss: 0.065175\n",
      "Train Epoch: 9 [128/21026 (1%)]\tLoss: 0.062258\n",
      "Train Epoch: 9 [192/21026 (1%)]\tLoss: 0.066780\n",
      "Train Epoch: 9 [256/21026 (1%)]\tLoss: 0.060417\n",
      "Train Epoch: 9 [320/21026 (2%)]\tLoss: 0.066023\n",
      "Train Epoch: 9 [384/21026 (2%)]\tLoss: 0.064269\n",
      "Train Epoch: 9 [448/21026 (2%)]\tLoss: 0.060937\n",
      "Train Epoch: 9 [512/21026 (2%)]\tLoss: 0.064381\n",
      "Train Epoch: 9 [576/21026 (3%)]\tLoss: 0.065929\n",
      "Train Epoch: 9 [640/21026 (3%)]\tLoss: 0.064844\n",
      "Train Epoch: 9 [704/21026 (3%)]\tLoss: 0.065827\n",
      "Train Epoch: 9 [768/21026 (4%)]\tLoss: 0.071054\n",
      "Train Epoch: 9 [832/21026 (4%)]\tLoss: 0.064009\n",
      "Train Epoch: 9 [896/21026 (4%)]\tLoss: 0.064845\n",
      "Train Epoch: 9 [960/21026 (5%)]\tLoss: 0.061645\n",
      "Train Epoch: 9 [1024/21026 (5%)]\tLoss: 0.066157\n",
      "Train Epoch: 9 [1088/21026 (5%)]\tLoss: 0.060542\n",
      "Train Epoch: 9 [1152/21026 (5%)]\tLoss: 0.066528\n",
      "Train Epoch: 9 [1216/21026 (6%)]\tLoss: 0.064798\n",
      "Train Epoch: 9 [1280/21026 (6%)]\tLoss: 0.064302\n",
      "Train Epoch: 9 [1344/21026 (6%)]\tLoss: 0.068423\n",
      "Train Epoch: 9 [1408/21026 (7%)]\tLoss: 0.066542\n",
      "Train Epoch: 9 [1472/21026 (7%)]\tLoss: 0.061621\n",
      "Train Epoch: 9 [1536/21026 (7%)]\tLoss: 0.078699\n",
      "Train Epoch: 9 [1600/21026 (8%)]\tLoss: 0.068668\n",
      "Train Epoch: 9 [1664/21026 (8%)]\tLoss: 0.063738\n",
      "Train Epoch: 9 [1728/21026 (8%)]\tLoss: 0.064893\n",
      "Train Epoch: 9 [1792/21026 (9%)]\tLoss: 0.066390\n",
      "Train Epoch: 9 [1856/21026 (9%)]\tLoss: 0.063659\n",
      "Train Epoch: 9 [1920/21026 (9%)]\tLoss: 0.064918\n",
      "Train Epoch: 9 [1984/21026 (9%)]\tLoss: 0.066973\n",
      "Train Epoch: 9 [2048/21026 (10%)]\tLoss: 0.065041\n",
      "Train Epoch: 9 [2112/21026 (10%)]\tLoss: 0.064635\n",
      "Train Epoch: 9 [2176/21026 (10%)]\tLoss: 0.061592\n",
      "Train Epoch: 9 [2240/21026 (11%)]\tLoss: 0.064114\n",
      "Train Epoch: 9 [2304/21026 (11%)]\tLoss: 0.065932\n",
      "Train Epoch: 9 [2368/21026 (11%)]\tLoss: 0.058925\n",
      "Train Epoch: 9 [2432/21026 (12%)]\tLoss: 0.062882\n",
      "Train Epoch: 9 [2496/21026 (12%)]\tLoss: 0.067409\n",
      "Train Epoch: 9 [2560/21026 (12%)]\tLoss: 0.070694\n",
      "Train Epoch: 9 [2624/21026 (12%)]\tLoss: 0.061585\n",
      "Train Epoch: 9 [2688/21026 (13%)]\tLoss: 0.062771\n",
      "Train Epoch: 9 [2752/21026 (13%)]\tLoss: 0.062321\n",
      "Train Epoch: 9 [2816/21026 (13%)]\tLoss: 0.072595\n",
      "Train Epoch: 9 [2880/21026 (14%)]\tLoss: 0.059159\n",
      "Train Epoch: 9 [2944/21026 (14%)]\tLoss: 0.067100\n",
      "Train Epoch: 9 [3008/21026 (14%)]\tLoss: 0.063818\n",
      "Train Epoch: 9 [3072/21026 (15%)]\tLoss: 0.060596\n",
      "Train Epoch: 9 [3136/21026 (15%)]\tLoss: 0.061625\n",
      "Train Epoch: 9 [3200/21026 (15%)]\tLoss: 0.064191\n",
      "Train Epoch: 9 [3264/21026 (16%)]\tLoss: 0.063041\n",
      "Train Epoch: 9 [3328/21026 (16%)]\tLoss: 0.057739\n",
      "Train Epoch: 9 [3392/21026 (16%)]\tLoss: 0.057473\n",
      "Train Epoch: 9 [3456/21026 (16%)]\tLoss: 0.065359\n",
      "Train Epoch: 9 [3520/21026 (17%)]\tLoss: 0.060095\n",
      "Train Epoch: 9 [3584/21026 (17%)]\tLoss: 0.060709\n",
      "Train Epoch: 9 [3648/21026 (17%)]\tLoss: 0.062958\n",
      "Train Epoch: 9 [3712/21026 (18%)]\tLoss: 0.063056\n",
      "Train Epoch: 9 [3776/21026 (18%)]\tLoss: 0.064908\n",
      "Train Epoch: 9 [3840/21026 (18%)]\tLoss: 0.064801\n",
      "Train Epoch: 9 [3904/21026 (19%)]\tLoss: 0.061549\n",
      "Train Epoch: 9 [3968/21026 (19%)]\tLoss: 0.065322\n",
      "Train Epoch: 9 [4032/21026 (19%)]\tLoss: 0.063548\n",
      "Train Epoch: 9 [4096/21026 (19%)]\tLoss: 0.064162\n",
      "Train Epoch: 9 [4160/21026 (20%)]\tLoss: 0.073257\n",
      "Train Epoch: 9 [4224/21026 (20%)]\tLoss: 0.067596\n",
      "Train Epoch: 9 [4288/21026 (20%)]\tLoss: 0.062729\n",
      "Train Epoch: 9 [4352/21026 (21%)]\tLoss: 0.059548\n",
      "Train Epoch: 9 [4416/21026 (21%)]\tLoss: 0.065127\n",
      "Train Epoch: 9 [4480/21026 (21%)]\tLoss: 0.066040\n",
      "Train Epoch: 9 [4544/21026 (22%)]\tLoss: 0.063737\n",
      "Train Epoch: 9 [4608/21026 (22%)]\tLoss: 0.064951\n",
      "Train Epoch: 9 [4672/21026 (22%)]\tLoss: 0.063520\n",
      "Train Epoch: 9 [4736/21026 (22%)]\tLoss: 0.064234\n",
      "Train Epoch: 9 [4800/21026 (23%)]\tLoss: 0.062064\n",
      "Train Epoch: 9 [4864/21026 (23%)]\tLoss: 0.063791\n",
      "Train Epoch: 9 [4928/21026 (23%)]\tLoss: 0.063038\n",
      "Train Epoch: 9 [4992/21026 (24%)]\tLoss: 0.065605\n",
      "Train Epoch: 9 [5056/21026 (24%)]\tLoss: 0.061598\n",
      "Train Epoch: 9 [5120/21026 (24%)]\tLoss: 0.071746\n",
      "Train Epoch: 9 [5184/21026 (25%)]\tLoss: 0.061239\n",
      "Train Epoch: 9 [5248/21026 (25%)]\tLoss: 0.066835\n",
      "Train Epoch: 9 [5312/21026 (25%)]\tLoss: 0.064283\n",
      "Train Epoch: 9 [5376/21026 (26%)]\tLoss: 0.061388\n",
      "Train Epoch: 9 [5440/21026 (26%)]\tLoss: 0.064564\n",
      "Train Epoch: 9 [5504/21026 (26%)]\tLoss: 0.057781\n",
      "Train Epoch: 9 [5568/21026 (26%)]\tLoss: 0.062216\n",
      "Train Epoch: 9 [5632/21026 (27%)]\tLoss: 0.057319\n",
      "Train Epoch: 9 [5696/21026 (27%)]\tLoss: 0.065046\n",
      "Train Epoch: 9 [5760/21026 (27%)]\tLoss: 0.066838\n",
      "Train Epoch: 9 [5824/21026 (28%)]\tLoss: 0.060516\n",
      "Train Epoch: 9 [5888/21026 (28%)]\tLoss: 0.065268\n",
      "Train Epoch: 9 [5952/21026 (28%)]\tLoss: 0.062610\n",
      "Train Epoch: 9 [6016/21026 (29%)]\tLoss: 0.059347\n",
      "Train Epoch: 9 [6080/21026 (29%)]\tLoss: 0.061774\n",
      "Train Epoch: 9 [6144/21026 (29%)]\tLoss: 0.063358\n",
      "Train Epoch: 9 [6208/21026 (29%)]\tLoss: 0.062562\n",
      "Train Epoch: 9 [6272/21026 (30%)]\tLoss: 0.070569\n",
      "Train Epoch: 9 [6336/21026 (30%)]\tLoss: 0.062715\n",
      "Train Epoch: 9 [6400/21026 (30%)]\tLoss: 0.065387\n",
      "Train Epoch: 9 [6464/21026 (31%)]\tLoss: 0.063294\n",
      "Train Epoch: 9 [6528/21026 (31%)]\tLoss: 0.059591\n",
      "Train Epoch: 9 [6592/21026 (31%)]\tLoss: 0.070532\n",
      "Train Epoch: 9 [6656/21026 (32%)]\tLoss: 0.061554\n",
      "Train Epoch: 9 [6720/21026 (32%)]\tLoss: 0.056600\n",
      "Train Epoch: 9 [6784/21026 (32%)]\tLoss: 0.062358\n",
      "Train Epoch: 9 [6848/21026 (33%)]\tLoss: 0.061814\n",
      "Train Epoch: 9 [6912/21026 (33%)]\tLoss: 0.061942\n",
      "Train Epoch: 9 [6976/21026 (33%)]\tLoss: 0.061583\n",
      "Train Epoch: 9 [7040/21026 (33%)]\tLoss: 0.058870\n",
      "Train Epoch: 9 [7104/21026 (34%)]\tLoss: 0.061791\n",
      "Train Epoch: 9 [7168/21026 (34%)]\tLoss: 0.064306\n",
      "Train Epoch: 9 [7232/21026 (34%)]\tLoss: 0.064523\n",
      "Train Epoch: 9 [7296/21026 (35%)]\tLoss: 0.063759\n",
      "Train Epoch: 9 [7360/21026 (35%)]\tLoss: 0.065135\n",
      "Train Epoch: 9 [7424/21026 (35%)]\tLoss: 0.062013\n",
      "Train Epoch: 9 [7488/21026 (36%)]\tLoss: 0.069238\n",
      "Train Epoch: 9 [7552/21026 (36%)]\tLoss: 0.065412\n",
      "Train Epoch: 9 [7616/21026 (36%)]\tLoss: 0.064098\n",
      "Train Epoch: 9 [7680/21026 (36%)]\tLoss: 0.070040\n",
      "Train Epoch: 9 [7744/21026 (37%)]\tLoss: 0.063597\n",
      "Train Epoch: 9 [7808/21026 (37%)]\tLoss: 0.060191\n",
      "Train Epoch: 9 [7872/21026 (37%)]\tLoss: 0.061604\n",
      "Train Epoch: 9 [7936/21026 (38%)]\tLoss: 0.066446\n",
      "Train Epoch: 9 [8000/21026 (38%)]\tLoss: 0.058458\n",
      "Train Epoch: 9 [8064/21026 (38%)]\tLoss: 0.064254\n",
      "Train Epoch: 9 [8128/21026 (39%)]\tLoss: 0.065911\n",
      "Train Epoch: 9 [8192/21026 (39%)]\tLoss: 0.062945\n",
      "Train Epoch: 9 [8256/21026 (39%)]\tLoss: 0.056972\n",
      "Train Epoch: 9 [8320/21026 (40%)]\tLoss: 0.061056\n",
      "Train Epoch: 9 [8384/21026 (40%)]\tLoss: 0.061177\n",
      "Train Epoch: 9 [8448/21026 (40%)]\tLoss: 0.062954\n",
      "Train Epoch: 9 [8512/21026 (40%)]\tLoss: 0.062272\n",
      "Train Epoch: 9 [8576/21026 (41%)]\tLoss: 0.067118\n",
      "Train Epoch: 9 [8640/21026 (41%)]\tLoss: 0.061073\n",
      "Train Epoch: 9 [8704/21026 (41%)]\tLoss: 0.065740\n",
      "Train Epoch: 9 [8768/21026 (42%)]\tLoss: 0.065949\n",
      "Train Epoch: 9 [8832/21026 (42%)]\tLoss: 0.067040\n",
      "Train Epoch: 9 [8896/21026 (42%)]\tLoss: 0.062343\n",
      "Train Epoch: 9 [8960/21026 (43%)]\tLoss: 0.066639\n",
      "Train Epoch: 9 [9024/21026 (43%)]\tLoss: 0.063354\n",
      "Train Epoch: 9 [9088/21026 (43%)]\tLoss: 0.069074\n",
      "Train Epoch: 9 [9152/21026 (43%)]\tLoss: 0.064587\n",
      "Train Epoch: 9 [9216/21026 (44%)]\tLoss: 0.063358\n",
      "Train Epoch: 9 [9280/21026 (44%)]\tLoss: 0.067022\n",
      "Train Epoch: 9 [9344/21026 (44%)]\tLoss: 0.062644\n",
      "Train Epoch: 9 [9408/21026 (45%)]\tLoss: 0.067095\n",
      "Train Epoch: 9 [9472/21026 (45%)]\tLoss: 0.061347\n",
      "Train Epoch: 9 [9536/21026 (45%)]\tLoss: 0.070870\n",
      "Train Epoch: 9 [9600/21026 (46%)]\tLoss: 0.063004\n",
      "Train Epoch: 9 [9664/21026 (46%)]\tLoss: 0.065028\n",
      "Train Epoch: 9 [9728/21026 (46%)]\tLoss: 0.068308\n",
      "Train Epoch: 9 [9792/21026 (47%)]\tLoss: 0.066167\n",
      "Train Epoch: 9 [9856/21026 (47%)]\tLoss: 0.068055\n",
      "Train Epoch: 9 [9920/21026 (47%)]\tLoss: 0.060712\n",
      "Train Epoch: 9 [9984/21026 (47%)]\tLoss: 0.069067\n",
      "Train Epoch: 9 [10048/21026 (48%)]\tLoss: 0.066437\n",
      "Train Epoch: 9 [10112/21026 (48%)]\tLoss: 0.069875\n",
      "Train Epoch: 9 [10176/21026 (48%)]\tLoss: 0.058801\n",
      "Train Epoch: 9 [10240/21026 (49%)]\tLoss: 0.067349\n",
      "Train Epoch: 9 [10304/21026 (49%)]\tLoss: 0.059979\n",
      "Train Epoch: 9 [10368/21026 (49%)]\tLoss: 0.064893\n",
      "Train Epoch: 9 [10432/21026 (50%)]\tLoss: 0.067001\n",
      "Train Epoch: 9 [10496/21026 (50%)]\tLoss: 0.067045\n",
      "Train Epoch: 9 [10560/21026 (50%)]\tLoss: 0.062974\n",
      "Train Epoch: 9 [10624/21026 (50%)]\tLoss: 0.064729\n",
      "Train Epoch: 9 [10688/21026 (51%)]\tLoss: 0.065029\n",
      "Train Epoch: 9 [10752/21026 (51%)]\tLoss: 0.075415\n",
      "Train Epoch: 9 [10816/21026 (51%)]\tLoss: 0.071819\n",
      "Train Epoch: 9 [10880/21026 (52%)]\tLoss: 0.065614\n",
      "Train Epoch: 9 [10944/21026 (52%)]\tLoss: 0.067691\n",
      "Train Epoch: 9 [11008/21026 (52%)]\tLoss: 0.063845\n",
      "Train Epoch: 9 [11072/21026 (53%)]\tLoss: 0.066579\n",
      "Train Epoch: 9 [11136/21026 (53%)]\tLoss: 0.067016\n",
      "Train Epoch: 9 [11200/21026 (53%)]\tLoss: 0.065967\n",
      "Train Epoch: 9 [11264/21026 (53%)]\tLoss: 0.062958\n",
      "Train Epoch: 9 [11328/21026 (54%)]\tLoss: 0.069737\n",
      "Train Epoch: 9 [11392/21026 (54%)]\tLoss: 0.068317\n",
      "Train Epoch: 9 [11456/21026 (54%)]\tLoss: 0.075571\n",
      "Train Epoch: 9 [11520/21026 (55%)]\tLoss: 0.068503\n",
      "Train Epoch: 9 [11584/21026 (55%)]\tLoss: 0.063409\n",
      "Train Epoch: 9 [11648/21026 (55%)]\tLoss: 0.068195\n",
      "Train Epoch: 9 [11712/21026 (56%)]\tLoss: 0.066236\n",
      "Train Epoch: 9 [11776/21026 (56%)]\tLoss: 0.067767\n",
      "Train Epoch: 9 [11840/21026 (56%)]\tLoss: 0.067081\n",
      "Train Epoch: 9 [11904/21026 (57%)]\tLoss: 0.064544\n",
      "Train Epoch: 9 [11968/21026 (57%)]\tLoss: 0.069500\n",
      "Train Epoch: 9 [12032/21026 (57%)]\tLoss: 0.062741\n",
      "Train Epoch: 9 [12096/21026 (57%)]\tLoss: 0.068782\n",
      "Train Epoch: 9 [12160/21026 (58%)]\tLoss: 0.070161\n",
      "Train Epoch: 9 [12224/21026 (58%)]\tLoss: 0.061539\n",
      "Train Epoch: 9 [12288/21026 (58%)]\tLoss: 0.064768\n",
      "Train Epoch: 9 [12352/21026 (59%)]\tLoss: 0.064418\n",
      "Train Epoch: 9 [12416/21026 (59%)]\tLoss: 0.064546\n",
      "Train Epoch: 9 [12480/21026 (59%)]\tLoss: 0.071887\n",
      "Train Epoch: 9 [12544/21026 (60%)]\tLoss: 0.073931\n",
      "Train Epoch: 9 [12608/21026 (60%)]\tLoss: 0.075329\n",
      "Train Epoch: 9 [12672/21026 (60%)]\tLoss: 0.068352\n",
      "Train Epoch: 9 [12736/21026 (60%)]\tLoss: 0.077198\n",
      "Train Epoch: 9 [12800/21026 (61%)]\tLoss: 0.070386\n",
      "Train Epoch: 9 [12864/21026 (61%)]\tLoss: 0.075956\n",
      "Train Epoch: 9 [12928/21026 (61%)]\tLoss: 0.075753\n",
      "Train Epoch: 9 [12992/21026 (62%)]\tLoss: 0.064414\n",
      "Train Epoch: 9 [13056/21026 (62%)]\tLoss: 0.063661\n",
      "Train Epoch: 9 [13120/21026 (62%)]\tLoss: 0.071640\n",
      "Train Epoch: 9 [13184/21026 (63%)]\tLoss: 0.077336\n",
      "Train Epoch: 9 [13248/21026 (63%)]\tLoss: 0.062766\n",
      "Train Epoch: 9 [13312/21026 (63%)]\tLoss: 0.076149\n",
      "Train Epoch: 9 [13376/21026 (64%)]\tLoss: 0.079616\n",
      "Train Epoch: 9 [13440/21026 (64%)]\tLoss: 0.067599\n",
      "Train Epoch: 9 [13504/21026 (64%)]\tLoss: 0.079786\n",
      "Train Epoch: 9 [13568/21026 (64%)]\tLoss: 0.074022\n",
      "Train Epoch: 9 [13632/21026 (65%)]\tLoss: 0.071655\n",
      "Train Epoch: 9 [13696/21026 (65%)]\tLoss: 0.074415\n",
      "Train Epoch: 9 [13760/21026 (65%)]\tLoss: 0.069312\n",
      "Train Epoch: 9 [13824/21026 (66%)]\tLoss: 0.070255\n",
      "Train Epoch: 9 [13888/21026 (66%)]\tLoss: 0.065520\n",
      "Train Epoch: 9 [13952/21026 (66%)]\tLoss: 0.072358\n",
      "Train Epoch: 9 [14016/21026 (67%)]\tLoss: 0.071419\n",
      "Train Epoch: 9 [14080/21026 (67%)]\tLoss: 0.073721\n",
      "Train Epoch: 9 [14144/21026 (67%)]\tLoss: 0.073495\n",
      "Train Epoch: 9 [14208/21026 (67%)]\tLoss: 0.074318\n",
      "Train Epoch: 9 [14272/21026 (68%)]\tLoss: 0.070800\n",
      "Train Epoch: 9 [14336/21026 (68%)]\tLoss: 0.076345\n",
      "Train Epoch: 9 [14400/21026 (68%)]\tLoss: 0.068105\n",
      "Train Epoch: 9 [14464/21026 (69%)]\tLoss: 0.071688\n",
      "Train Epoch: 9 [14528/21026 (69%)]\tLoss: 0.078564\n",
      "Train Epoch: 9 [14592/21026 (69%)]\tLoss: 0.071362\n",
      "Train Epoch: 9 [14656/21026 (70%)]\tLoss: 0.068230\n",
      "Train Epoch: 9 [14720/21026 (70%)]\tLoss: 0.073709\n",
      "Train Epoch: 9 [14784/21026 (70%)]\tLoss: 0.075494\n",
      "Train Epoch: 9 [14848/21026 (71%)]\tLoss: 0.071178\n",
      "Train Epoch: 9 [14912/21026 (71%)]\tLoss: 0.073392\n",
      "Train Epoch: 9 [14976/21026 (71%)]\tLoss: 0.074532\n",
      "Train Epoch: 9 [15040/21026 (71%)]\tLoss: 0.070450\n",
      "Train Epoch: 9 [15104/21026 (72%)]\tLoss: 0.074258\n",
      "Train Epoch: 9 [15168/21026 (72%)]\tLoss: 0.072329\n",
      "Train Epoch: 9 [15232/21026 (72%)]\tLoss: 0.068670\n",
      "Train Epoch: 9 [15296/21026 (73%)]\tLoss: 0.072785\n",
      "Train Epoch: 9 [15360/21026 (73%)]\tLoss: 0.064946\n",
      "Train Epoch: 9 [15424/21026 (73%)]\tLoss: 0.072141\n",
      "Train Epoch: 9 [15488/21026 (74%)]\tLoss: 0.076315\n",
      "Train Epoch: 9 [15552/21026 (74%)]\tLoss: 0.073313\n",
      "Train Epoch: 9 [15616/21026 (74%)]\tLoss: 0.071249\n",
      "Train Epoch: 9 [15680/21026 (74%)]\tLoss: 0.065805\n",
      "Train Epoch: 9 [15744/21026 (75%)]\tLoss: 0.066120\n",
      "Train Epoch: 9 [15808/21026 (75%)]\tLoss: 0.070436\n",
      "Train Epoch: 9 [15872/21026 (75%)]\tLoss: 0.072458\n",
      "Train Epoch: 9 [15936/21026 (76%)]\tLoss: 0.068472\n",
      "Train Epoch: 9 [16000/21026 (76%)]\tLoss: 0.062958\n",
      "Train Epoch: 9 [16064/21026 (76%)]\tLoss: 0.075679\n",
      "Train Epoch: 9 [16128/21026 (77%)]\tLoss: 0.071861\n",
      "Train Epoch: 9 [16192/21026 (77%)]\tLoss: 0.068149\n",
      "Train Epoch: 9 [16256/21026 (77%)]\tLoss: 0.071443\n",
      "Train Epoch: 9 [16320/21026 (78%)]\tLoss: 0.071881\n",
      "Train Epoch: 9 [16384/21026 (78%)]\tLoss: 0.063316\n",
      "Train Epoch: 9 [16448/21026 (78%)]\tLoss: 0.069132\n",
      "Train Epoch: 9 [16512/21026 (78%)]\tLoss: 0.075722\n",
      "Train Epoch: 9 [16576/21026 (79%)]\tLoss: 0.073056\n",
      "Train Epoch: 9 [16640/21026 (79%)]\tLoss: 0.071012\n",
      "Train Epoch: 9 [16704/21026 (79%)]\tLoss: 0.080090\n",
      "Train Epoch: 9 [16768/21026 (80%)]\tLoss: 0.071457\n",
      "Train Epoch: 9 [16832/21026 (80%)]\tLoss: 0.071191\n",
      "Train Epoch: 9 [16896/21026 (80%)]\tLoss: 0.074437\n",
      "Train Epoch: 9 [16960/21026 (81%)]\tLoss: 0.076143\n",
      "Train Epoch: 9 [17024/21026 (81%)]\tLoss: 0.066305\n",
      "Train Epoch: 9 [17088/21026 (81%)]\tLoss: 0.077815\n",
      "Train Epoch: 9 [17152/21026 (81%)]\tLoss: 0.066183\n",
      "Train Epoch: 9 [17216/21026 (82%)]\tLoss: 0.069401\n",
      "Train Epoch: 9 [17280/21026 (82%)]\tLoss: 0.077661\n",
      "Train Epoch: 9 [17344/21026 (82%)]\tLoss: 0.077427\n",
      "Train Epoch: 9 [17408/21026 (83%)]\tLoss: 0.067395\n",
      "Train Epoch: 9 [17472/21026 (83%)]\tLoss: 0.078645\n",
      "Train Epoch: 9 [17536/21026 (83%)]\tLoss: 0.073353\n",
      "Train Epoch: 9 [17600/21026 (84%)]\tLoss: 0.074588\n",
      "Train Epoch: 9 [17664/21026 (84%)]\tLoss: 0.070072\n",
      "Train Epoch: 9 [17728/21026 (84%)]\tLoss: 0.073226\n",
      "Train Epoch: 9 [17792/21026 (84%)]\tLoss: 0.061545\n",
      "Train Epoch: 9 [17856/21026 (85%)]\tLoss: 0.070739\n",
      "Train Epoch: 9 [17920/21026 (85%)]\tLoss: 0.070287\n",
      "Train Epoch: 9 [17984/21026 (85%)]\tLoss: 0.067939\n",
      "Train Epoch: 9 [18048/21026 (86%)]\tLoss: 0.071295\n",
      "Train Epoch: 9 [18112/21026 (86%)]\tLoss: 0.068662\n",
      "Train Epoch: 9 [18176/21026 (86%)]\tLoss: 0.072622\n",
      "Train Epoch: 9 [18240/21026 (87%)]\tLoss: 0.081730\n",
      "Train Epoch: 9 [18304/21026 (87%)]\tLoss: 0.068296\n",
      "Train Epoch: 9 [18368/21026 (87%)]\tLoss: 0.076648\n",
      "Train Epoch: 9 [18432/21026 (88%)]\tLoss: 0.069349\n",
      "Train Epoch: 9 [18496/21026 (88%)]\tLoss: 0.069091\n",
      "Train Epoch: 9 [18560/21026 (88%)]\tLoss: 0.071812\n",
      "Train Epoch: 9 [18624/21026 (88%)]\tLoss: 0.075869\n",
      "Train Epoch: 9 [18688/21026 (89%)]\tLoss: 0.065966\n",
      "Train Epoch: 9 [18752/21026 (89%)]\tLoss: 0.073043\n",
      "Train Epoch: 9 [18816/21026 (89%)]\tLoss: 0.071053\n",
      "Train Epoch: 9 [18880/21026 (90%)]\tLoss: 0.070177\n",
      "Train Epoch: 9 [18944/21026 (90%)]\tLoss: 0.074173\n",
      "Train Epoch: 9 [19008/21026 (90%)]\tLoss: 0.074131\n",
      "Train Epoch: 9 [19072/21026 (91%)]\tLoss: 0.071622\n",
      "Train Epoch: 9 [19136/21026 (91%)]\tLoss: 0.070697\n",
      "Train Epoch: 9 [19200/21026 (91%)]\tLoss: 0.071508\n",
      "Train Epoch: 9 [19264/21026 (91%)]\tLoss: 0.068534\n",
      "Train Epoch: 9 [19328/21026 (92%)]\tLoss: 0.073220\n",
      "Train Epoch: 9 [19392/21026 (92%)]\tLoss: 0.080380\n",
      "Train Epoch: 9 [19456/21026 (92%)]\tLoss: 0.070705\n",
      "Train Epoch: 9 [19520/21026 (93%)]\tLoss: 0.071731\n",
      "Train Epoch: 9 [19584/21026 (93%)]\tLoss: 0.071936\n",
      "Train Epoch: 9 [19648/21026 (93%)]\tLoss: 0.071768\n",
      "Train Epoch: 9 [19712/21026 (94%)]\tLoss: 0.075715\n",
      "Train Epoch: 9 [19776/21026 (94%)]\tLoss: 0.072310\n",
      "Train Epoch: 9 [19840/21026 (94%)]\tLoss: 0.075768\n",
      "Train Epoch: 9 [19904/21026 (95%)]\tLoss: 0.070323\n",
      "Train Epoch: 9 [19968/21026 (95%)]\tLoss: 0.080361\n",
      "Train Epoch: 9 [20032/21026 (95%)]\tLoss: 0.076449\n",
      "Train Epoch: 9 [20096/21026 (95%)]\tLoss: 0.074884\n",
      "Train Epoch: 9 [20160/21026 (96%)]\tLoss: 0.073467\n",
      "Train Epoch: 9 [20224/21026 (96%)]\tLoss: 0.072463\n",
      "Train Epoch: 9 [20288/21026 (96%)]\tLoss: 0.070230\n",
      "Train Epoch: 9 [20352/21026 (97%)]\tLoss: 0.069364\n",
      "Train Epoch: 9 [20416/21026 (97%)]\tLoss: 0.074464\n",
      "Train Epoch: 9 [20480/21026 (97%)]\tLoss: 0.067909\n",
      "Train Epoch: 9 [20544/21026 (98%)]\tLoss: 0.078822\n",
      "Train Epoch: 9 [20608/21026 (98%)]\tLoss: 0.068537\n",
      "Train Epoch: 9 [20672/21026 (98%)]\tLoss: 0.077801\n",
      "Train Epoch: 9 [20736/21026 (98%)]\tLoss: 0.083213\n",
      "Train Epoch: 9 [20800/21026 (99%)]\tLoss: 0.079615\n",
      "Train Epoch: 9 [20864/21026 (99%)]\tLoss: 0.075115\n",
      "Train Epoch: 9 [20928/21026 (99%)]\tLoss: 0.080182\n",
      "Train Epoch: 9 [11152/21026 (100%)]\tLoss: 0.077892\n",
      "====> Epoch: 9 Average loss: 0.00211945\n",
      "Train Epoch: 10 [0/21026 (0%)]\tLoss: 0.065526\n",
      "Train Epoch: 10 [64/21026 (0%)]\tLoss: 0.062037\n",
      "Train Epoch: 10 [128/21026 (1%)]\tLoss: 0.061864\n",
      "Train Epoch: 10 [192/21026 (1%)]\tLoss: 0.060624\n",
      "Train Epoch: 10 [256/21026 (1%)]\tLoss: 0.059162\n",
      "Train Epoch: 10 [320/21026 (2%)]\tLoss: 0.062533\n",
      "Train Epoch: 10 [384/21026 (2%)]\tLoss: 0.056285\n",
      "Train Epoch: 10 [448/21026 (2%)]\tLoss: 0.058812\n",
      "Train Epoch: 10 [512/21026 (2%)]\tLoss: 0.060712\n",
      "Train Epoch: 10 [576/21026 (3%)]\tLoss: 0.058857\n",
      "Train Epoch: 10 [640/21026 (3%)]\tLoss: 0.054177\n",
      "Train Epoch: 10 [704/21026 (3%)]\tLoss: 0.056684\n",
      "Train Epoch: 10 [768/21026 (4%)]\tLoss: 0.055694\n",
      "Train Epoch: 10 [832/21026 (4%)]\tLoss: 0.067439\n",
      "Train Epoch: 10 [896/21026 (4%)]\tLoss: 0.061041\n",
      "Train Epoch: 10 [960/21026 (5%)]\tLoss: 0.057743\n",
      "Train Epoch: 10 [1024/21026 (5%)]\tLoss: 0.058572\n",
      "Train Epoch: 10 [1088/21026 (5%)]\tLoss: 0.064085\n",
      "Train Epoch: 10 [1152/21026 (5%)]\tLoss: 0.064832\n",
      "Train Epoch: 10 [1216/21026 (6%)]\tLoss: 0.060379\n",
      "Train Epoch: 10 [1280/21026 (6%)]\tLoss: 0.062922\n",
      "Train Epoch: 10 [1344/21026 (6%)]\tLoss: 0.059386\n",
      "Train Epoch: 10 [1408/21026 (7%)]\tLoss: 0.056725\n",
      "Train Epoch: 10 [1472/21026 (7%)]\tLoss: 0.060963\n",
      "Train Epoch: 10 [1536/21026 (7%)]\tLoss: 0.055731\n",
      "Train Epoch: 10 [1600/21026 (8%)]\tLoss: 0.055327\n",
      "Train Epoch: 10 [1664/21026 (8%)]\tLoss: 0.057838\n",
      "Train Epoch: 10 [1728/21026 (8%)]\tLoss: 0.057237\n",
      "Train Epoch: 10 [1792/21026 (9%)]\tLoss: 0.056211\n",
      "Train Epoch: 10 [1856/21026 (9%)]\tLoss: 0.057421\n",
      "Train Epoch: 10 [1920/21026 (9%)]\tLoss: 0.059971\n",
      "Train Epoch: 10 [1984/21026 (9%)]\tLoss: 0.059901\n",
      "Train Epoch: 10 [2048/21026 (10%)]\tLoss: 0.059448\n",
      "Train Epoch: 10 [2112/21026 (10%)]\tLoss: 0.058636\n",
      "Train Epoch: 10 [2176/21026 (10%)]\tLoss: 0.060394\n",
      "Train Epoch: 10 [2240/21026 (11%)]\tLoss: 0.056310\n",
      "Train Epoch: 10 [2304/21026 (11%)]\tLoss: 0.060744\n",
      "Train Epoch: 10 [2368/21026 (11%)]\tLoss: 0.058634\n",
      "Train Epoch: 10 [2432/21026 (12%)]\tLoss: 0.060521\n",
      "Train Epoch: 10 [2496/21026 (12%)]\tLoss: 0.054827\n",
      "Train Epoch: 10 [2560/21026 (12%)]\tLoss: 0.058680\n",
      "Train Epoch: 10 [2624/21026 (12%)]\tLoss: 0.056261\n",
      "Train Epoch: 10 [2688/21026 (13%)]\tLoss: 0.060394\n",
      "Train Epoch: 10 [2752/21026 (13%)]\tLoss: 0.060775\n",
      "Train Epoch: 10 [2816/21026 (13%)]\tLoss: 0.056927\n",
      "Train Epoch: 10 [2880/21026 (14%)]\tLoss: 0.061111\n",
      "Train Epoch: 10 [2944/21026 (14%)]\tLoss: 0.059467\n",
      "Train Epoch: 10 [3008/21026 (14%)]\tLoss: 0.062077\n",
      "Train Epoch: 10 [3072/21026 (15%)]\tLoss: 0.059051\n",
      "Train Epoch: 10 [3136/21026 (15%)]\tLoss: 0.059602\n",
      "Train Epoch: 10 [3200/21026 (15%)]\tLoss: 0.064213\n",
      "Train Epoch: 10 [3264/21026 (16%)]\tLoss: 0.058968\n",
      "Train Epoch: 10 [3328/21026 (16%)]\tLoss: 0.054540\n",
      "Train Epoch: 10 [3392/21026 (16%)]\tLoss: 0.058886\n",
      "Train Epoch: 10 [3456/21026 (16%)]\tLoss: 0.059218\n",
      "Train Epoch: 10 [3520/21026 (17%)]\tLoss: 0.057238\n",
      "Train Epoch: 10 [3584/21026 (17%)]\tLoss: 0.062913\n",
      "Train Epoch: 10 [3648/21026 (17%)]\tLoss: 0.061675\n",
      "Train Epoch: 10 [3712/21026 (18%)]\tLoss: 0.052543\n",
      "Train Epoch: 10 [3776/21026 (18%)]\tLoss: 0.056387\n",
      "Train Epoch: 10 [3840/21026 (18%)]\tLoss: 0.058086\n",
      "Train Epoch: 10 [3904/21026 (19%)]\tLoss: 0.059541\n",
      "Train Epoch: 10 [3968/21026 (19%)]\tLoss: 0.058819\n",
      "Train Epoch: 10 [4032/21026 (19%)]\tLoss: 0.056908\n",
      "Train Epoch: 10 [4096/21026 (19%)]\tLoss: 0.061630\n",
      "Train Epoch: 10 [4160/21026 (20%)]\tLoss: 0.058591\n",
      "Train Epoch: 10 [4224/21026 (20%)]\tLoss: 0.057057\n",
      "Train Epoch: 10 [4288/21026 (20%)]\tLoss: 0.058184\n",
      "Train Epoch: 10 [4352/21026 (21%)]\tLoss: 0.058299\n",
      "Train Epoch: 10 [4416/21026 (21%)]\tLoss: 0.056336\n",
      "Train Epoch: 10 [4480/21026 (21%)]\tLoss: 0.054229\n",
      "Train Epoch: 10 [4544/21026 (22%)]\tLoss: 0.058458\n",
      "Train Epoch: 10 [4608/21026 (22%)]\tLoss: 0.054250\n",
      "Train Epoch: 10 [4672/21026 (22%)]\tLoss: 0.056611\n",
      "Train Epoch: 10 [4736/21026 (22%)]\tLoss: 0.066035\n",
      "Train Epoch: 10 [4800/21026 (23%)]\tLoss: 0.054123\n",
      "Train Epoch: 10 [4864/21026 (23%)]\tLoss: 0.055442\n",
      "Train Epoch: 10 [4928/21026 (23%)]\tLoss: 0.056789\n",
      "Train Epoch: 10 [4992/21026 (24%)]\tLoss: 0.057488\n",
      "Train Epoch: 10 [5056/21026 (24%)]\tLoss: 0.056507\n",
      "Train Epoch: 10 [5120/21026 (24%)]\tLoss: 0.059692\n",
      "Train Epoch: 10 [5184/21026 (25%)]\tLoss: 0.058684\n",
      "Train Epoch: 10 [5248/21026 (25%)]\tLoss: 0.058736\n",
      "Train Epoch: 10 [5312/21026 (25%)]\tLoss: 0.050647\n",
      "Train Epoch: 10 [5376/21026 (26%)]\tLoss: 0.059028\n",
      "Train Epoch: 10 [5440/21026 (26%)]\tLoss: 0.059070\n",
      "Train Epoch: 10 [5504/21026 (26%)]\tLoss: 0.062281\n",
      "Train Epoch: 10 [5568/21026 (26%)]\tLoss: 0.054169\n",
      "Train Epoch: 10 [5632/21026 (27%)]\tLoss: 0.055440\n",
      "Train Epoch: 10 [5696/21026 (27%)]\tLoss: 0.055899\n",
      "Train Epoch: 10 [5760/21026 (27%)]\tLoss: 0.058614\n",
      "Train Epoch: 10 [5824/21026 (28%)]\tLoss: 0.057898\n",
      "Train Epoch: 10 [5888/21026 (28%)]\tLoss: 0.055977\n",
      "Train Epoch: 10 [5952/21026 (28%)]\tLoss: 0.056098\n",
      "Train Epoch: 10 [6016/21026 (29%)]\tLoss: 0.056807\n",
      "Train Epoch: 10 [6080/21026 (29%)]\tLoss: 0.054718\n",
      "Train Epoch: 10 [6144/21026 (29%)]\tLoss: 0.057905\n",
      "Train Epoch: 10 [6208/21026 (29%)]\tLoss: 0.055481\n",
      "Train Epoch: 10 [6272/21026 (30%)]\tLoss: 0.052695\n",
      "Train Epoch: 10 [6336/21026 (30%)]\tLoss: 0.055276\n",
      "Train Epoch: 10 [6400/21026 (30%)]\tLoss: 0.054152\n",
      "Train Epoch: 10 [6464/21026 (31%)]\tLoss: 0.056209\n",
      "Train Epoch: 10 [6528/21026 (31%)]\tLoss: 0.060211\n",
      "Train Epoch: 10 [6592/21026 (31%)]\tLoss: 0.057698\n",
      "Train Epoch: 10 [6656/21026 (32%)]\tLoss: 0.057369\n",
      "Train Epoch: 10 [6720/21026 (32%)]\tLoss: 0.059647\n",
      "Train Epoch: 10 [6784/21026 (32%)]\tLoss: 0.056429\n",
      "Train Epoch: 10 [6848/21026 (33%)]\tLoss: 0.059882\n",
      "Train Epoch: 10 [6912/21026 (33%)]\tLoss: 0.060975\n",
      "Train Epoch: 10 [6976/21026 (33%)]\tLoss: 0.057054\n",
      "Train Epoch: 10 [7040/21026 (33%)]\tLoss: 0.051107\n",
      "Train Epoch: 10 [7104/21026 (34%)]\tLoss: 0.063202\n",
      "Train Epoch: 10 [7168/21026 (34%)]\tLoss: 0.055486\n",
      "Train Epoch: 10 [7232/21026 (34%)]\tLoss: 0.061405\n",
      "Train Epoch: 10 [7296/21026 (35%)]\tLoss: 0.058440\n",
      "Train Epoch: 10 [7360/21026 (35%)]\tLoss: 0.059640\n",
      "Train Epoch: 10 [7424/21026 (35%)]\tLoss: 0.054128\n",
      "Train Epoch: 10 [7488/21026 (36%)]\tLoss: 0.059945\n",
      "Train Epoch: 10 [7552/21026 (36%)]\tLoss: 0.062859\n",
      "Train Epoch: 10 [7616/21026 (36%)]\tLoss: 0.053863\n",
      "Train Epoch: 10 [7680/21026 (36%)]\tLoss: 0.065321\n",
      "Train Epoch: 10 [7744/21026 (37%)]\tLoss: 0.057290\n",
      "Train Epoch: 10 [7808/21026 (37%)]\tLoss: 0.059226\n",
      "Train Epoch: 10 [7872/21026 (37%)]\tLoss: 0.058916\n",
      "Train Epoch: 10 [7936/21026 (38%)]\tLoss: 0.054083\n",
      "Train Epoch: 10 [8000/21026 (38%)]\tLoss: 0.061572\n",
      "Train Epoch: 10 [8064/21026 (38%)]\tLoss: 0.054980\n",
      "Train Epoch: 10 [8128/21026 (39%)]\tLoss: 0.060121\n",
      "Train Epoch: 10 [8192/21026 (39%)]\tLoss: 0.056395\n",
      "Train Epoch: 10 [8256/21026 (39%)]\tLoss: 0.062152\n",
      "Train Epoch: 10 [8320/21026 (40%)]\tLoss: 0.054072\n",
      "Train Epoch: 10 [8384/21026 (40%)]\tLoss: 0.053781\n",
      "Train Epoch: 10 [8448/21026 (40%)]\tLoss: 0.060169\n",
      "Train Epoch: 10 [8512/21026 (40%)]\tLoss: 0.056560\n",
      "Train Epoch: 10 [8576/21026 (41%)]\tLoss: 0.058566\n",
      "Train Epoch: 10 [8640/21026 (41%)]\tLoss: 0.064299\n",
      "Train Epoch: 10 [8704/21026 (41%)]\tLoss: 0.066192\n",
      "Train Epoch: 10 [8768/21026 (42%)]\tLoss: 0.062982\n",
      "Train Epoch: 10 [8832/21026 (42%)]\tLoss: 0.062883\n",
      "Train Epoch: 10 [8896/21026 (42%)]\tLoss: 0.061396\n",
      "Train Epoch: 10 [8960/21026 (43%)]\tLoss: 0.059595\n",
      "Train Epoch: 10 [9024/21026 (43%)]\tLoss: 0.062056\n",
      "Train Epoch: 10 [9088/21026 (43%)]\tLoss: 0.064301\n",
      "Train Epoch: 10 [9152/21026 (43%)]\tLoss: 0.062914\n",
      "Train Epoch: 10 [9216/21026 (44%)]\tLoss: 0.058402\n",
      "Train Epoch: 10 [9280/21026 (44%)]\tLoss: 0.059512\n",
      "Train Epoch: 10 [9344/21026 (44%)]\tLoss: 0.059345\n",
      "Train Epoch: 10 [9408/21026 (45%)]\tLoss: 0.061216\n",
      "Train Epoch: 10 [9472/21026 (45%)]\tLoss: 0.057925\n",
      "Train Epoch: 10 [9536/21026 (45%)]\tLoss: 0.062260\n",
      "Train Epoch: 10 [9600/21026 (46%)]\tLoss: 0.057613\n",
      "Train Epoch: 10 [9664/21026 (46%)]\tLoss: 0.064025\n",
      "Train Epoch: 10 [9728/21026 (46%)]\tLoss: 0.060943\n",
      "Train Epoch: 10 [9792/21026 (47%)]\tLoss: 0.060172\n",
      "Train Epoch: 10 [9856/21026 (47%)]\tLoss: 0.065277\n",
      "Train Epoch: 10 [9920/21026 (47%)]\tLoss: 0.055468\n",
      "Train Epoch: 10 [9984/21026 (47%)]\tLoss: 0.065420\n",
      "Train Epoch: 10 [10048/21026 (48%)]\tLoss: 0.057240\n",
      "Train Epoch: 10 [10112/21026 (48%)]\tLoss: 0.057873\n",
      "Train Epoch: 10 [10176/21026 (48%)]\tLoss: 0.067451\n",
      "Train Epoch: 10 [10240/21026 (49%)]\tLoss: 0.057411\n",
      "Train Epoch: 10 [10304/21026 (49%)]\tLoss: 0.064547\n",
      "Train Epoch: 10 [10368/21026 (49%)]\tLoss: 0.056491\n",
      "Train Epoch: 10 [10432/21026 (50%)]\tLoss: 0.055052\n",
      "Train Epoch: 10 [10496/21026 (50%)]\tLoss: 0.061358\n",
      "Train Epoch: 10 [10560/21026 (50%)]\tLoss: 0.058070\n",
      "Train Epoch: 10 [10624/21026 (50%)]\tLoss: 0.062315\n",
      "Train Epoch: 10 [10688/21026 (51%)]\tLoss: 0.068004\n",
      "Train Epoch: 10 [10752/21026 (51%)]\tLoss: 0.072433\n",
      "Train Epoch: 10 [10816/21026 (51%)]\tLoss: 0.064840\n",
      "Train Epoch: 10 [10880/21026 (52%)]\tLoss: 0.067001\n",
      "Train Epoch: 10 [10944/21026 (52%)]\tLoss: 0.063951\n",
      "Train Epoch: 10 [11008/21026 (52%)]\tLoss: 0.062175\n",
      "Train Epoch: 10 [11072/21026 (53%)]\tLoss: 0.061746\n",
      "Train Epoch: 10 [11136/21026 (53%)]\tLoss: 0.060884\n",
      "Train Epoch: 10 [11200/21026 (53%)]\tLoss: 0.063636\n",
      "Train Epoch: 10 [11264/21026 (53%)]\tLoss: 0.069811\n",
      "Train Epoch: 10 [11328/21026 (54%)]\tLoss: 0.062836\n",
      "Train Epoch: 10 [11392/21026 (54%)]\tLoss: 0.066962\n",
      "Train Epoch: 10 [11456/21026 (54%)]\tLoss: 0.062044\n",
      "Train Epoch: 10 [11520/21026 (55%)]\tLoss: 0.066793\n",
      "Train Epoch: 10 [11584/21026 (55%)]\tLoss: 0.061481\n",
      "Train Epoch: 10 [11648/21026 (55%)]\tLoss: 0.068222\n",
      "Train Epoch: 10 [11712/21026 (56%)]\tLoss: 0.061548\n",
      "Train Epoch: 10 [11776/21026 (56%)]\tLoss: 0.062542\n",
      "Train Epoch: 10 [11840/21026 (56%)]\tLoss: 0.060345\n",
      "Train Epoch: 10 [11904/21026 (57%)]\tLoss: 0.068675\n",
      "Train Epoch: 10 [11968/21026 (57%)]\tLoss: 0.062711\n",
      "Train Epoch: 10 [12032/21026 (57%)]\tLoss: 0.062348\n",
      "Train Epoch: 10 [12096/21026 (57%)]\tLoss: 0.065148\n",
      "Train Epoch: 10 [12160/21026 (58%)]\tLoss: 0.066485\n",
      "Train Epoch: 10 [12224/21026 (58%)]\tLoss: 0.063127\n",
      "Train Epoch: 10 [12288/21026 (58%)]\tLoss: 0.061541\n",
      "Train Epoch: 10 [12352/21026 (59%)]\tLoss: 0.068430\n",
      "Train Epoch: 10 [12416/21026 (59%)]\tLoss: 0.065268\n",
      "Train Epoch: 10 [12480/21026 (59%)]\tLoss: 0.067542\n",
      "Train Epoch: 10 [12544/21026 (60%)]\tLoss: 0.066214\n",
      "Train Epoch: 10 [12608/21026 (60%)]\tLoss: 0.065609\n",
      "Train Epoch: 10 [12672/21026 (60%)]\tLoss: 0.061314\n",
      "Train Epoch: 10 [12736/21026 (60%)]\tLoss: 0.063306\n",
      "Train Epoch: 10 [12800/21026 (61%)]\tLoss: 0.062814\n",
      "Train Epoch: 10 [12864/21026 (61%)]\tLoss: 0.055849\n",
      "Train Epoch: 10 [12928/21026 (61%)]\tLoss: 0.062313\n",
      "Train Epoch: 10 [12992/21026 (62%)]\tLoss: 0.069680\n",
      "Train Epoch: 10 [13056/21026 (62%)]\tLoss: 0.064087\n",
      "Train Epoch: 10 [13120/21026 (62%)]\tLoss: 0.058771\n",
      "Train Epoch: 10 [13184/21026 (63%)]\tLoss: 0.069780\n",
      "Train Epoch: 10 [13248/21026 (63%)]\tLoss: 0.060229\n",
      "Train Epoch: 10 [13312/21026 (63%)]\tLoss: 0.066714\n",
      "Train Epoch: 10 [13376/21026 (64%)]\tLoss: 0.062093\n",
      "Train Epoch: 10 [13440/21026 (64%)]\tLoss: 0.068497\n",
      "Train Epoch: 10 [13504/21026 (64%)]\tLoss: 0.064769\n",
      "Train Epoch: 10 [13568/21026 (64%)]\tLoss: 0.058690\n",
      "Train Epoch: 10 [13632/21026 (65%)]\tLoss: 0.061475\n",
      "Train Epoch: 10 [13696/21026 (65%)]\tLoss: 0.064765\n",
      "Train Epoch: 10 [13760/21026 (65%)]\tLoss: 0.061327\n",
      "Train Epoch: 10 [13824/21026 (66%)]\tLoss: 0.070577\n",
      "Train Epoch: 10 [13888/21026 (66%)]\tLoss: 0.064408\n",
      "Train Epoch: 10 [13952/21026 (66%)]\tLoss: 0.059841\n",
      "Train Epoch: 10 [14016/21026 (67%)]\tLoss: 0.079173\n",
      "Train Epoch: 10 [14080/21026 (67%)]\tLoss: 0.069317\n",
      "Train Epoch: 10 [14144/21026 (67%)]\tLoss: 0.059526\n",
      "Train Epoch: 10 [14208/21026 (67%)]\tLoss: 0.067765\n",
      "Train Epoch: 10 [14272/21026 (68%)]\tLoss: 0.067749\n",
      "Train Epoch: 10 [14336/21026 (68%)]\tLoss: 0.063041\n",
      "Train Epoch: 10 [14400/21026 (68%)]\tLoss: 0.064038\n",
      "Train Epoch: 10 [14464/21026 (69%)]\tLoss: 0.062816\n",
      "Train Epoch: 10 [14528/21026 (69%)]\tLoss: 0.065051\n",
      "Train Epoch: 10 [14592/21026 (69%)]\tLoss: 0.060110\n",
      "Train Epoch: 10 [14656/21026 (70%)]\tLoss: 0.067950\n",
      "Train Epoch: 10 [14720/21026 (70%)]\tLoss: 0.061673\n",
      "Train Epoch: 10 [14784/21026 (70%)]\tLoss: 0.059705\n",
      "Train Epoch: 10 [14848/21026 (71%)]\tLoss: 0.060518\n",
      "Train Epoch: 10 [14912/21026 (71%)]\tLoss: 0.065270\n",
      "Train Epoch: 10 [14976/21026 (71%)]\tLoss: 0.059580\n",
      "Train Epoch: 10 [15040/21026 (71%)]\tLoss: 0.062873\n",
      "Train Epoch: 10 [15104/21026 (72%)]\tLoss: 0.066971\n",
      "Train Epoch: 10 [15168/21026 (72%)]\tLoss: 0.057073\n",
      "Train Epoch: 10 [15232/21026 (72%)]\tLoss: 0.058904\n",
      "Train Epoch: 10 [15296/21026 (73%)]\tLoss: 0.059145\n",
      "Train Epoch: 10 [15360/21026 (73%)]\tLoss: 0.060008\n",
      "Train Epoch: 10 [15424/21026 (73%)]\tLoss: 0.062616\n",
      "Train Epoch: 10 [15488/21026 (74%)]\tLoss: 0.057538\n",
      "Train Epoch: 10 [15552/21026 (74%)]\tLoss: 0.056023\n",
      "Train Epoch: 10 [15616/21026 (74%)]\tLoss: 0.059609\n",
      "Train Epoch: 10 [15680/21026 (74%)]\tLoss: 0.059574\n",
      "Train Epoch: 10 [15744/21026 (75%)]\tLoss: 0.057438\n",
      "Train Epoch: 10 [15808/21026 (75%)]\tLoss: 0.058224\n",
      "Train Epoch: 10 [15872/21026 (75%)]\tLoss: 0.058429\n",
      "Train Epoch: 10 [15936/21026 (76%)]\tLoss: 0.068437\n",
      "Train Epoch: 10 [16000/21026 (76%)]\tLoss: 0.067886\n",
      "Train Epoch: 10 [16064/21026 (76%)]\tLoss: 0.062076\n",
      "Train Epoch: 10 [16128/21026 (77%)]\tLoss: 0.062581\n",
      "Train Epoch: 10 [16192/21026 (77%)]\tLoss: 0.065818\n",
      "Train Epoch: 10 [16256/21026 (77%)]\tLoss: 0.060507\n",
      "Train Epoch: 10 [16320/21026 (78%)]\tLoss: 0.064323\n",
      "Train Epoch: 10 [16384/21026 (78%)]\tLoss: 0.061579\n",
      "Train Epoch: 10 [16448/21026 (78%)]\tLoss: 0.061896\n",
      "Train Epoch: 10 [16512/21026 (78%)]\tLoss: 0.059793\n",
      "Train Epoch: 10 [16576/21026 (79%)]\tLoss: 0.061311\n",
      "Train Epoch: 10 [16640/21026 (79%)]\tLoss: 0.059991\n",
      "Train Epoch: 10 [16704/21026 (79%)]\tLoss: 0.064126\n",
      "Train Epoch: 10 [16768/21026 (80%)]\tLoss: 0.064615\n",
      "Train Epoch: 10 [16832/21026 (80%)]\tLoss: 0.061634\n",
      "Train Epoch: 10 [16896/21026 (80%)]\tLoss: 0.058943\n",
      "Train Epoch: 10 [16960/21026 (81%)]\tLoss: 0.062533\n",
      "Train Epoch: 10 [17024/21026 (81%)]\tLoss: 0.063891\n",
      "Train Epoch: 10 [17088/21026 (81%)]\tLoss: 0.065532\n",
      "Train Epoch: 10 [17152/21026 (81%)]\tLoss: 0.064459\n",
      "Train Epoch: 10 [17216/21026 (82%)]\tLoss: 0.060702\n",
      "Train Epoch: 10 [17280/21026 (82%)]\tLoss: 0.059795\n",
      "Train Epoch: 10 [17344/21026 (82%)]\tLoss: 0.066809\n",
      "Train Epoch: 10 [17408/21026 (83%)]\tLoss: 0.061083\n",
      "Train Epoch: 10 [17472/21026 (83%)]\tLoss: 0.059257\n",
      "Train Epoch: 10 [17536/21026 (83%)]\tLoss: 0.060012\n",
      "Train Epoch: 10 [17600/21026 (84%)]\tLoss: 0.056269\n",
      "Train Epoch: 10 [17664/21026 (84%)]\tLoss: 0.055205\n",
      "Train Epoch: 10 [17728/21026 (84%)]\tLoss: 0.064487\n",
      "Train Epoch: 10 [17792/21026 (84%)]\tLoss: 0.065742\n",
      "Train Epoch: 10 [17856/21026 (85%)]\tLoss: 0.061683\n",
      "Train Epoch: 10 [17920/21026 (85%)]\tLoss: 0.057754\n",
      "Train Epoch: 10 [17984/21026 (85%)]\tLoss: 0.065959\n",
      "Train Epoch: 10 [18048/21026 (86%)]\tLoss: 0.066949\n",
      "Train Epoch: 10 [18112/21026 (86%)]\tLoss: 0.060070\n",
      "Train Epoch: 10 [18176/21026 (86%)]\tLoss: 0.062483\n",
      "Train Epoch: 10 [18240/21026 (87%)]\tLoss: 0.066627\n",
      "Train Epoch: 10 [18304/21026 (87%)]\tLoss: 0.065843\n",
      "Train Epoch: 10 [18368/21026 (87%)]\tLoss: 0.064862\n",
      "Train Epoch: 10 [18432/21026 (88%)]\tLoss: 0.056267\n",
      "Train Epoch: 10 [18496/21026 (88%)]\tLoss: 0.063299\n",
      "Train Epoch: 10 [18560/21026 (88%)]\tLoss: 0.061478\n",
      "Train Epoch: 10 [18624/21026 (88%)]\tLoss: 0.063753\n",
      "Train Epoch: 10 [18688/21026 (89%)]\tLoss: 0.068624\n",
      "Train Epoch: 10 [18752/21026 (89%)]\tLoss: 0.062218\n",
      "Train Epoch: 10 [18816/21026 (89%)]\tLoss: 0.068843\n",
      "Train Epoch: 10 [18880/21026 (90%)]\tLoss: 0.065895\n",
      "Train Epoch: 10 [18944/21026 (90%)]\tLoss: 0.066314\n",
      "Train Epoch: 10 [19008/21026 (90%)]\tLoss: 0.057664\n",
      "Train Epoch: 10 [19072/21026 (91%)]\tLoss: 0.066283\n",
      "Train Epoch: 10 [19136/21026 (91%)]\tLoss: 0.061385\n",
      "Train Epoch: 10 [19200/21026 (91%)]\tLoss: 0.067856\n",
      "Train Epoch: 10 [19264/21026 (91%)]\tLoss: 0.058211\n",
      "Train Epoch: 10 [19328/21026 (92%)]\tLoss: 0.062361\n",
      "Train Epoch: 10 [19392/21026 (92%)]\tLoss: 0.067027\n",
      "Train Epoch: 10 [19456/21026 (92%)]\tLoss: 0.061658\n",
      "Train Epoch: 10 [19520/21026 (93%)]\tLoss: 0.067250\n",
      "Train Epoch: 10 [19584/21026 (93%)]\tLoss: 0.067029\n",
      "Train Epoch: 10 [19648/21026 (93%)]\tLoss: 0.066925\n",
      "Train Epoch: 10 [19712/21026 (94%)]\tLoss: 0.064094\n",
      "Train Epoch: 10 [19776/21026 (94%)]\tLoss: 0.061802\n",
      "Train Epoch: 10 [19840/21026 (94%)]\tLoss: 0.062129\n",
      "Train Epoch: 10 [19904/21026 (95%)]\tLoss: 0.068302\n",
      "Train Epoch: 10 [19968/21026 (95%)]\tLoss: 0.063603\n",
      "Train Epoch: 10 [20032/21026 (95%)]\tLoss: 0.075116\n",
      "Train Epoch: 10 [20096/21026 (95%)]\tLoss: 0.063011\n",
      "Train Epoch: 10 [20160/21026 (96%)]\tLoss: 0.066712\n",
      "Train Epoch: 10 [20224/21026 (96%)]\tLoss: 0.061837\n",
      "Train Epoch: 10 [20288/21026 (96%)]\tLoss: 0.059833\n",
      "Train Epoch: 10 [20352/21026 (97%)]\tLoss: 0.066530\n",
      "Train Epoch: 10 [20416/21026 (97%)]\tLoss: 0.063112\n",
      "Train Epoch: 10 [20480/21026 (97%)]\tLoss: 0.065554\n",
      "Train Epoch: 10 [20544/21026 (98%)]\tLoss: 0.075768\n",
      "Train Epoch: 10 [20608/21026 (98%)]\tLoss: 0.065541\n",
      "Train Epoch: 10 [20672/21026 (98%)]\tLoss: 0.061996\n",
      "Train Epoch: 10 [20736/21026 (98%)]\tLoss: 0.064712\n",
      "Train Epoch: 10 [20800/21026 (99%)]\tLoss: 0.067682\n",
      "Train Epoch: 10 [20864/21026 (99%)]\tLoss: 0.070752\n",
      "Train Epoch: 10 [20928/21026 (99%)]\tLoss: 0.058077\n",
      "Train Epoch: 10 [11152/21026 (100%)]\tLoss: 0.069225\n",
      "====> Epoch: 10 Average loss: 0.00191564\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    global step\n",
    "    servant.train()\n",
    "    writer = SummaryWriter()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        org_domain = data[0]\n",
    "        new_domain = data[1]\n",
    "        batch_size = new_domain.shape[0]\n",
    "        with torch.no_grad():\n",
    "            _,latent_true = master(org_domain)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        _,latent_hat = servant(new_domain)\n",
    "        loss = loss_fn(latent_hat,latent_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_size * batch_idx, len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))\n",
    "\n",
    "        writer.add_scalar(\"AE Loss\", loss.item() / len(data[0]),step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "          epoch, avg_loss))\n",
    "    writer.flush()\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    servant.eval()\n",
    "    writer = SummaryWriter()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            org_domain = data[0]\n",
    "            new_domain = data[1]\n",
    "            _,latent_true = master(org_domain)\n",
    "            _,latent_hat = servant(new_domain)\n",
    "            test_loss += loss_fn(latent_hat,latent_true).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.8f}'.format(test_loss))\n",
    "    writer.add_scalar(\"AE test Loss\", test_loss,epoch)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "for epoch in range(1, 10 + 1):\n",
    "    step = 0\n",
    "    smallest_loss = 1000\n",
    "    avg_loss = train(epoch)\n",
    "    if avg_loss < smallest_loss:\n",
    "        torch.save(servant.state_dict(), './AE_params/dry-wet_2.best')\n",
    "    #test(epoch)\n",
    "    torch.save(servant.state_dict(), './AE_params/dry-wet_2.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = random.randint(1,20000)\n",
    "data = train_data.__getitem__(num)\n",
    "imgs = []\n",
    "org = data[0]\n",
    "new = data[1]\n",
    "\n",
    "_,latent = master(data[0].reshape(1,3,128,128))\n",
    "_,new_lat = servant(data[1].reshape(1,3,128,128))\n",
    "imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "imgs.append(Image.fromarray(new.numpy().transpose(1,2,0)))\n",
    "\n",
    "sem = master.decode(new_lat).detach().cpu().argmax(dim=1)\n",
    "\n",
    "imgs.append(generate_semantic_im(org,master))\n",
    "imgs.append(replace(sem.numpy().reshape(1,128,128).transpose(1,2,0)))\n",
    "fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "for i, img in enumerate(imgs):\n",
    "    axs[0, i].imshow(np.asarray(img))\n",
    "    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.44303402e-02, 2.06218101e-03, 2.28911429e-03, ...,\n",
       "         2.27603111e-02, 1.40220314e-01, 1.25275359e-01],\n",
       "        [3.41317966e-04, 2.14852334e-05, 2.73792966e-06, ...,\n",
       "         4.40190546e-03, 2.41087424e-03, 4.87988479e-02],\n",
       "        [3.49047710e-04, 1.61600758e-06, 4.78934953e-06, ...,\n",
       "         1.45536847e-03, 7.36606133e-04, 1.80284325e-02],\n",
       "        ...,\n",
       "        [5.27722314e-02, 4.04320098e-03, 3.88075295e-03, ...,\n",
       "         3.80736333e-03, 7.25114252e-03, 7.86732212e-02],\n",
       "        [1.91590860e-02, 1.37074906e-02, 5.26635488e-03, ...,\n",
       "         8.27475917e-03, 5.11673139e-03, 2.74521858e-02],\n",
       "        [2.56188989e-01, 3.66126336e-02, 4.03792597e-02, ...,\n",
       "         4.48789522e-02, 4.59873043e-02, 1.63385183e-01]],\n",
       "\n",
       "       [[9.17957544e-01, 9.77948189e-01, 9.93574440e-01, ...,\n",
       "         1.77677050e-01, 2.00150967e-01, 4.36161101e-01],\n",
       "        [9.70984817e-01, 9.99780357e-01, 9.99453604e-01, ...,\n",
       "         2.49222424e-02, 7.11276457e-02, 8.01168382e-02],\n",
       "        [9.58364189e-01, 9.99690175e-01, 9.99315381e-01, ...,\n",
       "         2.21191924e-02, 9.39366147e-02, 1.09113529e-01],\n",
       "        ...,\n",
       "        [9.15340632e-02, 8.83980654e-03, 7.78749678e-03, ...,\n",
       "         1.02889910e-02, 5.81889227e-03, 7.22497925e-02],\n",
       "        [6.59639984e-02, 4.67216223e-03, 7.20763765e-03, ...,\n",
       "         3.56285763e-03, 1.14939781e-02, 2.47448664e-02],\n",
       "        [2.16293126e-01, 7.16688409e-02, 9.15740430e-02, ...,\n",
       "         7.20831975e-02, 1.67189300e-01, 1.67753279e-01]],\n",
       "\n",
       "       [[1.91889897e-01, 4.50444594e-03, 1.52361961e-02, ...,\n",
       "         8.09306221e-04, 2.82483222e-03, 2.29845922e-02],\n",
       "        [2.63010990e-03, 1.20872108e-04, 1.55105063e-05, ...,\n",
       "         1.52213906e-04, 1.82324457e-05, 3.24344710e-02],\n",
       "        [1.33486846e-02, 3.53108626e-05, 2.83773315e-05, ...,\n",
       "         3.88222979e-05, 7.42169432e-05, 3.43349157e-03],\n",
       "        ...,\n",
       "        [3.86699662e-02, 3.62995788e-02, 7.55085330e-03, ...,\n",
       "         1.70689020e-02, 2.99311779e-03, 1.72548279e-01],\n",
       "        [3.04783583e-02, 3.67153552e-03, 6.54077344e-03, ...,\n",
       "         3.08219879e-03, 6.46730931e-03, 9.21114236e-02],\n",
       "        [1.70264915e-01, 5.24816029e-02, 4.94418256e-02, ...,\n",
       "         8.08187351e-02, 7.62692466e-02, 3.48495871e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.29561627e-02, 8.63586273e-03, 9.20303515e-04, ...,\n",
       "         1.35369354e-03, 5.33488928e-05, 5.57120107e-02],\n",
       "        [2.32528546e-03, 1.22224792e-05, 8.33848826e-05, ...,\n",
       "         2.57273829e-08, 4.43048435e-07, 9.51719267e-05],\n",
       "        [3.71495895e-02, 6.73404837e-04, 3.26660520e-04, ...,\n",
       "         4.92525601e-07, 3.30058072e-07, 4.01646877e-03],\n",
       "        ...,\n",
       "        [2.19612122e-02, 5.44927735e-03, 2.37565651e-03, ...,\n",
       "         9.15860385e-03, 4.28554183e-03, 4.66605797e-02],\n",
       "        [5.04594631e-02, 1.90494303e-03, 2.86788656e-03, ...,\n",
       "         7.29174167e-03, 1.19774519e-02, 2.59374008e-02],\n",
       "        [1.82896867e-01, 6.12368360e-02, 1.31241484e-02, ...,\n",
       "         1.41338706e-01, 3.54543328e-02, 1.21593438e-01]],\n",
       "\n",
       "       [[4.53314930e-01, 2.13259146e-01, 1.31607249e-01, ...,\n",
       "         6.58997595e-02, 4.23574261e-02, 1.56826675e-01],\n",
       "        [1.70270979e-01, 2.56669503e-02, 2.23584436e-02, ...,\n",
       "         4.56043473e-03, 1.40444899e-03, 3.41981724e-02],\n",
       "        [8.23394582e-02, 5.70032783e-02, 9.42790508e-03, ...,\n",
       "         2.18607648e-03, 7.22478610e-04, 4.80113067e-02],\n",
       "        ...,\n",
       "        [1.45970702e-01, 8.20423197e-03, 1.45183085e-02, ...,\n",
       "         5.65207470e-03, 1.02938330e-02, 5.89010604e-02],\n",
       "        [7.87993520e-02, 1.54320765e-02, 1.86698399e-02, ...,\n",
       "         5.25602326e-03, 7.70713622e-03, 4.45044003e-02],\n",
       "        [3.86644006e-01, 8.17924440e-02, 3.90338488e-02, ...,\n",
       "         7.08551332e-02, 4.33237329e-02, 1.91351697e-01]],\n",
       "\n",
       "       [[7.98146427e-02, 1.00019295e-02, 1.21537400e-02, ...,\n",
       "         1.56582836e-02, 1.66914109e-02, 2.76667863e-01],\n",
       "        [3.45966662e-03, 2.94758302e-05, 3.46216075e-05, ...,\n",
       "         3.25174164e-03, 2.13616714e-03, 5.16019985e-02],\n",
       "        [3.28433746e-03, 1.37598045e-06, 4.75167053e-06, ...,\n",
       "         8.65689581e-05, 2.09526581e-04, 4.68994491e-02],\n",
       "        ...,\n",
       "        [2.32952535e-02, 1.36448408e-03, 6.10124902e-04, ...,\n",
       "         2.75929086e-03, 1.87015953e-03, 4.13588472e-02],\n",
       "        [2.17194594e-02, 6.24422159e-04, 6.47169072e-04, ...,\n",
       "         2.65368284e-03, 2.25138455e-03, 3.71159278e-02],\n",
       "        [1.20271280e-01, 2.69497875e-02, 1.13859121e-02, ...,\n",
       "         2.65369453e-02, 1.71096232e-02, 1.11791097e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem.numpy().reshape(13,128,128)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63f12c6fbc5857ff15291fb417d86d25a665886c9e9c67e60a616b8bb414d7e7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
