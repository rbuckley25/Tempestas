{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e17c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "import carla\n",
    "\n",
    "import torch\n",
    "\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from agents.navigation.basic_agent import BasicAgent\n",
    "\n",
    "import gym\n",
    "import gym_carla\n",
    "\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da9fbb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to Carla server...\n",
      "Carla server connected!\n",
      "WeatherParameters(cloudiness=60.000000, cloudiness=60.000000, precipitation=0.000000, precipitation_deposits=50.000000, wind_intensity=10.000000, sun_azimuth_angle=-1.000000, sun_altitude_angle=15.000000, fog_density=2.000000, fog_distance=0.750000, fog_falloff=0.100000, wetness=0.000000, scattering_intensity=1.000000, mie_scattering_scale=0.030000, rayleigh_scattering_scale=0.033100)\n"
     ]
    }
   ],
   "source": [
    "# parameters for the gym_carla environment\n",
    "params = {\n",
    "    'number_of_vehicles': 2,\n",
    "    'number_of_walkers': 0,\n",
    "    'display_size': 256,  # screen size of bird-eye render\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': True,  # whether to use discrete control space\n",
    "    'discrete_acc': [2.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-0.3, 0.0, 0.3],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.3, 0.3],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.lincoln*',  # filter for defining ego vehicle\n",
    "    'port': 3000,  # connection port\n",
    "    'town': 'Town04',  # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 200,  # maximum timesteps per episode\n",
    "    'max_waypt': 12,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'obs_size': 128, #obs image sizes\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 1.8,  # threshold for out of lane\n",
    "    'desired_speed': 5,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 20,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "    'routes':None,\n",
    "    'weather':'WetCloudySunset',\n",
    "    'Collect_Data':False\n",
    "}\n",
    "\n",
    "# Set gym-carla environment\n",
    "env = gym.make('carla-v0', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b042d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,23, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return F.log_softmax(self.conv12(x),dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed1e173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        #latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23be881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(9,80)\n",
    "        self.lin2 = nn.Linear(80,50)\n",
    "        self.lin3 = nn.Linear(50,25)\n",
    "        self.lin4 = nn.Linear(25,15)\n",
    "        self.lin5 = nn.Linear(15,8)\n",
    "        self.lin6 = nn.Linear(8,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        x = self.lin6(x)\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e54b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (lin1): Linear(in_features=9, out_features=80, bias=True)\n",
       "  (lin2): Linear(in_features=80, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (lin4): Linear(in_features=25, out_features=15, bias=True)\n",
       "  (lin5): Linear(in_features=15, out_features=8, bias=True)\n",
       "  (lin6): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerceptionNet()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./AE_params/model_44.best'))\n",
    "model.eval()\n",
    "\n",
    "vis_net = DQN(3).to(device)\n",
    "vis_net.load_state_dict(torch.load('./model_params_CL/model_7.best'))\n",
    "vis_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88446b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.navigation.basic_agent import BasicAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca63c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [(False,1500),(True,200)]:\n",
    "    num_episodes = i[1]\n",
    "    min_overall_loss = 1000\n",
    "    data = DataCollector(env,test=i[0])\n",
    "    for i_episode in range(num_episodes):\n",
    "        rewards = 0\n",
    "        # Initialize the environment and state\n",
    "        obs = env.reset()\n",
    "        agent = BasicAgent(env.ego)\n",
    "        agent.set_target_speed(10)\n",
    "        #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "        ego_location = env.ego.get_location()\n",
    "        ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "        #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "        ego_pos = np.asarray((ego_dir[0],ego_dir[1][0],ego_dir[1][1]),dtype=np.float32)\n",
    "        state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "\n",
    "        #inital state with previous action for data collection consistency\n",
    "        action = np.array([0])\n",
    "        reward = 0\n",
    "\n",
    "        #ego_location = env.ego.get_location()\n",
    "        #end = get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "        #agent.set_destination(carla.Location(*end))\n",
    "\n",
    "        data.collect_step(obs, state, reward, action)\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        loss = episode_loss = 1000\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                action = random.randint(0,2)\n",
    "            env.show_images(np.asarray(generate_semantic_im(obs['camera'])))\n",
    "            next_obs, reward, done, info  = env.step(action)\n",
    "            rewards += reward\n",
    "\n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            #collect every other image\n",
    "            data.collect_step(obs,state,reward,np.array([action]))\n",
    "            \n",
    "            #update state\n",
    "            state = torch.tensor(next_state)\n",
    "            obs = next_obs\n",
    "            #obs = torch.tensor(next_obs)\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                data.stop_recording()\n",
    "                break\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a83d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lane_dis(waypoints, x, y):\n",
    "  \"\"\"\n",
    "  Calculate distance from (x, y) to waypoints.\n",
    "  :param waypoints: a list of list storing waypoints like [[x0, y0], [x1, y1], ...]\n",
    "  :param x: x position of vehicle\n",
    "  :param y: y position of vehicle\n",
    "  :return: a tuple of the distance and the closest waypoint orientation \n",
    "  :added - and waypoint\n",
    "  \"\"\"\n",
    "  dis_min = 1000\n",
    "  dis_sec = 999\n",
    "  waypt = waypoints[0]\n",
    "  for pt in waypoints:\n",
    "    d = np.sqrt((x-pt[0])**2 + (y-pt[1])**2)\n",
    "    if d < dis_min:\n",
    "      dis_min = d\n",
    "      waypt1 =pt\n",
    "    elif d > dis_min and d < dis_sec:\n",
    "      dis_sec = d\n",
    "      waypt2=pt\n",
    "  return waypt1, waypt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25e2c979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.800000011920929"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run_step().steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a92f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-4.786291122436523, -82.82028198242188, 90.47937774658203], [-4.828125, -77.8204574584961, 90.47937774658203])\n"
     ]
    }
   ],
   "source": [
    "print(get_lane_dis(env.waypoints,ego_location.x,ego_location.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c327d9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(x=-97.393295, y=-78.767982, z=0.012080)\n"
     ]
    }
   ],
   "source": [
    "print(env.ego.get_location())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfe4444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(x=-76.864464, y=-105.633743, z=0.000015)\n",
      "Location(x=-9.575274, y=58.183372, z=0.034874)\n"
     ]
    }
   ],
   "source": [
    "print(env.waypoint_queue.pop()[0].transform.location)\n",
    "print(env.waypoint_queue.popleft()[0].transform.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0b22ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0020483587868511677"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run_step().steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d92240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "class DataCollector():\n",
    "    def __init__(self, env, recorder = False, test=False):\n",
    "        self.timestamp = time.strftime(\"%a, %d %b %Y %H:%M:%S\")\n",
    "        if not env:\n",
    "            raise Exception(\"Please provide a carla gym env object\")\n",
    "        self.recorder = recorder\n",
    "        \n",
    "        dir_str = './Datasets/'+env.weather+'/'+env.town\n",
    "        \n",
    "        #collect test set\n",
    "        if test:\n",
    "            dir_str = dir_str +'/test'\n",
    "            \n",
    "        self.rgb_dir = dir_str+'/RGB/'\n",
    "        self.sem_dir = dir_str+'/Semantic/'\n",
    "        self.state_dir = dir_str+'/States/'\n",
    "        self.recording_dir = dir_str+'/Recordings/'\n",
    "        if not os.path.isdir(dir_str):\n",
    "            os.makedirs(dir_str)\n",
    "            os.makedirs(self.rgb_dir)\n",
    "            os.makedirs(self.sem_dir)\n",
    "            os.makedirs(self.state_dir)\n",
    "            os.makedirs(self.recording_dir)\n",
    "        \n",
    "        if self.recorder:\n",
    "            self.record()\n",
    "    \n",
    "            \n",
    "    \n",
    "    def collect_step(self, obs, state, reward, action):\n",
    "        self.timestamp = time.strftime(\"%a, %d %b %Y %H:%M:%S\")\n",
    "        # save image data \n",
    "        im = Image.fromarray(obs['camera'])\n",
    "        im.save(self.rgb_dir+self.timestamp+\".jpeg\")\n",
    "        np.save(self.sem_dir+self.timestamp,obs['semantic'][:, :, :1])\n",
    "        #convert tensor to numpy array and append action to state vector\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.cpu()\n",
    "            state = state.numpy()\n",
    "        state = np.append(state,action)\n",
    "        np.save(self.state_dir+self.timestamp,state)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def record(self):\n",
    "        env.client.start_recorder(self.recording_dir+self.timestamp+\".log\")\n",
    "\n",
    "    def stop_recording(self):\n",
    "        if self.recorder:\n",
    "            env.client.stop_recorder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0821a29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs['camera'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1923b97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14772/4276729940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'camera'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image.fromarray(obs['camera'],'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c279eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_tags(sem_image):\n",
    "    recode_dict = {0:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,\n",
    "                    10:10,11:11,12:12,13:0,14:3,15:1,16:3,17:2,18:5,19:3,20:4,21:3,22:9\n",
    "                    }\n",
    "    for value in recode_dict.keys():\n",
    "        sem_image[sem_image==value] = recode_dict[value]\n",
    "    return sem_image\n",
    "\n",
    "tag_convert_dict = {0:[70,130,180],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }\n",
    "\n",
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic\n",
    "\n",
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = torch.tensor(RGB_image)\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    out,latent_space = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332fef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
