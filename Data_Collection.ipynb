{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "import carla\n",
    "\n",
    "import torch\n",
    "\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from models import PerceptionNet\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from agents.navigation.basic_agent import BasicAgent\n",
    "\n",
    "import gym\n",
    "import gym_carla\n",
    "\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the gym_carla environment\n",
    "params = {\n",
    "    'number_of_vehicles': 2,\n",
    "    'number_of_walkers': 0,\n",
    "    'display_size': 256,  # screen size of bird-eye render\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': True,  # whether to use discrete control space\n",
    "    'discrete_acc': [2.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-0.3, 0.0, 0.3],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.3, 0.3],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.lincoln*',  # filter for defining ego vehicle\n",
    "    'port': 2000,  # connection port\n",
    "    'town': 'Town04',  # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 200,  # maximum timesteps per episode\n",
    "    'max_waypt': 12,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'obs_size': 128, #obs image sizes\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 1.8,  # threshold for out of lane\n",
    "    'desired_speed': 5,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 20,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "    'routes':None,\n",
    "    'weather':'WetCloudySunset',\n",
    "    'Collect_Data':False\n",
    "}\n",
    "\n",
    "# Set gym-carla environment\n",
    "env = gym.make('carla-v0', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(9,80)\n",
    "        self.lin2 = nn.Linear(80,50)\n",
    "        self.lin3 = nn.Linear(50,25)\n",
    "        self.lin4 = nn.Linear(25,15)\n",
    "        self.lin5 = nn.Linear(15,8)\n",
    "        self.lin6 = nn.Linear(8,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        x = self.lin6(x)\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_tags(sem_image):\n",
    "    recode_dict = {0:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,\n",
    "                    10:10,11:11,12:12,13:0,14:3,15:1,16:3,17:2,18:5,19:3,20:4,21:3,22:9\n",
    "                    }\n",
    "    for value in recode_dict.keys():\n",
    "        sem_image[sem_image==value] = recode_dict[value]\n",
    "    return sem_image\n",
    "\n",
    "tag_convert_dict = {0:[70,130,180],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }\n",
    "\n",
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic\n",
    "\n",
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = torch.tensor(RGB_image)\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    out,latent_space = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "class DataCollector():\n",
    "    def __init__(self, env, recorder = False, test=False):\n",
    "        self.timestamp = time.strftime(\"%a, %d %b %Y %H:%M:%S\")\n",
    "        if not env:\n",
    "            raise Exception(\"Please provide a carla gym env object\")\n",
    "        self.recorder = recorder\n",
    "        \n",
    "        dir_str = './Data/'+env.weather+'/'+env.town\n",
    "        \n",
    "        #collect test set\n",
    "        if test:\n",
    "            dir_str = dir_str +'/test'\n",
    "            \n",
    "        self.rgb_dir = dir_str+'/RGB/'\n",
    "        self.sem_dir = dir_str+'/Semantic/'\n",
    "        self.state_dir = dir_str+'/States/'\n",
    "        self.recording_dir = dir_str+'/Recordings/'\n",
    "        if not os.path.isdir(dir_str):\n",
    "            os.makedirs(dir_str)\n",
    "            os.makedirs(self.rgb_dir)\n",
    "            os.makedirs(self.sem_dir)\n",
    "            os.makedirs(self.state_dir)\n",
    "            os.makedirs(self.recording_dir)\n",
    "        \n",
    "        if self.recorder:\n",
    "            self.record()\n",
    "    \n",
    "            \n",
    "    \n",
    "    def collect_step(self, obs, state, reward, action):\n",
    "        self.timestamp = time.strftime(\"%a, %d %b %Y %H:%M:%S\")\n",
    "        # save image data \n",
    "        im = Image.fromarray(obs['camera'])\n",
    "        im.save(self.rgb_dir+self.timestamp+\".jpeg\")\n",
    "        np.save(self.sem_dir+self.timestamp,obs['semantic'][:, :, :1])\n",
    "        #convert tensor to numpy array and append action to state vector\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.cpu()\n",
    "            state = state.numpy()\n",
    "        state = np.append(state,action)\n",
    "        np.save(self.state_dir+self.timestamp,state)\n",
    "        \n",
    "        \n",
    "    def record(self):\n",
    "        env.client.start_recorder(self.recording_dir+self.timestamp+\".log\")\n",
    "\n",
    "    def stop_recording(self):\n",
    "        if self.recorder:\n",
    "            env.client.stop_recorder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e54b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerceptionNet(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./AE_params/model_44.best'))\n",
    "model.eval()\n",
    "\n",
    "vis_net = DQN(3).to(device)\n",
    "vis_net.load_state_dict(torch.load('./model_params_CL/model_7.best'))\n",
    "vis_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [(False,1500),(True,200)]:\n",
    "    num_episodes = i[1]\n",
    "    min_overall_loss = 1000\n",
    "    data = DataCollector(env,test=i[0])\n",
    "    for i_episode in range(num_episodes):\n",
    "        rewards = 0\n",
    "        # Initialize the environment and state\n",
    "        obs = env.reset()\n",
    "        agent = BasicAgent(env.ego)\n",
    "        agent.set_target_speed(10)\n",
    "        #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "        ego_location = env.ego.get_location()\n",
    "        ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "        #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "        ego_pos = np.asarray((ego_dir[0][0],ego_dir[0][1][0],ego_dir[0][1][1]),dtype=np.float32)\n",
    "        state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "\n",
    "        #inital state with previous action for data collection consistency\n",
    "        action = np.array([0])\n",
    "        reward = 0\n",
    "\n",
    "        #ego_location = env.ego.get_location()\n",
    "        #end = get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "        #agent.set_destination(carla.Location(*end))\n",
    "\n",
    "        data.collect_step(obs, state, reward, action)\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        loss = episode_loss = 1000\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                action = random.randint(0,2)\n",
    "            env.show_images(np.asarray(generate_semantic_im(obs['camera'])))\n",
    "            next_obs, reward, done, info  = env.step(action)\n",
    "            rewards += reward\n",
    "\n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            #collect every other image\n",
    "            data.collect_step(obs,state,reward,np.array([action]))\n",
    "            \n",
    "            #update state\n",
    "            state = torch.tensor(next_state)\n",
    "            obs = next_obs\n",
    "            #obs = torch.tensor(next_obs)\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                data.stop_recording()\n",
    "                break\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
