{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832eda18",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Min Distance RL with CARLA CL (Larger Model) 13/01/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d54a80",
   "metadata": {},
   "source": [
    "At the recommednation of one of the authors of the paper I will try to adapt the following [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to work acheive the same results as in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257b48f",
   "metadata": {},
   "source": [
    "Notes: Pygame only runs on python 3.7, pytorch must be install direclty into the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d2016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "import gym_carla\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09526aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "connecting to Carla server...\n",
      "Carla server connected!\n",
      "WeatherParameters(cloudiness=5.000000, cloudiness=5.000000, precipitation=0.000000, precipitation_deposits=0.000000, wind_intensity=10.000000, sun_azimuth_angle=-1.000000, sun_altitude_angle=45.000000, fog_density=2.000000, fog_distance=0.750000, fog_falloff=0.100000, wetness=0.000000, scattering_intensity=1.000000, mie_scattering_scale=0.030000, rayleigh_scattering_scale=0.033100)\n"
     ]
    }
   ],
   "source": [
    "# parameters for the gym_carla environment\n",
    "params = {\n",
    "    'number_of_vehicles': 3,\n",
    "    'number_of_walkers': 0,\n",
    "    'display_size': 256,  # screen size of bird-eye render\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': True,  # whether to use discrete control space\n",
    "    'discrete_acc': [2.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-0.3, 0.0, 0.3],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.3, 0.3],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.lincoln*',  # filter for defining ego vehicle\n",
    "    'port': 3000,  # connection port\n",
    "    'town': 'Town04',  # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 500,  # maximum timesteps per episode\n",
    "    'max_waypt': 12,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 4,  # threshold for out of lane\n",
    "    'desired_speed': 5,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 20,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "    'routes':{'Town04':{'H':[192]}},\n",
    "    'weather':'ClearNoon',\n",
    "    'obs_size':128,\n",
    "    'Collect_Data': False\n",
    "}\n",
    "\n",
    "#{'Town04':{'E':[0,301,334,120,75,51],\n",
    "#                                'M':[191,131,197,210,371,348,141,320],\n",
    "#                                'H':[251,161,264,234,167,182]\n",
    "#                                }\n",
    "#                    },\n",
    "\n",
    "# Set gym-carla environment\n",
    "env = gym.make('carla-v0', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d45005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a90688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        #latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5df687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs,input_size):\n",
    "        super(Full_DQN, self).__init__()\n",
    "\n",
    "        self.lin0 = nn.Linear(input_size+9,150)\n",
    "        self.lin1 = nn.Linear(150,100)\n",
    "        self.lin2 = nn.Linear(100,50)\n",
    "        self.lin3 = nn.Linear(50,25)\n",
    "        self.lin4 = nn.Linear(25,15)\n",
    "        self.lin5 = nn.Linear(15,8)\n",
    "        self.lin6 = nn.Linear(8,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.lin0(x))\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        x = self.lin6(x)\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c9bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Linear) or isinstance(layer,torch.nn.Linear):\n",
    "        nn.init.kaiming_uniform_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869cac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 25000\n",
    "TARGET_UPDATE = 256\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = Full_DQN(n_actions,64).to(device)\n",
    "policy_net.load_state_dict(torch.load('./model_params_CL/Full_model_15.best'))\n",
    "#policy_net.apply(initalize_weights)\n",
    "policy_net.train()\n",
    "\n",
    "target_net = Full_DQN(n_actions,64).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.train()\n",
    "\n",
    "model = PerceptionNet()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./AE_params/model_46.best'))\n",
    "model.eval()\n",
    "#don't want to update AE model \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model <= 7\n",
    "optimizer = optim.RMSprop(policy_net.parameters(),lr=0.005)\n",
    "\n",
    "#Model 8 & from 2022 \n",
    "#optimizer = optim.Adam(policy_net.parameters(),lr=0.005)\n",
    "\n",
    "\n",
    "memory = ReplayMemory(7500)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #arg max select the idex of the largest value and view changes shape from (1,) to (1,1)\n",
    "            #try test net\n",
    "            return policy_net(state.float()).argmax().view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18320225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(input_size):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    #reshape state_batch for nn\n",
    "  \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    \n",
    "    # selects column of output that was selceted \n",
    "    state_action_values = policy_net(torch.reshape(state_batch,(BATCH_SIZE,1,input_size)).float()).gather(1,action_batch)\n",
    "    \n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(torch.reshape(non_final_next_states,\n",
    "                                        (list(non_final_next_states.shape)[0],1,input_size)).float()).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #gradient clipping\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fadb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = {'Town04':{'H':[193]}} \n",
    "env.routes = route\n",
    "env.routes_dict = route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c90a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss:1.0,Episode:1\n",
      "-102.83874003901101\n",
      "avg loss:1.0,Episode:2\n",
      "-23.25768094659996\n",
      "avg loss:1.0,Episode:3\n",
      "-3.737273555076193\n",
      "avg loss:1.0,Episode:4\n",
      "-62.91163050907522\n",
      "avg loss:1.0,Episode:5\n",
      "-118.25541641888321\n",
      "avg loss:1.0,Episode:6\n",
      "-13.284196387242467\n",
      "avg loss:1.0,Episode:7\n",
      "1.1557539981575857\n",
      "avg loss:1.0,Episode:8\n",
      "-0.8440708427346877\n",
      "avg loss:1.0,Episode:9\n",
      "-127.68671431428939\n",
      "avg loss:5.696716184026144,Episode:10\n",
      "-115.52442139525475\n",
      "avg loss:13.738332124736464,Episode:11\n",
      "-75.75252685744944\n",
      "avg loss:7.448925634590262,Episode:12\n",
      "-43.326676133770206\n",
      "avg loss:5.152460638477993,Episode:13\n",
      "-128.9674625847498\n",
      "avg loss:5.475365453662263,Episode:14\n",
      "-9.818252110115715\n",
      "avg loss:5.393479292631833,Episode:15\n",
      "-28.452562294862567\n",
      "avg loss:4.359970026385251,Episode:16\n",
      "-28.37261748069097\n",
      "avg loss:4.03359720773375,Episode:17\n",
      "-7.293552216514394\n",
      "avg loss:4.288802056319235,Episode:18\n",
      "-83.49227752817393\n",
      "avg loss:4.212726006616938,Episode:19\n",
      "-6.053403535385666\n",
      "avg loss:3.5930008245545686,Episode:20\n",
      "-22.065763858853526\n",
      "avg loss:3.425869329665107,Episode:21\n",
      "-49.78642586526492\n",
      "avg loss:3.61787206366158,Episode:22\n",
      "4.75636682478553\n",
      "avg loss:2.8345478583767534,Episode:23\n",
      "-6.254322177313249\n",
      "avg loss:2.867387821064151,Episode:24\n",
      "-14.80481380825724\n",
      "avg loss:2.9009079832501437,Episode:25\n",
      "-5.823856987194354\n",
      "avg loss:3.525356383211984,Episode:26\n",
      "7.3038983786277925\n",
      "avg loss:4.4431458113857625,Episode:27\n",
      "4.830551033826133\n",
      "avg loss:3.801448400730524,Episode:28\n",
      "-25.796087452967164\n",
      "avg loss:3.636643643455531,Episode:29\n",
      "-26.238130441932107\n",
      "avg loss:5.033347642327038,Episode:30\n",
      "-81.59099161467572\n",
      "avg loss:4.899617326256997,Episode:31\n",
      "-7.191569919244971\n",
      "avg loss:3.8100677488593986,Episode:32\n",
      "-20.762525530721835\n",
      "avg loss:3.795256111925295,Episode:33\n",
      "7.48835271271674\n",
      "avg loss:4.2497121442005685,Episode:34\n",
      "-9.38717298079968\n",
      "avg loss:3.5476304342176257,Episode:35\n",
      "-30.986214466952255\n",
      "avg loss:3.1696953907929406,Episode:36\n",
      "-26.627331071228312\n",
      "avg loss:3.3803340007887024,Episode:37\n",
      "-0.845141792794081\n",
      "avg loss:2.482773027476158,Episode:38\n",
      "-34.013594036042406\n",
      "avg loss:2.504834218742365,Episode:39\n",
      "-24.798555457755384\n",
      "avg loss:2.4821128961513472,Episode:40\n",
      "-21.85110890967025\n",
      "avg loss:2.3672143477292753,Episode:41\n",
      "-58.41803053009808\n",
      "avg loss:2.8396458129075173,Episode:42\n",
      "-30.596953588802272\n",
      "avg loss:2.5087460263419823,Episode:43\n",
      "-19.515436207057824\n",
      "avg loss:2.4444755382680405,Episode:44\n",
      "-53.48586423427097\n",
      "avg loss:1.9789997320922603,Episode:45\n",
      "-0.34826817507168073\n",
      "avg loss:1.8233114526676166,Episode:46\n",
      "-15.430014390224981\n",
      "avg loss:1.7587181801992526,Episode:47\n",
      "-28.657424244622476\n",
      "avg loss:2.076298056342466,Episode:48\n",
      "-122.7442843713917\n",
      "avg loss:1.540094155146164,Episode:49\n",
      "-12.333031920361204\n",
      "avg loss:1.5123424198695623,Episode:50\n",
      "11.39628417986567\n",
      "avg loss:1.2620735769365883,Episode:51\n",
      "-23.79562750149391\n",
      "avg loss:1.2558645499825205,Episode:52\n",
      "-55.957895119093074\n",
      "avg loss:1.400136856603112,Episode:53\n",
      "-1.8662664309499917\n",
      "avg loss:1.4004684111295331,Episode:54\n",
      "-12.510073573988297\n",
      "avg loss:1.2296730408060224,Episode:55\n",
      "-51.21564090250188\n",
      "avg loss:1.2815350088292186,Episode:56\n",
      "1.3604828939129447\n",
      "avg loss:1.3553850720964729,Episode:57\n",
      "-24.656783153022072\n",
      "avg loss:1.4998509536877191,Episode:58\n",
      "-24.751957540833708\n",
      "avg loss:1.3260288022285291,Episode:59\n",
      "11.315030727789747\n",
      "avg loss:1.3862496393468544,Episode:60\n",
      "-13.278357489396717\n",
      "avg loss:1.5407603984762372,Episode:61\n",
      "3.4221352716133353\n",
      "avg loss:1.4320552379629723,Episode:62\n",
      "-131.38113739673275\n",
      "avg loss:1.604844914829955,Episode:63\n",
      "-18.278468453385464\n",
      "avg loss:1.7833084396455665,Episode:64\n",
      "-1.3364632181914438\n",
      "avg loss:1.7704183170441896,Episode:65\n",
      "-11.87522243539775\n",
      "avg loss:1.7770741426154284,Episode:66\n",
      "-20.456309897605628\n",
      "avg loss:1.7884878911461648,Episode:67\n",
      "-5.713272477895439\n",
      "avg loss:1.8489955045153241,Episode:68\n",
      "-59.117351629568034\n",
      "avg loss:1.8565208807567042,Episode:69\n",
      "-35.71229546446699\n",
      "avg loss:1.860572984911324,Episode:70\n",
      "-13.847736020304351\n",
      "avg loss:2.049477469434889,Episode:71\n",
      "-1.3560612286452383\n",
      "avg loss:2.1111380992808666,Episode:72\n",
      "-0.8285944286284348\n",
      "avg loss:2.0104322319022065,Episode:73\n",
      "-55.65534599030396\n",
      "avg loss:2.1688231170505685,Episode:74\n",
      "-74.38357418487607\n",
      "avg loss:2.0170325709448673,Episode:75\n",
      "-23.70537197711641\n",
      "avg loss:1.9808370372624495,Episode:76\n",
      "9.066201326305077\n",
      "avg loss:1.9391776525004387,Episode:77\n",
      "-8.296306681254524\n",
      "avg loss:1.966405674004328,Episode:78\n",
      "-1.5840902717832819\n",
      "avg loss:2.045833813237605,Episode:79\n",
      "-10.44758199809028\n",
      "avg loss:1.8013978628013978,Episode:80\n",
      "-45.46108955529063\n",
      "avg loss:1.802525758305473,Episode:81\n",
      "-11.289849672176002\n",
      "avg loss:1.8378787104432255,Episode:82\n",
      "-30.30691453111649\n",
      "avg loss:1.937042669663473,Episode:83\n",
      "-73.46992762852332\n",
      "avg loss:2.018032629705049,Episode:84\n",
      "-30.833330046962537\n",
      "avg loss:2.0114045683767015,Episode:85\n",
      "-10.778323940145068\n",
      "avg loss:2.1401637419924033,Episode:86\n",
      "-34.83804809120799\n",
      "avg loss:2.0401792053328704,Episode:87\n",
      "-9.675324032065195\n",
      "avg loss:2.122976148988054,Episode:88\n",
      "6.836463033291121\n",
      "avg loss:2.0043173209695055,Episode:89\n",
      "-22.935604768674416\n",
      "avg loss:1.8347071252247218,Episode:90\n",
      "-15.924082093548138\n",
      "avg loss:1.918409591279441,Episode:91\n",
      "-27.530473956083963\n",
      "avg loss:1.7524522423743236,Episode:92\n",
      "-85.93385368048904\n",
      "avg loss:2.011097367184574,Episode:93\n",
      "-10.830527349474941\n",
      "avg loss:2.262536933387337,Episode:94\n",
      "-10.840976806399954\n",
      "avg loss:2.0770290487386833,Episode:95\n",
      "-28.66616900343559\n",
      "avg loss:1.9550432568758798,Episode:96\n",
      "-15.14063103193304\n",
      "avg loss:2.274519628031363,Episode:97\n",
      "-29.58957910863178\n",
      "avg loss:2.109929997958449,Episode:98\n",
      "7.972906199748319\n",
      "avg loss:1.9999020516881871,Episode:99\n",
      "9.649379976050234\n",
      "avg loss:2.119990748056533,Episode:100\n",
      "12.323758148008139\n",
      "avg loss:2.1597563474739774,Episode:101\n",
      "-4.284765411283741\n",
      "avg loss:2.1762888893246437,Episode:102\n",
      "-51.32340516687939\n",
      "avg loss:2.310679792054204,Episode:103\n",
      "-19.783671578437943\n",
      "avg loss:2.3401175962542204,Episode:104\n",
      "-51.300459672853336\n",
      "avg loss:2.264040722793485,Episode:105\n",
      "-7.711963284830677\n",
      "avg loss:2.2409460141794004,Episode:106\n",
      "-53.55032566775817\n",
      "avg loss:2.1813785355570974,Episode:107\n",
      "-60.140556796865894\n",
      "avg loss:2.181510750748712,Episode:108\n",
      "-4.324505517419501\n",
      "avg loss:2.15943804908747,Episode:109\n",
      "-109.83177598484221\n",
      "avg loss:2.175236533337048,Episode:110\n",
      "-116.71558785103123\n",
      "avg loss:2.134240053811611,Episode:111\n",
      "-55.10306407064633\n",
      "avg loss:2.1359414306005022,Episode:112\n",
      "-18.968729516202487\n",
      "avg loss:2.0567464951607763,Episode:113\n",
      "1.718168608836649\n",
      "avg loss:2.1025244244293018,Episode:114\n",
      "-8.539644642193945\n",
      "avg loss:2.460525627968699,Episode:115\n",
      "-33.890738704117624\n",
      "avg loss:2.5548886426256114,Episode:116\n",
      "-137.38152922945483\n",
      "avg loss:2.498002989884843,Episode:117\n",
      "-46.8304473465045\n",
      "avg loss:2.5410929519099783,Episode:118\n",
      "-1.870991344727294\n",
      "avg loss:2.5714484635243777,Episode:119\n",
      "-4.304382424240542\n",
      "avg loss:2.277653588467545,Episode:120\n",
      "9.108691392446495\n",
      "avg loss:2.4047431727240838,Episode:121\n",
      "9.725756713189277\n",
      "avg loss:2.9765958875651415,Episode:122\n",
      "-53.12127829196242\n",
      "avg loss:5.253807437802972,Episode:123\n",
      "-33.37718585316652\n",
      "avg loss:2.6909915377377436,Episode:124\n",
      "-6.782960816201783\n",
      "avg loss:2.4547560589545285,Episode:125\n",
      "-72.79233882957496\n",
      "avg loss:2.74998160542567,Episode:126\n",
      "-44.3990775964401\n",
      "avg loss:2.4541478676459403,Episode:127\n",
      "11.546971362601653\n",
      "avg loss:2.327254357279817,Episode:128\n",
      "8.87743670175525\n",
      "avg loss:2.775210574148027,Episode:129\n",
      "2.045649623272901\n",
      "avg loss:2.4748510625676667,Episode:130\n",
      "-11.383626658021765\n",
      "avg loss:2.9841284016825043,Episode:131\n",
      "-10.47632409827782\n",
      "avg loss:2.847205648277854,Episode:132\n",
      "-1.5928954454154294\n",
      "avg loss:2.7963509196090524,Episode:133\n",
      "-27.610880249409107\n",
      "avg loss:2.6303270001871915,Episode:134\n",
      "-56.118574212968845\n",
      "avg loss:2.704628140776882,Episode:135\n",
      "-55.81584945582306\n",
      "avg loss:2.462336140943953,Episode:136\n",
      "-21.377241727325725\n",
      "avg loss:2.5722316261412352,Episode:137\n",
      "-2.665471007443262\n",
      "avg loss:2.5531024578460553,Episode:138\n",
      "-3.448053393512815\n",
      "avg loss:2.4603772872101097,Episode:139\n",
      "-9.30230127685703\n",
      "avg loss:2.8605303414632886,Episode:140\n",
      "-1.378529513257881\n",
      "avg loss:2.878987430621752,Episode:141\n",
      "1.2582688408431402\n",
      "avg loss:2.838743799988353,Episode:142\n",
      "-12.191447947083613\n",
      "avg loss:2.9021227013115927,Episode:143\n",
      "6.540165871291137\n",
      "avg loss:2.681628952605112,Episode:144\n",
      "-17.07070647063614\n",
      "avg loss:2.6083304649298205,Episode:145\n",
      "-6.748412184304511\n",
      "avg loss:2.7481313052322105,Episode:146\n",
      "-12.96426865434953\n",
      "avg loss:3.2053156221759815,Episode:147\n",
      "-2.3088953971488984\n",
      "avg loss:3.1969417795122803,Episode:148\n",
      "-64.26518267628565\n",
      "avg loss:2.9530158352477427,Episode:149\n",
      "-74.17251024241783\n",
      "avg loss:3.3125282236959266,Episode:150\n",
      "1.6638943329250466\n",
      "avg loss:3.0119410933760635,Episode:151\n",
      "-58.4986203769978\n",
      "avg loss:3.0931954726913324,Episode:152\n",
      "-5.387190297004114\n",
      "avg loss:3.4920559497756374,Episode:153\n",
      "10.07075565598499\n",
      "avg loss:3.327367924587085,Episode:154\n",
      "-43.55381672601899\n",
      "avg loss:3.1102147474431403,Episode:155\n",
      "-77.12052953938564\n",
      "avg loss:2.932041295442428,Episode:156\n",
      "-41.139678143920975\n",
      "avg loss:3.0433836739939273,Episode:157\n",
      "-104.65579064910708\n",
      "avg loss:3.0628278678114333,Episode:158\n",
      "-56.934354582111894\n",
      "avg loss:3.083788978138633,Episode:159\n",
      "-60.86994992115353\n",
      "avg loss:3.18957824613989,Episode:160\n",
      "-92.57772398962605\n",
      "avg loss:3.1236222242006777,Episode:161\n",
      "13.879898454766717\n",
      "avg loss:3.1954506993637573,Episode:162\n",
      "-36.36008010779799\n",
      "avg loss:2.7522812058608426,Episode:163\n",
      "-18.323152303316423\n",
      "avg loss:2.9427460028422763,Episode:164\n",
      "13.982803498814064\n",
      "avg loss:2.6903905844862543,Episode:165\n",
      "-46.230789693351326\n",
      "avg loss:2.6353745091661382,Episode:166\n",
      "-46.565626807426185\n",
      "avg loss:2.8196623608679436,Episode:167\n",
      "14.32430454960732\n",
      "avg loss:2.8980393109842564,Episode:168\n",
      "14.32222445914175\n",
      "avg loss:2.759338470793065,Episode:169\n",
      "-7.203116800518137\n",
      "avg loss:2.748651267093767,Episode:170\n",
      "7.763052089032133\n",
      "avg loss:2.8108536366755184,Episode:171\n",
      "-65.03884539553766\n",
      "avg loss:2.777813775888272,Episode:172\n",
      "-35.92523485958651\n",
      "avg loss:2.628798210358314,Episode:173\n",
      "-8.578965283512924\n",
      "avg loss:2.630371021649662,Episode:174\n",
      "-101.10330055396238\n",
      "avg loss:2.6201960573661336,Episode:175\n",
      "-6.051416433388727\n",
      "avg loss:2.4961985614973012,Episode:176\n",
      "3.827913381398748\n",
      "avg loss:2.5679443284768,Episode:177\n",
      "-73.1219797933419\n",
      "avg loss:2.5011581167899344,Episode:178\n",
      "-14.410414432717387\n",
      "avg loss:2.8488030386049896,Episode:179\n",
      "1.1908172209654069\n",
      "avg loss:2.241501186266259,Episode:180\n",
      "-67.52243402817531\n",
      "avg loss:2.2476010226701613,Episode:181\n",
      "-7.590835346921022\n",
      "avg loss:2.415174521414353,Episode:182\n",
      "-112.75326282287863\n",
      "avg loss:2.555601154988525,Episode:183\n",
      "6.087841661967715\n",
      "avg loss:2.554971237289517,Episode:184\n",
      "12.194799330048752\n",
      "avg loss:2.6971198767388893,Episode:185\n",
      "-6.705443438332809\n",
      "avg loss:2.6808586278868094,Episode:186\n",
      "2.7404869029275503\n",
      "avg loss:2.7461509424795336,Episode:187\n",
      "3.1075208585193295\n",
      "avg loss:2.8302552961761394,Episode:188\n",
      "-11.482294715705471\n",
      "avg loss:2.9501764595266624,Episode:189\n",
      "-151.33894075006506\n",
      "avg loss:2.842041582242628,Episode:190\n",
      "-9.258365840174958\n",
      "avg loss:2.9499540873910766,Episode:191\n",
      "-15.894901498413658\n",
      "avg loss:3.1960915275267756,Episode:192\n",
      "-42.66954376176851\n",
      "avg loss:3.122633661114249,Episode:193\n",
      "-71.68086804725417\n",
      "avg loss:3.282039673958706,Episode:194\n",
      "6.0686206519170725\n",
      "avg loss:3.2177466711595217,Episode:195\n",
      "-52.48108801952374\n",
      "avg loss:3.0342665759463867,Episode:196\n",
      "-87.65085193991379\n",
      "avg loss:3.377412339612434,Episode:197\n",
      "8.368781484147398\n",
      "avg loss:3.379238283063618,Episode:198\n",
      "-38.126212638136636\n",
      "avg loss:3.0289488672012896,Episode:199\n",
      "7.202162983972443\n",
      "avg loss:3.0156541705886992,Episode:200\n",
      "-5.850341111610321\n",
      "avg loss:3.0175108759205056,Episode:201\n",
      "7.636316239792677\n",
      "avg loss:2.9378640186242673,Episode:202\n",
      "10.181514352367373\n",
      "avg loss:3.106790798282772,Episode:203\n",
      "1.8485498458818697\n",
      "avg loss:3.0022425091156055,Episode:204\n",
      "11.236826326596034\n",
      "avg loss:2.7386033068458535,Episode:205\n",
      "-39.562102444771305\n",
      "avg loss:3.585204478636287,Episode:206\n",
      "-23.206035672890522\n",
      "avg loss:3.0913891000628335,Episode:207\n",
      "-23.05210458802909\n",
      "avg loss:3.475389468089838,Episode:208\n",
      "-33.04438800305803\n",
      "avg loss:3.3513927721806254,Episode:209\n",
      "-35.61013594784133\n",
      "avg loss:3.505444652611918,Episode:210\n",
      "-3.3167947722214777\n",
      "avg loss:3.492621568328475,Episode:211\n",
      "21.670468762687904\n",
      "avg loss:3.777514764489097,Episode:212\n",
      "-22.846520306042578\n",
      "avg loss:3.8536767171252975,Episode:213\n",
      "-40.75734131833246\n",
      "avg loss:3.9860083853276835,Episode:214\n",
      "76.97748810899152\n",
      "avg loss:4.187909547997532,Episode:215\n",
      "-49.935257545565236\n",
      "avg loss:3.9248777711898994,Episode:216\n",
      "-66.83776477953842\n",
      "avg loss:3.9485897338269087,Episode:217\n",
      "12.720424573335634\n",
      "avg loss:4.19774782286903,Episode:218\n",
      "-13.589230637273515\n",
      "avg loss:4.451717743258019,Episode:219\n",
      "3.1677040032751336\n",
      "avg loss:4.226931605581061,Episode:220\n",
      "28.108160556363764\n",
      "avg loss:4.172014489802785,Episode:221\n",
      "-4.2573115547832145\n",
      "avg loss:4.54592488399321,Episode:222\n",
      "-49.63187634985359\n",
      "avg loss:4.468564227655306,Episode:223\n",
      "15.767695822381775\n",
      "avg loss:4.314377514319349,Episode:224\n",
      "105.50195386697781\n",
      "avg loss:4.320833933323861,Episode:225\n",
      "-10.832675366339817\n",
      "avg loss:4.166089076349273,Episode:226\n",
      "7.439345844870367\n",
      "avg loss:4.6533442730823555,Episode:227\n",
      "24.74086543571228\n",
      "avg loss:4.595652634326631,Episode:228\n",
      "-50.59929277989447\n",
      "avg loss:5.058155010186208,Episode:229\n",
      "8.786385555046714\n",
      "avg loss:4.735773188819386,Episode:230\n",
      "17.428539669550034\n",
      "avg loss:4.558271922667957,Episode:231\n",
      "7.894936304136504\n",
      "avg loss:4.627641981688793,Episode:232\n",
      "6.833523394788749\n",
      "avg loss:5.588098729468094,Episode:233\n",
      "11.459884454508142\n",
      "avg loss:5.648009809530733,Episode:234\n",
      "8.407758359920688\n",
      "avg loss:6.0324012411900325,Episode:235\n",
      "11.6865084650826\n",
      "avg loss:5.148280479148298,Episode:236\n",
      "-23.67548902243462\n",
      "avg loss:4.766507961025579,Episode:237\n",
      "-28.983289168420995\n",
      "avg loss:4.86502192549506,Episode:238\n",
      "8.689559707882026\n",
      "avg loss:5.2006248278783485,Episode:239\n",
      "13.979597304417581\n",
      "avg loss:4.880364559809404,Episode:240\n",
      "0.6453982971771595\n",
      "avg loss:5.06064280207333,Episode:241\n",
      "24.589727887229017\n",
      "avg loss:5.412678988538831,Episode:242\n",
      "-8.527964946824204\n",
      "avg loss:4.839214546605192,Episode:243\n",
      "-174.05492384175102\n",
      "avg loss:4.662708495545912,Episode:244\n",
      "-91.08950973364125\n",
      "avg loss:4.821364523339513,Episode:245\n",
      "-143.54706119879808\n",
      "avg loss:5.035681799233712,Episode:246\n",
      "-21.13341466126994\n",
      "avg loss:4.691808677997494,Episode:247\n",
      "89.31098903032533\n",
      "avg loss:4.541468392913429,Episode:248\n",
      "-16.08716524896884\n",
      "avg loss:4.696364420982791,Episode:249\n",
      "-7.180423606528434\n",
      "avg loss:4.295714680789742,Episode:250\n",
      "-67.33928520635519\n",
      "avg loss:4.503022490662619,Episode:251\n",
      "-12.987018581282435\n",
      "avg loss:4.877399854200931,Episode:252\n",
      "9.435609638665383\n",
      "avg loss:4.712346321795824,Episode:253\n",
      "1.1051882542321039\n",
      "avg loss:4.979258238606672,Episode:254\n",
      "3.5725270343015083\n",
      "avg loss:4.605481080319121,Episode:255\n",
      "11.262555416286368\n",
      "avg loss:4.305246297041356,Episode:256\n",
      "-38.49405251166969\n",
      "avg loss:4.092369245043803,Episode:257\n",
      "-61.608889592515986\n",
      "avg loss:3.8276154290829854,Episode:258\n",
      "-13.04842087766393\n",
      "avg loss:4.089430469107972,Episode:259\n",
      "-59.48974205029213\n",
      "avg loss:4.191715889391646,Episode:260\n",
      "-12.999499287047698\n",
      "avg loss:3.9606751088924264,Episode:261\n",
      "-40.97322720813514\n",
      "avg loss:3.957174400421422,Episode:262\n",
      "-103.53277059157222\n",
      "avg loss:4.641773343246078,Episode:263\n",
      "-6.370817410872853\n",
      "avg loss:4.84484599879976,Episode:264\n",
      "-73.29420486694454\n",
      "avg loss:4.629069733842436,Episode:265\n",
      "-7.813410978474007\n",
      "avg loss:4.530893817951605,Episode:266\n",
      "-27.298565248040667\n",
      "avg loss:4.8694899850651625,Episode:267\n",
      "-87.53226763096278\n",
      "avg loss:5.081509021542841,Episode:268\n",
      "-36.250653720987515\n",
      "avg loss:5.1872327282045685,Episode:269\n",
      "-16.963043126731097\n",
      "avg loss:4.936279608099505,Episode:270\n",
      "22.78792848174117\n",
      "avg loss:5.069367031006209,Episode:271\n",
      "14.439016254634012\n",
      "avg loss:5.30568569977008,Episode:272\n",
      "-74.88537989389569\n",
      "avg loss:5.155729220867561,Episode:273\n",
      "-30.6738347714988\n",
      "avg loss:5.230565034344257,Episode:274\n",
      "-70.3686856445602\n",
      "avg loss:5.017196061522009,Episode:275\n",
      "-156.8364727494424\n",
      "avg loss:4.541275382626039,Episode:276\n",
      "-91.62534752579359\n",
      "avg loss:4.4850447405098866,Episode:277\n",
      "8.595065461409877\n",
      "avg loss:4.820994795860736,Episode:278\n",
      "-84.9755252135479\n",
      "avg loss:5.165381425721103,Episode:279\n",
      "-52.579434439321155\n",
      "avg loss:4.815815116497904,Episode:280\n",
      "-45.351042289138206\n",
      "avg loss:4.750999906887527,Episode:281\n",
      "14.314086138838668\n",
      "avg loss:5.363632356013794,Episode:282\n",
      "-55.49378596269395\n",
      "avg loss:5.224698523328667,Episode:283\n",
      "-103.30060866104284\n",
      "avg loss:5.96134191821824,Episode:284\n",
      "-70.47013797687327\n",
      "avg loss:6.015376651608354,Episode:285\n",
      "-101.59851418531991\n",
      "avg loss:5.309049684480431,Episode:286\n",
      "-43.41932781262192\n",
      "avg loss:5.0736614862556415,Episode:287\n",
      "-23.80735430002475\n",
      "avg loss:5.265404910561935,Episode:288\n",
      "6.9172704315541225\n",
      "avg loss:5.1100026948354325,Episode:289\n",
      "-106.65663878904743\n",
      "avg loss:4.913881716153442,Episode:290\n",
      "9.793607278574257\n",
      "avg loss:4.996637261082831,Episode:291\n",
      "-9.575910365794607\n",
      "avg loss:4.868581801725219,Episode:292\n",
      "-3.298763573161473\n",
      "avg loss:5.021758334954014,Episode:293\n",
      "-59.41232021358722\n",
      "avg loss:5.123083084577207,Episode:294\n",
      "7.42900849392084\n",
      "avg loss:5.137883523361744,Episode:295\n",
      "-70.09898382830102\n",
      "avg loss:5.194483295545541,Episode:296\n",
      "-8.094815831546052\n",
      "avg loss:5.3365852312810835,Episode:297\n",
      "-117.06355310836203\n",
      "avg loss:4.946379295936306,Episode:298\n",
      "-40.15043855899002\n",
      "avg loss:5.1593213104176545,Episode:299\n",
      "-59.362534584189234\n",
      "avg loss:5.20802532763298,Episode:300\n",
      "-34.012301625271974\n",
      "avg loss:5.282337672893734,Episode:301\n",
      "-86.93505266030645\n",
      "avg loss:5.3391700053901765,Episode:302\n",
      "-43.320633191796475\n",
      "avg loss:5.730508320303298,Episode:303\n",
      "-60.02544778136192\n",
      "avg loss:5.444596902707895,Episode:304\n",
      "-31.88655209980709\n",
      "avg loss:5.6197566552916784,Episode:305\n",
      "-29.310411834376463\n",
      "avg loss:5.577786854544435,Episode:306\n",
      "-38.08483693364859\n",
      "avg loss:5.424389257613691,Episode:307\n",
      "-36.262462304791114\n",
      "avg loss:4.88556439080624,Episode:308\n",
      "-137.31733815985837\n",
      "avg loss:4.991342891357743,Episode:309\n",
      "-36.57077258305587\n",
      "avg loss:5.27429401390046,Episode:310\n",
      "-36.5354448397623\n",
      "avg loss:5.834801415593851,Episode:311\n",
      "-36.30849949096215\n",
      "avg loss:6.593526101740458,Episode:312\n",
      "-45.9508619302984\n",
      "avg loss:6.288257889251416,Episode:313\n",
      "-35.0761151227714\n",
      "avg loss:5.57876624303257,Episode:314\n",
      "-37.468367449278134\n",
      "avg loss:5.380783892646047,Episode:315\n",
      "-44.717416131427186\n",
      "avg loss:5.643523073904062,Episode:316\n",
      "-31.283267880276085\n",
      "avg loss:5.921404612619529,Episode:317\n",
      "-48.47530076506205\n",
      "avg loss:5.683488137468103,Episode:318\n",
      "-38.45915174708281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20813/3962606560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_semantic_im\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'camera'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20813/1818069952.py\u001b[0m in \u001b[0;36mgenerate_semantic_im\u001b[0;34m(RGB_image)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20813/584634242.py\u001b[0m in \u001b[0;36mreplace\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mRGB_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_convert_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGB_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGB_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRGB_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "steps_done = 0\n",
    "use_fixed_idx = 0\n",
    "env.use_fixed = 'H'\n",
    "levels = ['H','H','H']\n",
    "next_lvl = 300\n",
    "min_overall_loss = 1000\n",
    "min_reward = -1000\n",
    "input_size = 73\n",
    "start_points = [197,348,371,193,192]\n",
    "route_num = 3\n",
    "stage_min_reward = -1000\n",
    "for i_episode in range(1,num_episodes+1):\n",
    "    eps_loss = []\n",
    "    min_loss = 100\n",
    "    rewards = 0\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0],ego_dir[1][0],ego_dir[1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state).reshape(1,9,1,1)\n",
    "\n",
    "    new_obs = torch.tensor(obs['camera'])\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    \n",
    "    _,latent_space = model(new_obs)\n",
    "    state = torch.cat((state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "\n",
    "\n",
    "    episode_loss = loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        obs, reward, done, info  = env.step(action.item())\n",
    "        env.show_images(np.asarray(generate_semantic_im(obs['camera'])))\n",
    "        if t > 20:\n",
    "            rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if not done:\n",
    "            \n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            new_obs = torch.tensor(obs['camera'])\n",
    "            new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "            _,latent_space = model(new_obs)\n",
    "            info_state = torch.tensor(next_state).reshape(1,9,1,1)\n",
    "            next_state = torch.cat((info_state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "        else:\n",
    "            next_state = None\n",
    "        #simulator doesnt run for first 20 steps due to load time\n",
    "        if t > 20:\n",
    "            memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = optimize_model(input_size)\n",
    "        if loss:\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "            eps_loss.append(loss)\n",
    "        else:\n",
    "            eps_loss.append(1)\n",
    "        \n",
    "         # Update the target network\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            avg_loss = sum(eps_loss)/len(eps_loss)\n",
    "            print('{}:{},{}:{}'.format('avg loss', avg_loss, 'Episode', i_episode))\n",
    "            writer.add_scalar(\"Avg Loss/train\", avg_loss, i_episode)\n",
    "            writer.add_scalar(\"Min Loss/train\", min_loss, i_episode)\n",
    "            break\n",
    "    print(rewards) \n",
    "    '''\n",
    "    if i_episode % (num_episodes//5) == 0:\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/Full_model_16_'+str(route_num)+'.best')\n",
    "        if i_episode != num_episodes:\n",
    "            route = {'Town04':{'H':[start_points[route_num]]}} \n",
    "            env.routes = route\n",
    "            env.routes_dict = route\n",
    "            route_num += 1\n",
    "    '''\n",
    "    len_episode = t+1\n",
    "    writer.add_scalar(\"Lenght/Epoch\", len_episode, i_episode)\n",
    "    writer.add_scalar(\"Reward/Episode\", rewards, i_episode)\n",
    "    \n",
    "    #save model if better than previous episode\n",
    "    if rewards > min_reward:\n",
    "        min_reward = rewards\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/Full_model_17.best')\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "print('Steps Done: ', steps_done)\n",
    "torch.save(target_net.state_dict(), './model_params_CL/Full_model_17.final')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa528f",
   "metadata": {},
   "source": [
    "### Visualise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_net.state_dict(), './model_params_CL/model_10.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fef6c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Full_DQN(\n",
       "  (lin0): Linear(in_features=73, out_features=150, bias=True)\n",
       "  (lin1): Linear(in_features=150, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (lin4): Linear(in_features=25, out_features=15, bias=True)\n",
       "  (lin5): Linear(in_features=15, out_features=8, bias=True)\n",
       "  (lin6): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_net = Full_DQN(n_actions,64).to(device)\n",
    "vis_net.load_state_dict(torch.load('./model_params_CL/Full_model_14.best'))\n",
    "vis_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e04790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropResizeTransform:\n",
    "\n",
    "    def __init__(self, top, left, width, height, size):\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return TF.resized_crop(x,self.top,self.left,self.width,self.height,self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "572b19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = {'Town04':{'H':[192]}} \n",
    "env.routes = route\n",
    "env.routes_dict = route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21657e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20813/3538553781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0msem_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_semantic_im\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'camera'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msem_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m#save_images(obs['camera'],sem_image,t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mnew_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'camera'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Carla/gym-carla/gym_carla/envs/carla_env.py\u001b[0m in \u001b[0;36mshow_images\u001b[0;34m(self, new_semantic)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;31m# Display birdeye image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0mbirdeye_surface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_to_display_surface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbirdeye\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbirdeye_surface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Carla/gym-carla/gym_carla/envs/misc.py\u001b[0m in \u001b[0;36mrgb_to_display_surface\u001b[0;34m(rgb, display_size)\u001b[0m\n\u001b[1;32m    245\u001b[0m   \"\"\"\n\u001b[1;32m    246\u001b[0m   \u001b[0msurface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSurface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m   \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdisplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m   \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    164\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    165\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# n-dimensional interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    861\u001b[0m                                                   \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                                                   \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                                                   cval=cval))\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mwarped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 4\n",
    "env.use_fixed = 'H'\n",
    "env.route_idx = 1\n",
    "levels = ['E','M','H']\n",
    "next_lvl = 4\n",
    "min_overall_loss = 1000\n",
    "input_size = 73\n",
    "for i_episode in range(num_episodes):\n",
    "    eps_loss = []\n",
    "    rewards = []\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0],ego_dir[1][0],ego_dir[1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state).reshape(1,9,1,1)\n",
    "\n",
    "    new_obs = torch.tensor(obs['camera'])\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    \n",
    "    _,latent_space = model(new_obs)\n",
    "    state = torch.cat((state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    loss = episode_loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = vis_net(state.float()).argmax().view(1,1)\n",
    "            obs, reward, done, info  = env.step(action.item())\n",
    "            sem_image = generate_semantic_im(obs['camera'])\n",
    "            env.show_images(np.asarray(sem_image))\n",
    "            #save_images(obs['camera'],sem_image,t)\n",
    "            new_obs = torch.tensor(obs['camera'])\n",
    "            new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "            _,latent_space = model(new_obs)\n",
    "            rewards.append(reward)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            \n",
    "            info_state = torch.tensor(next_state).reshape(1,9,1,1)\n",
    "            next_state = torch.cat((info_state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('########')\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b68e9665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.1970e+00]],\n",
       "\n",
       "         [[-7.0292e-01]],\n",
       "\n",
       "         [[ 7.1127e-01]],\n",
       "\n",
       "         [[ 3.4733e-01]],\n",
       "\n",
       "         [[-9.8691e-03]],\n",
       "\n",
       "         [[-2.5533e-05]],\n",
       "\n",
       "         [[ 2.8702e-02]],\n",
       "\n",
       "         [[-2.4277e+00]],\n",
       "\n",
       "         [[-0.0000e+00]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1331b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def save_images(rgb,generated,step):\n",
    "    im = Image.fromarray(rgb)\n",
    "    im.save('./Datasets/movie/RGB/'+str(step)+\".jpeg\")\n",
    "    generated.save('./Datasets/movie/Sem/'+str(step)+'.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9001c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        sample,latent_space = model(sample.reshape(1,3,128,128))\n",
    "        sample = sample.cpu().argmax(dim=1)\n",
    "        print(latent_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs = torch.tensor(obs['camera'])\n",
    "new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "out,latent_space = model(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04bc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[0,0,0],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a816a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2411a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = torch.tensor(RGB_image)\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    out,latent_space = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1213fbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F3884733810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_semantic_im(obs['camera'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(obs['camera'], 'RGB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
