{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832eda18",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Min Distance RL with CARLA CL (Larger Model) 13/01/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d54a80",
   "metadata": {},
   "source": [
    "At the recommednation of one of the authors of the paper I will try to adapt the following [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to work acheive the same results as in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257b48f",
   "metadata": {},
   "source": [
    "Notes: Pygame only runs on python 3.7, pytorch must be install direclty into the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from utils import replace, generate_semantic_im\n",
    "import os\n",
    "\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "import gym_carla\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09526aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the gym_carla environment\n",
    "params = {\n",
    "    'number_of_vehicles': 3,\n",
    "    'number_of_walkers': 0,\n",
    "    'display_size': 256,  # screen size of bird-eye render\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': True,  # whether to use discrete control space\n",
    "    'discrete_acc': [2.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-0.3, 0.0, 0.3],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.3, 0.3],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.lincoln*',  # filter for defining ego vehicle\n",
    "    'port': 2000,  # connection port\n",
    "    'town': 'Town04',  # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 500,  # maximum timesteps per episode\n",
    "    'max_waypt': 12,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 4,  # threshold for out of lane\n",
    "    'desired_speed': 5,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 20,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "    'routes':{'Town04':{'H':[193]}},\n",
    "    'weather':'ClearNoon',\n",
    "    'obs_size':128,\n",
    "    'Collect_Data': False\n",
    "}\n",
    "\n",
    "#{'Town04':{'E':[0,301,334,120,75,51],\n",
    "#                                'M':[191,131,197,210,371,348,141,320],\n",
    "#                                'H':[251,161,264,234,167,182]\n",
    "#                                }\n",
    "#                    },\n",
    "\n",
    "# Set gym-carla environment\n",
    "env = gym.make('carla-v0', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d45005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a90688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        #latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5df687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs,input_size):\n",
    "        super(Full_DQN, self).__init__()\n",
    "\n",
    "        self.lin0 = nn.Linear(input_size+9,150)\n",
    "        self.lin1 = nn.Linear(150,100)\n",
    "        self.lin2 = nn.Linear(100,50)\n",
    "        self.lin3 = nn.Linear(50,25)\n",
    "        self.lin4 = nn.Linear(25,15)\n",
    "        self.lin5 = nn.Linear(15,8)\n",
    "        self.lin6 = nn.Linear(8,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.lin0(x))\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        x = self.lin6(x)\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Linear) or isinstance(layer,torch.nn.Linear):\n",
    "        nn.init.kaiming_uniform_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869cac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 80000\n",
    "TARGET_UPDATE = 256\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = Full_DQN(n_actions,64).to(device)\n",
    "#policy_net.load_state_dict(torch.load('./model_params_CL/Full_model_17.best'))\n",
    "policy_net.apply(initalize_weights)\n",
    "policy_net.train()\n",
    "\n",
    "target_net = Full_DQN(n_actions,64).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.train()\n",
    "\n",
    "model = PerceptionNet()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./AE_params/model_46.best'))\n",
    "model.eval()\n",
    "#don't want to update AE model \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model <= 7\n",
    "optimizer = optim.RMSprop(policy_net.parameters(),lr=0.005)\n",
    "memory = ReplayMemory(7500)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #arg max select the idex of the largest value and view changes shape from (1,) to (1,1)\n",
    "            #try test net\n",
    "            return policy_net(state.float()).argmax().view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18320225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(input_size):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    #reshape state_batch for nn\n",
    "  \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    \n",
    "    # selects column of output that was selceted \n",
    "    state_action_values = policy_net(torch.reshape(state_batch,(BATCH_SIZE,1,input_size)).float()).gather(1,action_batch)\n",
    "    \n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(torch.reshape(non_final_next_states,\n",
    "                                        (list(non_final_next_states.shape)[0],1,input_size)).float()).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #gradient clipping\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = {'Town04':{'H':[197]}} \n",
    "env.routes = route\n",
    "env.routes_dict = route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c90a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2500\n",
    "steps_done = 0\n",
    "use_fixed_idx = 0\n",
    "env.use_fixed = 'H'\n",
    "levels = ['H','H','H']\n",
    "next_lvl = 300\n",
    "min_overall_loss = 1000\n",
    "min_reward = -1000\n",
    "input_size = 73\n",
    "start_points = [197,348,371,193,192]\n",
    "route_num = 0\n",
    "stage_min_reward = -1000\n",
    "for i_episode in range(1,num_episodes+1):\n",
    "    eps_loss = []\n",
    "    min_loss = 100\n",
    "    rewards = 0\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0][0],ego_dir[0][1][0],ego_dir[0][1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state).reshape(1,9,1,1)\n",
    "\n",
    "    new_obs = torch.tensor(obs['camera'])\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    \n",
    "    _,latent_space = model(new_obs)\n",
    "    state = torch.cat((state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "\n",
    "\n",
    "    episode_loss = loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        obs, reward, done, info  = env.step(action.item())\n",
    "        \n",
    "        if t > 20:\n",
    "            rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if not done:\n",
    "            \n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            new_obs = torch.tensor(obs['camera'])\n",
    "            new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "            env.show_images(np.asarray(generate_semantic_im(new_obs,model)))\n",
    "            _,latent_space = model(new_obs)\n",
    "            info_state = torch.tensor(next_state).reshape(1,9,1,1)\n",
    "            next_state = torch.cat((info_state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "        else:\n",
    "            next_state = None\n",
    "        #simulator doesnt run for first 20 steps due to load time\n",
    "        if t > 20:\n",
    "            memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = optimize_model(input_size)\n",
    "        if loss:\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "            eps_loss.append(loss)\n",
    "        else:\n",
    "            eps_loss.append(1)\n",
    "        \n",
    "         # Update the target network\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            avg_loss = sum(eps_loss)/len(eps_loss)\n",
    "            print('{}:{},{}:{}'.format('avg loss', avg_loss, 'Episode', i_episode))\n",
    "            writer.add_scalar(\"Avg Loss/train\", avg_loss, i_episode)\n",
    "            writer.add_scalar(\"Min Loss/train\", min_loss, i_episode)\n",
    "            break\n",
    "    print(rewards) \n",
    "    \n",
    "    if i_episode % (num_episodes//5) == 0:\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/Full_model_16_'+str(route_num)+'.best')\n",
    "        if i_episode != num_episodes:\n",
    "            route = {'Town04':{'H':[start_points[route_num]]}} \n",
    "            env.routes = route\n",
    "            env.routes_dict = route\n",
    "            route_num += 1\n",
    "            steps_done = 0\n",
    "    \n",
    "    len_episode = t+1\n",
    "    writer.add_scalar(\"Lenght/Epoch\", len_episode, i_episode)\n",
    "    writer.add_scalar(\"Reward/Episode\", rewards, i_episode)\n",
    "    \n",
    "    #save model if better than previous episode\n",
    "    if rewards > min_reward:\n",
    "        min_reward = rewards\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/Full_model_18.best')\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "print('Steps Done: ', steps_done)\n",
    "torch.save(target_net.state_dict(), './model_params_CL/Full_model_18.final')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa528f",
   "metadata": {},
   "source": [
    "### Visualise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_net = Full_DQN(n_actions,64).to(device)\n",
    "vis_net.load_state_dict(torch.load('./model_params_CL/Full_model_17.best'))\n",
    "vis_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22edadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./AE_params/dry-wet_3.best'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_model = PerceptionNet()\n",
    "dry_model.to(device)\n",
    "dry_model.load_state_dict(torch.load('./AE_params/model_46.best'))\n",
    "dry_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = {'Town04':{'H':[193]}} \n",
    "env.routes = route\n",
    "env.routes_dict = route\n",
    "env.weather = 'HardRainNoon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21657e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 4\n",
    "env.use_fixed = 'H'\n",
    "env.route_idx = 1\n",
    "levels = ['E','M','H']\n",
    "next_lvl = 4\n",
    "min_overall_loss = 1000\n",
    "input_size = 73\n",
    "for i_episode in range(num_episodes):\n",
    "    eps_loss = []\n",
    "    rewards = []\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0][0],ego_dir[0][1][0],ego_dir[0][1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state).reshape(1,9,1,1)\n",
    "\n",
    "    new_obs = torch.tensor(obs['camera'])\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    \n",
    "    _,latent_space = model(new_obs)\n",
    "    state = torch.cat((state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    loss = episode_loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = vis_net(state.float()).argmax().view(1,1)\n",
    "            obs, reward, done, info  = env.step(action.item())\n",
    "            \n",
    "            #save_images(obs['camera'],sem_image,t)\n",
    "            new_obs = torch.tensor(obs['camera'])\n",
    "            new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "            _,latent_space = model(new_obs)\n",
    "\n",
    "            sem = dry_model.decode(latent_space).detach().cpu().argmax(dim=1)\n",
    "            sem = replace(sem.numpy().reshape(1,128,128).transpose(1,2,0))\n",
    "            env.show_images(sem)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            \n",
    "            info_state = torch.tensor(next_state).reshape(1,9,1,1)\n",
    "            next_state = torch.cat((info_state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('########')\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def save_images(rgb,generated,step):\n",
    "    im = Image.fromarray(rgb)\n",
    "    im.save('./Datasets/movie/RGB/'+str(step)+\".jpeg\")\n",
    "    generated.save('./Datasets/movie/Sem/'+str(step)+'.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[0,0,0],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2411a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = torch.tensor(RGB_image)\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    out,latent_space = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
