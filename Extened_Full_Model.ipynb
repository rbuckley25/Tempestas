{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832eda18",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Min Distance RL with CARLA CL (Larger Model) 13/01/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d54a80",
   "metadata": {},
   "source": [
    "At the recommednation of one of the authors of the paper I will try to adapt the following [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to work acheive the same results as in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257b48f",
   "metadata": {},
   "source": [
    "Notes: Pygame only runs on python 3.7, pytorch must be install direclty into the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d2016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from utils import replace, generate_semantic_im\n",
    "import os\n",
    "\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from models import PerceptionNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "import gym_carla\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09526aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to Carla server...\n",
      "Carla server connected!\n",
      "WeatherParameters(cloudiness=5.000000, cloudiness=5.000000, precipitation=0.000000, precipitation_deposits=0.000000, wind_intensity=10.000000, sun_azimuth_angle=-1.000000, sun_altitude_angle=45.000000, fog_density=2.000000, fog_distance=0.750000, fog_falloff=0.100000, wetness=0.000000, scattering_intensity=1.000000, mie_scattering_scale=0.030000, rayleigh_scattering_scale=0.033100)\n"
     ]
    }
   ],
   "source": [
    "# parameters for the gym_carla environment\n",
    "params = {\n",
    "    'number_of_vehicles': 3,\n",
    "    'number_of_walkers': 0,\n",
    "    'display_size': 256,  # screen size of bird-eye render\n",
    "    'max_past_step': 1,  # the number of past steps to draw\n",
    "    'dt': 0.1,  # time interval between two frames\n",
    "    'discrete': True,  # whether to use discrete control space\n",
    "    'discrete_acc': [2.0],  # discrete value of accelerations\n",
    "    'discrete_steer': [-0.3, 0.0, 0.3],  # discrete value of steering angles\n",
    "    'continuous_accel_range': [-3.0, 3.0],  # continuous acceleration range\n",
    "    'continuous_steer_range': [-0.3, 0.3],  # continuous steering angle range\n",
    "    'ego_vehicle_filter': 'vehicle.lincoln*',  # filter for defining ego vehicle\n",
    "    'port': 2000,  # connection port\n",
    "    'town': 'Town04',  # which town to simulate\n",
    "    'task_mode': 'random',  # mode of the task, [random, roundabout (only for Town03)]\n",
    "    'max_time_episode': 500,  # maximum timesteps per episode\n",
    "    'max_waypt': 100,  # maximum number of waypoints\n",
    "    'obs_range': 32,  # observation range (meter)\n",
    "    'lidar_bin': 0.125,  # bin size of lidar sensor (meter)\n",
    "    'd_behind': 12,  # distance behind the ego vehicle (meter)\n",
    "    'out_lane_thres': 4,  # threshold for out of lane\n",
    "    'desired_speed': 5,  # desired speed (m/s)\n",
    "    'max_ego_spawn_times': 20,  # maximum times to spawn ego vehicle\n",
    "    'display_route': True,  # whether to render the desired route\n",
    "    'pixor_size': 64,  # size of the pixor labels\n",
    "    'pixor': False,  # whether to output PIXOR observation\n",
    "    'routes':None,\n",
    "    'weather':'ClearNoon',\n",
    "    'obs_size':128,\n",
    "    'Collect_Data': False\n",
    "}\n",
    "\n",
    "#{'Town04':{'E':[0,301,334,120,75,51],\n",
    "#                                'M':[191,131,197,210,371,348,141,320],\n",
    "#                                'H':[251,161,264,234,167,182]\n",
    "#                                }\n",
    "#                    },\n",
    "\n",
    "# Set gym-carla environment\n",
    "env = gym.make('carla-v0', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d45005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a90688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        #latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5df687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs,input_size):\n",
    "        super(Full_DQN, self).__init__()\n",
    "\n",
    "        self.lin0 = nn.Linear(input_size+3,256)\n",
    "        self.lin1 = nn.Linear(256,200)\n",
    "        self.lin2 = nn.Linear(200,150)\n",
    "        self.lin3 = nn.Linear(150,100)\n",
    "        self.lin4 = nn.Linear(100,50)\n",
    "        self.lin5 = nn.Linear(50,25)\n",
    "        self.lin6 = nn.Linear(25,15)\n",
    "        self.lin7 = nn.Linear(15,8)\n",
    "        self.lin8 = nn.Linear(8,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.lin0(x))\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        x = F.relu(self.lin6(x))\n",
    "        x = F.relu(self.lin7(x))\n",
    "        x = self.lin8(x)\n",
    "        \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c9bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Linear) or isinstance(layer,torch.nn.Linear):\n",
    "        nn.init.kaiming_uniform_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869cac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 150000\n",
    "TARGET_UPDATE = 256\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = Full_DQN(n_actions,64).to(device)\n",
    "#policy_net.load_state_dict(torch.load('./model_params_CL/Full_model_21.best'))\n",
    "policy_net.apply(initalize_weights)\n",
    "policy_net.train()\n",
    "\n",
    "target_net = Full_DQN(n_actions,64).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "model = PerceptionNet(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./AE_params/model_46.best'))\n",
    "model.eval()\n",
    "#don't want to update AE model \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Model <= 7\n",
    "optimizer = optim.RMSprop(policy_net.parameters(),lr=0.005)\n",
    "memory = ReplayMemory(7500)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #arg max select the idex of the largest value and view changes shape from (1,) to (1,1)\n",
    "            #try test net\n",
    "            return policy_net(state.float()).argmax().view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18320225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(input_size):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    #reshape state_batch for nn\n",
    "  \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    \n",
    "    # selects column of output that was selceted \n",
    "    state_action_values = policy_net(torch.reshape(state_batch,(BATCH_SIZE,1,input_size)).float()).gather(1,action_batch)\n",
    "    \n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(torch.reshape(non_final_next_states,\n",
    "                                        (list(non_final_next_states.shape)[0],1,input_size)).float()).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #gradient clipping\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = {'Town04':{'H':[197]}} \n",
    "env.routes = route\n",
    "env.routes_dict = route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c90a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.74241657981803\n",
      "60.664336180786265\n",
      "60.35194969741189\n",
      "59.87284695839159\n",
      "59.3033809797925\n",
      "58.735898138262755\n",
      "58.200010218899436\n",
      "57.76299299136612\n",
      "57.43018582862546\n",
      "57.10559676258487\n",
      "56.792213023937975\n",
      "56.47758770971691\n",
      "56.147919705258865\n",
      "55.78550743253667\n",
      "55.37514565166605\n",
      "54.9096135625614\n",
      "54.41279959843673\n",
      "53.95354229041613\n",
      "53.534100383976686\n",
      "53.11773543203255\n",
      "52.70503278389634\n",
      "52.27648620827182\n",
      "51.781341615121484\n",
      "51.28826410783137\n",
      "50.81411293106307\n",
      "50.36370062808541\n",
      "49.93912276076862\n",
      "49.53907350025076\n",
      "49.15764657082599\n",
      "48.78652468607172\n",
      "48.42163222742104\n",
      "avg loss:6.36621951963315,Episode:1\n",
      "-45.38763802794543\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.11564230717346\n",
      "448.0867299072901\n",
      "447.94536528378944\n",
      "447.69975814500674\n",
      "447.37762161250174\n",
      "447.0750421363542\n",
      "446.7985085158058\n",
      "446.54961695836687\n",
      "446.36779786963115\n",
      "446.19722052280434\n",
      "446.02518288230203\n",
      "445.8787272528646\n",
      "445.7700375163358\n",
      "445.64990450078125\n",
      "445.5232070699596\n",
      "445.3828275713569\n",
      "445.2360350209153\n",
      "445.0948322412458\n",
      "444.9416219429725\n",
      "444.807379654734\n",
      "444.7161042678476\n",
      "444.6110734372206\n",
      "444.445486132585\n",
      "444.2197902353687\n",
      "444.00222355447426\n",
      "443.86556163637704\n",
      "443.7455716619387\n",
      "443.6169905699923\n",
      "443.5286838775444\n",
      "443.4282902981901\n",
      "443.30591573739133\n",
      "443.1955997958511\n",
      "443.0789700979853\n",
      "442.9357143959128\n",
      "442.79977363606423\n",
      "442.6617934972591\n",
      "avg loss:6.067426071059699,Episode:2\n",
      "-34.0794305969429\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7476224591287\n",
      "489.7121933281159\n",
      "489.5373174715265\n",
      "489.2815037457418\n",
      "488.9808144538265\n",
      "488.6730479238933\n",
      "488.41307143401826\n",
      "488.17370176814165\n",
      "487.9389381146983\n",
      "487.74148422847725\n",
      "487.5832915560778\n",
      "487.44082586358974\n",
      "487.3077035621904\n",
      "487.147601700453\n",
      "486.92199090932763\n",
      "486.64125651520783\n",
      "486.3176969324238\n",
      "485.97574758974764\n",
      "485.6170551679602\n",
      "485.2690846250082\n",
      "484.9336100220428\n",
      "484.5803164345254\n",
      "484.2119410696812\n",
      "483.8568444902331\n",
      "483.48544541799424\n",
      "483.12192191767724\n",
      "482.80532486096683\n",
      "482.5484186426432\n",
      "482.3269996594238\n",
      "482.10198788210266\n",
      "481.8671643866431\n",
      "481.65137279038987\n",
      "481.4675850166599\n",
      "481.29449143041705\n",
      "481.1254873447065\n",
      "480.9266453255246\n",
      "480.6690005817234\n",
      "480.38813900038497\n",
      "480.1214886595538\n",
      "479.8619868112918\n",
      "479.6658346748371\n",
      "479.4679488270134\n",
      "479.2213771254125\n",
      "478.9774100274806\n",
      "478.7434386387655\n",
      "478.48253404577287\n",
      "478.2539571728353\n",
      "478.0595367500737\n",
      "477.908458874714\n",
      "477.74556322577826\n",
      "477.5449524379074\n",
      "477.33215591504614\n",
      "477.0918109182185\n",
      "476.80445622780604\n",
      "476.52316516943733\n",
      "476.2634987794179\n",
      "476.02009575461966\n",
      "475.7855698464495\n",
      "475.5417467409228\n",
      "475.32197873110454\n",
      "475.1259160453968\n",
      "474.9484773524713\n",
      "474.76820325382647\n",
      "474.5760657249973\n",
      "474.3971857914634\n",
      "474.2079406910598\n",
      "473.9883275377269\n",
      "473.7408037177991\n",
      "473.5043437632855\n",
      "473.3101866580456\n",
      "473.08308486653345\n",
      "472.8279115392835\n",
      "472.5624547410463\n",
      "472.3082502614765\n",
      "472.1059491453125\n",
      "471.94725655457825\n",
      "471.8218931929189\n",
      "471.72518149866994\n",
      "471.6179612208117\n",
      "471.50702831676534\n",
      "471.4057984566846\n",
      "471.296691478393\n",
      "471.20393289337187\n",
      "471.105989704824\n",
      "470.98526963069014\n",
      "470.87929313866493\n",
      "470.76530372724255\n",
      "470.62319989558887\n",
      "470.4878863755108\n",
      "avg loss:6.25371453490084,Episode:3\n",
      "53.54492542708694\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.6270303898956\n",
      "615.5723440533231\n",
      "615.315834227851\n",
      "614.9190158908032\n",
      "614.4327048740356\n",
      "613.9364864935065\n",
      "613.4487472140039\n",
      "612.9737896074519\n",
      "612.5307649630593\n",
      "612.1304847027997\n",
      "611.7646452171732\n",
      "611.4416324623146\n",
      "611.1618163470832\n",
      "610.8903944476234\n",
      "610.6176309513338\n",
      "610.3390955552571\n",
      "610.0462160165706\n",
      "609.7158011963486\n",
      "609.3638771318809\n",
      "609.0066276891082\n",
      "608.6451137045649\n",
      "608.2800798474685\n",
      "607.9074323454508\n",
      "607.4737043515512\n",
      "606.9827348709778\n",
      "606.4830229097267\n",
      "605.995745516357\n",
      "605.5200735522365\n",
      "605.0705903430105\n",
      "604.6705970400236\n",
      "604.307342434158\n",
      "603.9557497820294\n",
      "603.6092580873598\n",
      "603.2641763347242\n",
      "602.9126138547495\n",
      "602.5546592825077\n",
      "602.1781079666125\n",
      "601.7765155031683\n",
      "601.3452657049238\n",
      "600.864713848611\n",
      "600.3540597241523\n",
      "599.8591484461457\n",
      "599.3907575510709\n",
      "598.9464759368819\n",
      "598.5155436848142\n",
      "598.0892193330943\n",
      "597.6545549846117\n",
      "597.2249881500272\n",
      "596.8043167644371\n",
      "596.4382900685229\n",
      "596.1303700277142\n",
      "595.8413094213496\n",
      "595.5537760612241\n",
      "595.2554259050684\n",
      "594.9255535315206\n",
      "594.5402312079469\n",
      "avg loss:5.895648408416437,Episode:4\n",
      "-37.482947431358994\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6747335289487\n",
      "526.6961735100533\n",
      "526.8157826286745\n",
      "527.0005061710467\n",
      "527.1727549101975\n",
      "527.3463509980526\n",
      "527.5566841945168\n",
      "527.7484350644677\n",
      "527.9146561093766\n",
      "528.1053804284355\n",
      "528.2989045827475\n",
      "528.4686803169393\n",
      "528.6169319324024\n",
      "528.7481397680714\n",
      "528.8541632111927\n",
      "528.9869636546712\n",
      "529.1528590083207\n",
      "529.3015487597125\n",
      "529.4095097115224\n",
      "529.5268354446713\n",
      "529.6428586689935\n",
      "529.7238920919172\n",
      "529.7944490489292\n",
      "529.8873111213816\n",
      "530.0073975767607\n",
      "530.1510266554718\n",
      "530.294119629052\n",
      "530.3738745540946\n",
      "530.4108424501856\n",
      "530.4265527944228\n",
      "530.4584667584949\n",
      "530.5420154665377\n",
      "530.6210994774975\n",
      "530.6495354127586\n",
      "530.6795542293906\n",
      "530.7301918143385\n",
      "530.8086238991704\n",
      "530.9020292118096\n",
      "531.0121372076723\n",
      "531.1260540847778\n",
      "531.2213388703793\n",
      "531.3184775442654\n",
      "531.4147643449157\n",
      "531.4876302989477\n",
      "531.5602262166949\n",
      "531.6442493750379\n",
      "avg loss:5.282683268269594,Episode:5\n",
      "-25.05748465711261\n",
      "640.8547721229412\n",
      "640.8548111996895\n",
      "640.8548314579967\n",
      "640.854849316527\n",
      "640.8548620221393\n",
      "640.8548666951016\n",
      "640.8548657351907\n",
      "640.8548728079302\n",
      "640.8548728079302\n",
      "640.8548728079302\n",
      "640.8548723279748\n",
      "640.8548723279748\n",
      "640.8548728079302\n",
      "640.8548723279748\n",
      "640.8548723279748\n",
      "640.8548723279748\n",
      "640.8548723279748\n",
      "640.8548723279748\n",
      "640.7824546270945\n",
      "640.5018203832882\n",
      "640.1071947402877\n",
      "639.6648157640321\n",
      "639.2063465370536\n",
      "638.7438698102314\n",
      "638.3042030908878\n",
      "637.8814965524588\n",
      "637.5091518523554\n",
      "637.2122150740308\n",
      "636.93891734199\n",
      "636.6753140428793\n",
      "636.4160299490094\n",
      "636.1472639066913\n",
      "635.866526764628\n",
      "635.5672262052211\n",
      "635.2349711127679\n",
      "634.8750610450131\n",
      "634.5103299097033\n",
      "634.1529465466963\n",
      "633.8072692853289\n",
      "633.4721300652635\n",
      "633.1416355209278\n",
      "632.7734411680879\n",
      "632.3523878228806\n",
      "631.8980277460155\n",
      "631.4574430464972\n",
      "631.0507168394599\n",
      "630.778729399683\n",
      "630.5769258173909\n",
      "630.3645198441607\n",
      "630.1541720078267\n",
      "629.9279705147035\n",
      "629.6725775682014\n",
      "629.4130420555186\n",
      "629.113025170124\n",
      "628.7500205277187\n",
      "628.3435134659946\n",
      "627.944601395728\n",
      "627.5650296448337\n",
      "627.2049654572379\n",
      "626.8635424123062\n",
      "626.5112732317018\n",
      "626.161129200189\n",
      "625.8249948161593\n",
      "625.4862759441829\n",
      "625.1476691440648\n",
      "624.8244372602842\n",
      "624.5656124929369\n",
      "624.3361453456976\n",
      "624.1065893196753\n",
      "623.8624610786698\n",
      "623.5840147480759\n",
      "623.2795255867514\n",
      "622.9378157054568\n",
      "622.5466981018283\n",
      "622.1478446961373\n",
      "621.7575743395431\n",
      "621.383072024414\n",
      "621.0284077794284\n",
      "620.7057857215267\n",
      "620.4128136200426\n",
      "620.1321259930156\n",
      "619.8689701540188\n",
      "619.6202112609295\n",
      "619.353829079196\n",
      "619.0684110394618\n",
      "618.7616186688592\n",
      "618.4513985076445\n",
      "618.1461215215099\n",
      "617.8317880608245\n",
      "617.5214793763452\n",
      "617.2367700328896\n",
      "616.9295307700172\n",
      "avg loss:6.422363024131874,Episode:6\n",
      "-50.67372949249854\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3361150328223\n",
      "493.3029384101309\n",
      "493.1288318912751\n",
      "492.8641163590786\n",
      "492.5452488657847\n",
      "492.1980749928727\n",
      "491.85303780722313\n",
      "491.52934009476763\n",
      "491.2298687635898\n",
      "490.99862020236645\n",
      "490.78447887727305\n",
      "490.5671873250412\n",
      "490.3659344300527\n",
      "490.1582287091735\n",
      "489.8971090906206\n",
      "489.58534797163804\n",
      "489.25909574261016\n",
      "488.9849863330578\n",
      "488.71706456604414\n",
      "488.4216400464682\n",
      "488.10052305602443\n",
      "487.7508618960088\n",
      "487.3504273740632\n",
      "486.9317136169768\n",
      "486.5362545658616\n",
      "486.1650265611181\n",
      "485.8116413741494\n",
      "485.5055094730163\n",
      "485.21066924803796\n",
      "484.929316522775\n",
      "484.6569108407951\n",
      "484.37756030031477\n",
      "484.0794096428565\n",
      "483.7570532248556\n",
      "483.3987443069753\n",
      "483.00117778213416\n",
      "482.58823232383014\n",
      "482.1743856151122\n",
      "481.75571114917506\n",
      "481.3429852212085\n",
      "480.9270761134556\n",
      "480.51778047428314\n",
      "480.1159713943022\n",
      "479.68988062568377\n",
      "479.27471589573696\n",
      "478.86003840469834\n",
      "avg loss:6.176201213138301,Episode:7\n",
      "-29.23267049243052\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.30304744752254\n",
      "143.35526274765104\n",
      "143.6028972863905\n",
      "143.9878597989754\n",
      "144.43469775697235\n",
      "144.84133878750836\n",
      "145.1620472218756\n",
      "145.39207502498053\n",
      "145.6365747582683\n",
      "145.90671208425263\n",
      "146.17240857134996\n",
      "146.41955705142675\n",
      "146.67833731922002\n",
      "146.9583600645131\n",
      "147.2709695810798\n",
      "147.65999011850963\n",
      "148.08545858241746\n",
      "148.50643947598104\n",
      "148.90026235552756\n",
      "149.2810659510472\n",
      "149.66087239129982\n",
      "150.0454230603942\n",
      "150.47111800513736\n",
      "150.8805595888193\n",
      "151.21743829059906\n",
      "151.49835361193718\n",
      "151.78799334657455\n",
      "152.06772013121872\n",
      "152.34014639800313\n",
      "152.6312133555281\n",
      "152.95923407808934\n",
      "153.31720162598003\n",
      "153.69135592963985\n",
      "154.0976998529082\n",
      "154.52539849237158\n",
      "154.9359049362617\n",
      "155.35792908145385\n",
      "155.778807500588\n",
      "156.1900725175227\n",
      "156.591639167174\n",
      "156.97808048984518\n",
      "157.35892063862465\n",
      "157.73893876859674\n",
      "158.05537358285736\n",
      "158.32966517445578\n",
      "158.5953010084616\n",
      "158.86381758942966\n",
      "159.13165319697484\n",
      "159.42335410085948\n",
      "159.76855105754336\n",
      "160.18370296656343\n",
      "160.63283555045587\n",
      "161.0555766295239\n",
      "161.4720083731685\n",
      "161.85935287188428\n",
      "162.19308187255893\n",
      "162.470471170512\n",
      "162.71722557454555\n",
      "162.94428442103504\n",
      "163.17831004619634\n",
      "163.41897265594523\n",
      "163.67509619223685\n",
      "163.97091697608326\n",
      "164.25921641942418\n",
      "164.5436585079991\n",
      "164.8363368390097\n",
      "165.1368660945064\n",
      "165.43418172752126\n",
      "165.72759213350756\n",
      "166.06753188224963\n",
      "166.47726991803972\n",
      "166.87983796423381\n",
      "167.25157258713125\n",
      "167.62275943715807\n",
      "167.99361784982293\n",
      "168.33968494780805\n",
      "168.63444389850352\n",
      "168.92189555503177\n",
      "169.23358534119234\n",
      "169.56326225816215\n",
      "169.91642230620909\n",
      "170.29086720633924\n",
      "170.69935959411401\n",
      "171.14478752176635\n",
      "171.58764074360732\n",
      "172.0286889415805\n",
      "172.4836136716448\n",
      "172.94773972124725\n",
      "173.40582938323723\n",
      "173.82455984676446\n",
      "174.2400155976372\n",
      "174.66199515207376\n",
      "175.0763692429304\n",
      "175.44643289933856\n",
      "175.80698304803246\n",
      "176.17897467097436\n",
      "176.5582406605011\n",
      "176.9379754459242\n",
      "177.30375752920372\n",
      "177.65853804615853\n",
      "178.01001803229394\n",
      "178.33054899205172\n",
      "178.6595649240029\n",
      "179.00031459860224\n",
      "179.35090509275147\n",
      "179.73012614097237\n",
      "180.10293253867087\n",
      "180.43784895210814\n",
      "avg loss:4.702343249888892,Episode:8\n",
      "82.36524589529706\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n",
      "523.1809526805413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11012/1540099565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m#view latent output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_semantic_im\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Carla/gym-carla/gym_carla/envs/carla_env.py\u001b[0m in \u001b[0;36mshow_images\u001b[0;34m(self, new_semantic)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_route\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m       \u001b[0mbirdeye_render_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'waypoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbirdeye_render\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbirdeye_render_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m     \u001b[0mbirdeye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mbirdeye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbirdeye\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Carla/gym-carla/gym_carla/envs/render.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, display, render_types)\u001b[0m\n\u001b[1;32m    534\u001b[0m       self.walker_polygons)\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaypoints_surface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLOR_BLACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     self.render_waypoints(\n\u001b[1;32m    538\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaypoints_surface\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "steps_done = 0\n",
    "use_fixed_idx = 0\n",
    "env.use_fixed = 'H'\n",
    "levels = ['H','H','H']\n",
    "CL = False\n",
    "next_lvl = 300\n",
    "min_overall_loss = 1000\n",
    "min_reward = -1000\n",
    "input_size = 67\n",
    "start_points = [197,348,371,193,192]\n",
    "route_num = 4\n",
    "stage_min_reward = -1000\n",
    "for i_episode in range(1,num_episodes+1):\n",
    "    eps_loss = []\n",
    "    min_loss = 100\n",
    "    rewards = 0\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0][0],ego_dir[1][0]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(1)))\n",
    "    state = torch.tensor(state).reshape(1,3,1,1)\n",
    "\n",
    "    new_obs = torch.tensor(obs['camera'])\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    \n",
    "    _,latent_space = model(new_obs)\n",
    "    state = torch.cat((state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "    \n",
    "\n",
    "    episode_loss = loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        obs, reward, done, info  = env.step(action.item())\n",
    "        \n",
    "        if t > 20:\n",
    "            rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if not done:\n",
    "            \n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0]))\n",
    "            #ang = np.asarray(info['angular_vel'])\n",
    "            #acc = np.asarray(info['acceleration'])\n",
    "            distance_to_last = np.asarray(abs((info['DistanceToLast'][0])))\n",
    "            steer = np.asarray(info['steer'])\n",
    "            \n",
    "            #if furthest waypoint is reached or then calculate a new final waypoint\n",
    "            if distance_to_last < 2:\n",
    "                ego_location = env.ego.get_location()\n",
    "                env.final_waypoint = gym_carla.envs.misc.get_furthest_waypoint(env.waypoints,ego_location.x,ego_location.y)\n",
    "\n",
    "            next_state = np.concatenate((pos, distance_to_last, steer), axis=None)\n",
    "            \n",
    "            new_obs = torch.tensor(obs['camera'])\n",
    "            new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "\n",
    "            #view latent output\n",
    "            env.show_images(np.asarray(generate_semantic_im(new_obs,model)))\n",
    "\n",
    "            _,latent_space = model(new_obs)\n",
    "            info_state = torch.tensor(next_state).reshape(1,3,1,1)\n",
    "            next_state = torch.cat((info_state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "        else:\n",
    "            next_state = None\n",
    "        #simulator doesnt run for first 20 steps due to load time\n",
    "        if t > 20:\n",
    "            memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = optimize_model(input_size)\n",
    "        if loss:\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "            eps_loss.append(loss)\n",
    "        else:\n",
    "            eps_loss.append(1)\n",
    "        \n",
    "         # Update the target network\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            avg_loss = sum(eps_loss)/len(eps_loss)\n",
    "            print('{}:{},{}:{}'.format('avg loss', avg_loss, 'Episode', i_episode))\n",
    "            writer.add_scalar(\"Avg Loss/train\", avg_loss, i_episode)\n",
    "            writer.add_scalar(\"Min Loss/train\", min_loss, i_episode)\n",
    "            break\n",
    "    print(rewards) \n",
    "    if CL==True:\n",
    "        if i_episode % (num_episodes//2) == 0:\n",
    "            torch.save(best_model, './model_params_CL/Full_model_22_'+str(route_num)+'.best')\n",
    "            if i_episode != num_episodes:\n",
    "                route = {'Town04':{'H':[start_points[route_num]]}} \n",
    "                env.routes = route\n",
    "                env.routes_dict = route\n",
    "                route_num += 1\n",
    "                steps_done = 0\n",
    "    \n",
    "    len_episode = t+1\n",
    "    writer.add_scalar(\"Lenght/Epoch\", len_episode, i_episode)\n",
    "    writer.add_scalar(\"Reward/Episode\", rewards, i_episode)\n",
    "    \n",
    "    #save model if better than previous episode\n",
    "    if rewards > min_reward:\n",
    "        min_reward = rewards\n",
    "        torch.save(target_net.state_dict(), './model_params_CL/Full_model_22.best')\n",
    "        best_model = target_net.state_dict()\n",
    "\n",
    "print('Complete')\n",
    "print('Steps Done: ', steps_done)\n",
    "torch.save(target_net.state_dict(), './model_params_CL/Full_model_22.final')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44d9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[413.2103576660156, -142.3896942138672, 270.5986022949219]\n"
     ]
    }
   ],
   "source": [
    "print(env.waypoints[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa528f",
   "metadata": {},
   "source": [
    "### Visualise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_net.state_dict(), './model_params_CL/model_10.final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_net = Full_DQN(n_actions,64).to(device)\n",
    "vis_net.load_state_dict(torch.load('./model_params_CL/Full_model_20.best'))\n",
    "vis_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22edadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./AE_params/dry-wet_3.best'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_model = PerceptionNet()\n",
    "dry_model.to(device)\n",
    "dry_model.load_state_dict(torch.load('./AE_params/model_46.best'))\n",
    "dry_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = {'Town04':{'H':[193]}} \n",
    "env.routes = route\n",
    "env.routes_dict = route\n",
    "env.weather = 'HardRainNoon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21657e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 4\n",
    "env.use_fixed = 'H'\n",
    "env.route_idx = 1\n",
    "levels = ['E','M','H']\n",
    "next_lvl = 4\n",
    "min_overall_loss = 1000\n",
    "input_size = 73\n",
    "for i_episode in range(num_episodes):\n",
    "    eps_loss = []\n",
    "    rewards = []\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset()\n",
    "    #ego_dir retirves the distance and angle from vehicle to nearest waypoint\n",
    "    ego_location = env.ego.get_location()\n",
    "    ego_dir = gym_carla.envs.misc.get_lane_dis(env.waypoints,ego_location.x,ego_location.y)\n",
    "    #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "    ego_pos = np.asarray((ego_dir[0],ego_dir[1][0],ego_dir[1][1]),dtype=np.float32)\n",
    "    state = np.concatenate((ego_pos,np.zeros(6)))\n",
    "    state = torch.tensor(state).reshape(1,9,1,1)\n",
    "\n",
    "    new_obs = torch.tensor(obs['camera'])\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    \n",
    "    _,latent_space = model(new_obs)\n",
    "    state = torch.cat((state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    loss = episode_loss = 1000\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = vis_net(state.float()).argmax().view(1,1)\n",
    "            obs, reward, done, info  = env.step(action.item())\n",
    "            \n",
    "            #save_images(obs['camera'],sem_image,t)\n",
    "            new_obs = torch.tensor(obs['camera'])\n",
    "            new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "            _,latent_space = model(new_obs)\n",
    "\n",
    "            sem = dry_model.decode(latent_space).detach().cpu().argmax(dim=1)\n",
    "            sem = replace(sem.numpy().reshape(1,128,128).transpose(1,2,0))\n",
    "            env.show_images(sem)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            #pos gets a distanc d and array w which has to be seperated out in below line\n",
    "            pos = np.asarray((info['position'][0],info['position'][1][0],info['position'][1][1]))\n",
    "            ang = np.asarray(info['angular_vel'])\n",
    "            acc = np.asarray(info['acceleration'])\n",
    "            steer = np.asarray(info['steer'])\n",
    "            next_state = np.concatenate((pos, ang, acc, steer), axis=None)\n",
    "            \n",
    "            \n",
    "            info_state = torch.tensor(next_state).reshape(1,9,1,1)\n",
    "            next_state = torch.cat((info_state,latent_space.cpu()),1).reshape(1,input_size)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('########')\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def save_images(rgb,generated,step):\n",
    "    im = Image.fromarray(rgb)\n",
    "    im.save('./Datasets/movie/RGB/'+str(step)+\".jpeg\")\n",
    "    generated.save('./Datasets/movie/Sem/'+str(step)+'.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04bc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[0,0,0],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a816a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2411a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = torch.tensor(RGB_image)\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    out,latent_space = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33802985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
