{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81560af3",
   "metadata": {},
   "source": [
    "# Learning to Drive in Adverse Weather Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82560c8",
   "metadata": {},
   "source": [
    "## Pre-Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2e540",
   "metadata": {},
   "source": [
    "- Only got access to lab computers on 04/11/21 (Week 8)\n",
    "- Had to update version of Firefox as github would not work correctly\n",
    "- No sudo access so could only install things that were packaged or had their own installer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5a3b3",
   "metadata": {},
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdc298",
   "metadata": {},
   "source": [
    "### Setup Carla & Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87159d17",
   "metadata": {},
   "source": [
    "- Carla 0.9.12 was installed on the machines by IT in /opt little setup required just figuring out configs and some crashing errors\n",
    "- Anaconda was installed from a package, Quite easy and quick \n",
    "- Issues installing Pytorch into env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fc8bb",
   "metadata": {},
   "source": [
    "### Install Carla Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d415b29",
   "metadata": {},
   "source": [
    "- Installing Carla Gym not too hard, API not very well documented so had dig through code files\n",
    "- Some Issues as pygame only works with python <= 3.7, overcame this with anaconda env (there is a 3.8 version but it requires a sudo install)\n",
    "- Had to update some code do to older version of Carla being used (See Adapt Carla Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e162c",
   "metadata": {},
   "source": [
    "### Additional Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1ab95",
   "metadata": {},
   "source": [
    "- interp e2e driving installed fine but crashed after a number of runs due to low memory (Had to remove Logging as it was for TF 1.0)\n",
    "- d4rl would not install due to mujuco dependency (May be free now)\n",
    "- didn't try d3rlpy ye"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8e6ac",
   "metadata": {},
   "source": [
    "### Adapt Carla Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbb4ea",
   "metadata": {},
   "source": [
    "- Fix not deleting sensors correctly bug\n",
    "- Lidar Location errorhad to be fixed\n",
    "- Change Reward function the env return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468270f",
   "metadata": {},
   "source": [
    "### Alter Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22140db4",
   "metadata": {},
   "source": [
    "- Reached out to author of Latent Space driving paper and was recommended to follow [this](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) tutorial\n",
    "- Began tyring to get an understanding of the code and what it was doing\n",
    "- Changed the input from being gotten from getscreen() to using the returned image data from gym-env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc98de",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760adab7",
   "metadata": {},
   "source": [
    "### First Experiment - Minimize Distance to Waypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a68f46",
   "metadata": {},
   "source": [
    "- Recommed to start with an easier setup using minimum distance to waypoint\n",
    "- Used Town04 as it has more wide roads and less severe turns to start, \n",
    "- Changed carla env to output currentl vehicle states: (position relative to new waypoint, acceleration, angular velocity, previous steering angle)\n",
    "- Used Reward from Latent Space Paper\n",
    "- Implemented a single layer dense network\n",
    "- Implemented some logging of the data (Loss, avg_reward (removed) and lenght of episode) and view with tensorboard\n",
    "- Error: Was using absolute x,y position but changed to distance and angle from current way point instead (This changed input from 8,1 vec to 9,1 vec\n",
    "- Ran 100 and 500 episodes in the above model but results were poor and seemed to have little improvement\n",
    "- Changed archtechture to have 3 dense layers (9,15)(15,9)(9,3) this showed an immediate large improvment in perfromance\n",
    "- Agent was able to drive on straights and less severe turns quite well but has issues making sharp turns as it tries to premeptively turn to the waypoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc2797",
   "metadata": {},
   "source": [
    "### Week 3\n",
    "### First Exp ctd, Carla env and 2nd Exp - Minimize Distance to Waypoint with CL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5f7da",
   "metadata": {},
   "source": [
    "- Started Route Implementation\n",
    "- Fixed Speed issue \n",
    "- Catestrophic Forgetting happening after target net update\n",
    "- Ran for 1000 steps and had forgetting at step 256\n",
    "- Running changing target update to 20 and running for 100 did not improve results (Model 6)\n",
    "- Running changin target update to 50 and running for 100, as of 400 best performing model so far\n",
    "    - avg loss is about 0.03\n",
    "- Start Fix Routes (See if route generation is deterministic and only need to specify starting poin ???)\n",
    "    - Picked 20 routes with different levels of diffcult (5 Easy, 2 Medium Easy, 6 Medium, 3 Medium Hard and 4 Hard)\n",
    "    - Easy -> Straight\n",
    "    - Medium Easy -> Straight into slight turn\n",
    "    - Medium -> Medium Turn \n",
    "    - Medium Hard -> Straight into Hard turn\n",
    "    - Hard -> Hard Turn\n",
    "- Running first test for 500 episodes using set routes, keep eps lenght = 250, test adam opt with lr = 0.00025 (previous = 0.01), target update at 50 (Could not get to lower loss as much higher level of loss)\n",
    "- Set up CL learning so would change level or routes every 100 steps\n",
    "- Checkpointing best model from run\n",
    "- Forked gym-carla properly to share and pushed alterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801f881",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5fe356",
   "metadata": {},
   "source": [
    "### Long Term Goals\n",
    "- Rebuild Carla Gym\n",
    "- Implement Perceptron & Control Modules\n",
    "- Train on altering weather conditions\n",
    "- Train against GAN\n",
    "- Meta-Learning ?\n",
    "\n",
    "### Medium Term Goals\n",
    "- Start Autoencoder Training\n",
    "- Plan for Semester 2\n",
    "- Standardize testing\n",
    "- Train Perceptron in Standard conditions\n",
    "\n",
    "### Short Term\n",
    "\n",
    "- Look up best optimser and lr and loss function\n",
    "- Video of best results\n",
    "- Cumlative Reward\n",
    "- Sharp Turns\n",
    "- Start CNN training\n",
    "- Write up plan\n",
    "\n",
    "### Done\n",
    "- ~~Stabilize Training~~\n",
    "- ~~Try Longer run~~\n",
    "- ~~Add more dense layers~~ \n",
    "- ~~Fix speed~~ \n",
    "- ~~Change Avg reward to length of episode~~\n",
    "- ~~Fixed Routes~~\n",
    "- ~~Setup Checkpoints~~\n",
    "- ~~Setup forked gym~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01edf3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
