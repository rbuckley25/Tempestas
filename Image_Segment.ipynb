{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ec9984",
   "metadata": {},
   "source": [
    "# Learning to Drive in Adverse Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6519255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07881483",
   "metadata": {},
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "This project is designed to explore using Reinforcement learning to teach an autonomous agent to drive in adverse weather conditions.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- The project will be performed using an autonomous drving simulator called CARLA.\n",
    "- Python 3.8\n",
    "- Anconda\n",
    "\n",
    "[Project Github](https://github.com/rbuckley25/Tempestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdd1a5",
   "metadata": {},
   "source": [
    "### Image Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc38f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import carla\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "from models import PerceptionNet\n",
    "from utils import *\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58bddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, weather, town, test=False , transform=None, target_transform=None):\n",
    "        dirt = './Datasets/'+weather+'/'+town\n",
    "        if test:\n",
    "            dirt = dirt+'/test'\n",
    "        \n",
    "        self.sem_dir = dirt+'/Semantic'\n",
    "        self.rgb_dir = dirt+'/RGB'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.names = os.listdir(self.rgb_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.sem_dir))\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = os.path.join(self.rgb_dir, self.names[idx])\n",
    "        image = read_image(img_path)\n",
    "        label_name = self.names[idx].split('.')[0]+'.npy'\n",
    "        label = np.load(os.path.join(self.sem_dir, label_name))\n",
    "        label = torch.tensor(label).permute(2,0,1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4de061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropResizeTransform:\n",
    "    def __init__(self, top, left, width, height, size):\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return TF.resized_crop(x,self.top,self.left,self.width,self.height,self.size)\n",
    "\n",
    "class Hflip:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return TF.hflip(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378cfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEPerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VAEPerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x), self.conv6b(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        mu,logvar = self.encode(x)\n",
    "        latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fda9eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerceptionNet(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20e4d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Conv2d) or isinstance(layer,torch.nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "050d50fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initalize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46cb67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=lable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc66f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_batch,pred,mu,logvar):\n",
    "    cross = nn.CrossEntropyLoss(weight=lable_weights)\n",
    "    rec_loss = cross(y_batch,pred)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rec_loss + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337d41d",
   "metadata": {},
   "source": [
    "Reference: https://github.com/pytorch/examples/blob/master/vae/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd868ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = CropResizeTransform(64,14,50,100,(128,128))\n",
    "town03_data = CustomImageDataset('ClearNoon','Town03',test=False)\n",
    "town03_test_data = CustomImageDataset('ClearNoon','Town03',test=True)\n",
    "town04_data = CustomImageDataset('ClearNoon','Town04',test=False)\n",
    "town04_test_data = CustomImageDataset('ClearNoon','Town04',test=True)\n",
    "town07_data = CustomImageDataset('ClearNoon','Town07',test=False)\n",
    "#flipped data\n",
    "town03_data_hf = CustomImageDataset('ClearNoon','Town03',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town03_test_data_hf = CustomImageDataset('ClearNoon','Town03',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town04_data_hf = CustomImageDataset('ClearNoon','Town04',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town04_test_data_hf = CustomImageDataset('ClearNoon','Town04',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town07_data_hf = CustomImageDataset('ClearNoon','Town07',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "\n",
    "#town03_data_cropped = CustomImageDataset('Town03','.',test=False,transform=crop,target_transform=crop)\n",
    "#town03_test_data_cropped = CustomImageDataset('Town03','.',test=True,transform=crop,target_transform=crop)\n",
    "#town04_data_cropped = CustomImageDataset('Town04','.',test=False,transform=crop,target_transform=crop)\n",
    "#town04_test_data_cropped = CustomImageDataset('Town04','.',test=True,transform=crop,target_transform=crop)\n",
    "\n",
    "\n",
    "\n",
    "train_data = ConcatDataset([town03_data,town04_data,town07_data,town03_data_hf,town04_data_hf,town07_data_hf,\n",
    "                            ])\n",
    "#town03_data_cropped, town04_data_cropped\n",
    "test_data = ConcatDataset([town03_test_data,town04_test_data,town03_test_data_hf,town04_test_data_hf,\n",
    "                            ])\n",
    "#town03_test_data_cropped, town04_test_data_cropped\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138d98a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70488"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate label frequency for loss weigthing\n",
    "label_freq_weights = np.zeros(13)\n",
    "count = 0\n",
    "for index, image in enumerate(train_data):\n",
    "    if index % 100 == 0:\n",
    "        im = recode_tags(image[1],recode_dict)\n",
    "        values, counts = np.unique(im,return_counts=True)\n",
    "        label_freq_weights[values] += counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f80559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_4 = np.array([1121034.,  530218.,  116395.,  193273.,   23201.,   83135.,\n",
    "        258966., 6886841.,  121457.,  777388.,   23087.,  230427.,\n",
    "         71186.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5775d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 28.02585 ,  13.25545 ,   2.909875,   4.831825,   0.580025,\n",
       "         2.078375,   6.47415 , 172.171025,   3.036425,  19.4347  ,\n",
       "         0.577175,   5.760675,   1.77965 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_freq_weights_4/40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd54795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_3 = np.array([ 426548.,  103471.,   59734.,   61954.,    3507.,   17818.,\n",
    "         40000., 2355300.,   28709.,  355806.,    4624.,   49085.,\n",
    "         21033.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "67739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b77f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_2 = np.array([2198123.,  321519.,   74114.,  139571.,    9590.,   49337.,\n",
    "         94720., 1339440.,   78098.,  407971., 1242280.,  132659.,\n",
    "         40194.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a385913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to caculate an individual weight for each sample for BCELoss\n",
    "def calculate_weights(im_batch):\n",
    "    weights = torch.empty(13)\n",
    "    counts = torch.bincount(im_batch[i].reshape(128*128),minlength=13).numpy()\n",
    "    counts = np.where(counts < 136.,  765703., 4474860.,  176931.,\n",
    "         46304.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb8dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lables = np.where(label_freq_weights_3 < 10000, 15000, label_freq_weights_3)\n",
    "\n",
    "inverse_lables = 1/clean_lables\n",
    "normalized_lables = inverse_lables/sum(inverse_lables)\n",
    "lable_weights = torch.tensor(normalized_lables,dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9fdde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fad0a889ed0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(normalized_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to caculate an individual weight for each sample for BCELoss\n",
    "def calculate_weights(im_batch):\n",
    "    weights = torch.empty(13)\n",
    "    counts = torch.bincount(im_batch[i].reshape(128*128),minlength=13).numpy()\n",
    "    counts = np.where(counts < 1, 1, counts)\n",
    "    weights = torch.tensor((1/counts)/sum(1/counts))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d760b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/70488 (0%)]\tLoss: 1.304208\n",
      "0\n",
      "Train Epoch: 1 [512/70488 (1%)]\tLoss: 1.301491\n",
      "1\n",
      "Train Epoch: 1 [1024/70488 (1%)]\tLoss: 1.297177\n",
      "2\n",
      "Train Epoch: 1 [1536/70488 (2%)]\tLoss: 1.291199\n",
      "3\n",
      "Train Epoch: 1 [2048/70488 (3%)]\tLoss: 1.285601\n",
      "4\n",
      "Train Epoch: 1 [2560/70488 (4%)]\tLoss: 1.282356\n",
      "5\n",
      "Train Epoch: 1 [3072/70488 (4%)]\tLoss: 1.278005\n",
      "6\n",
      "Train Epoch: 1 [3584/70488 (5%)]\tLoss: 1.275161\n",
      "7\n",
      "Train Epoch: 1 [4096/70488 (6%)]\tLoss: 1.270067\n",
      "8\n",
      "Train Epoch: 1 [4608/70488 (7%)]\tLoss: 1.264881\n",
      "9\n",
      "Train Epoch: 1 [5120/70488 (7%)]\tLoss: 1.261491\n",
      "10\n",
      "Train Epoch: 1 [5632/70488 (8%)]\tLoss: 1.255329\n",
      "11\n",
      "Train Epoch: 1 [6144/70488 (9%)]\tLoss: 1.248807\n",
      "12\n",
      "Train Epoch: 1 [6656/70488 (9%)]\tLoss: 1.246168\n",
      "13\n",
      "Train Epoch: 1 [7168/70488 (10%)]\tLoss: 1.238505\n",
      "14\n",
      "Train Epoch: 1 [7680/70488 (11%)]\tLoss: 1.231486\n",
      "15\n",
      "Train Epoch: 1 [8192/70488 (12%)]\tLoss: 1.223813\n",
      "16\n",
      "Train Epoch: 1 [8704/70488 (12%)]\tLoss: 1.219899\n",
      "17\n",
      "Train Epoch: 1 [9216/70488 (13%)]\tLoss: 1.216299\n",
      "18\n",
      "Train Epoch: 1 [9728/70488 (14%)]\tLoss: 1.211539\n",
      "19\n",
      "Train Epoch: 1 [10240/70488 (14%)]\tLoss: 1.206263\n",
      "20\n",
      "Train Epoch: 1 [10752/70488 (15%)]\tLoss: 1.202107\n",
      "21\n",
      "Train Epoch: 1 [11264/70488 (16%)]\tLoss: 1.196097\n",
      "22\n",
      "Train Epoch: 1 [11776/70488 (17%)]\tLoss: 1.190015\n",
      "23\n",
      "Train Epoch: 1 [12288/70488 (17%)]\tLoss: 1.189957\n",
      "24\n",
      "Train Epoch: 1 [12800/70488 (18%)]\tLoss: 1.183458\n",
      "25\n",
      "Train Epoch: 1 [13312/70488 (19%)]\tLoss: 1.181961\n",
      "26\n",
      "Train Epoch: 1 [13824/70488 (20%)]\tLoss: 1.176865\n",
      "27\n",
      "Train Epoch: 1 [14336/70488 (20%)]\tLoss: 1.167854\n",
      "28\n",
      "Train Epoch: 1 [14848/70488 (21%)]\tLoss: 1.163838\n",
      "29\n",
      "Train Epoch: 1 [15360/70488 (22%)]\tLoss: 1.155881\n",
      "30\n",
      "Train Epoch: 1 [15872/70488 (22%)]\tLoss: 1.153656\n",
      "31\n",
      "Train Epoch: 1 [16384/70488 (23%)]\tLoss: 1.155218\n",
      "32\n",
      "Train Epoch: 1 [16896/70488 (24%)]\tLoss: 1.145009\n",
      "33\n",
      "Train Epoch: 1 [17408/70488 (25%)]\tLoss: 1.139981\n",
      "34\n",
      "Train Epoch: 1 [17920/70488 (25%)]\tLoss: 1.140927\n",
      "35\n",
      "Train Epoch: 1 [18432/70488 (26%)]\tLoss: 1.137716\n",
      "36\n",
      "Train Epoch: 1 [18944/70488 (27%)]\tLoss: 1.131025\n",
      "37\n",
      "Train Epoch: 1 [19456/70488 (28%)]\tLoss: 1.129883\n",
      "38\n",
      "Train Epoch: 1 [19968/70488 (28%)]\tLoss: 1.130656\n",
      "39\n",
      "Train Epoch: 1 [20480/70488 (29%)]\tLoss: 1.123890\n",
      "40\n",
      "Train Epoch: 1 [20992/70488 (30%)]\tLoss: 1.124166\n",
      "41\n",
      "Train Epoch: 1 [21504/70488 (30%)]\tLoss: 1.122032\n",
      "42\n",
      "Train Epoch: 1 [22016/70488 (31%)]\tLoss: 1.118784\n",
      "43\n",
      "Train Epoch: 1 [22528/70488 (32%)]\tLoss: 1.114702\n",
      "44\n",
      "Train Epoch: 1 [23040/70488 (33%)]\tLoss: 1.117640\n",
      "45\n",
      "Train Epoch: 1 [23552/70488 (33%)]\tLoss: 1.112757\n",
      "46\n",
      "Train Epoch: 1 [24064/70488 (34%)]\tLoss: 1.108504\n",
      "47\n",
      "Train Epoch: 1 [24576/70488 (35%)]\tLoss: 1.108498\n",
      "48\n",
      "Train Epoch: 1 [25088/70488 (36%)]\tLoss: 1.107773\n",
      "49\n",
      "Train Epoch: 1 [25600/70488 (36%)]\tLoss: 1.103520\n",
      "50\n",
      "Train Epoch: 1 [26112/70488 (37%)]\tLoss: 1.107856\n",
      "51\n",
      "Train Epoch: 1 [26624/70488 (38%)]\tLoss: 1.106963\n",
      "52\n",
      "Train Epoch: 1 [27136/70488 (38%)]\tLoss: 1.104491\n",
      "53\n",
      "Train Epoch: 1 [27648/70488 (39%)]\tLoss: 1.099161\n",
      "54\n",
      "Train Epoch: 1 [28160/70488 (40%)]\tLoss: 1.098165\n",
      "55\n",
      "Train Epoch: 1 [28672/70488 (41%)]\tLoss: 1.096191\n",
      "56\n",
      "Train Epoch: 1 [29184/70488 (41%)]\tLoss: 1.099573\n",
      "57\n",
      "Train Epoch: 1 [29696/70488 (42%)]\tLoss: 1.095600\n",
      "58\n",
      "Train Epoch: 1 [30208/70488 (43%)]\tLoss: 1.093957\n",
      "59\n",
      "Train Epoch: 1 [30720/70488 (43%)]\tLoss: 1.090238\n",
      "60\n",
      "Train Epoch: 1 [31232/70488 (44%)]\tLoss: 1.092499\n",
      "61\n",
      "Train Epoch: 1 [31744/70488 (45%)]\tLoss: 1.093127\n",
      "62\n",
      "Train Epoch: 1 [32256/70488 (46%)]\tLoss: 1.088861\n",
      "63\n",
      "Train Epoch: 1 [32768/70488 (46%)]\tLoss: 1.090373\n",
      "64\n",
      "Train Epoch: 1 [33280/70488 (47%)]\tLoss: 1.083892\n",
      "65\n",
      "Train Epoch: 1 [33792/70488 (48%)]\tLoss: 1.088207\n",
      "66\n",
      "Train Epoch: 1 [34304/70488 (49%)]\tLoss: 1.081146\n",
      "67\n",
      "Train Epoch: 1 [34816/70488 (49%)]\tLoss: 1.082081\n",
      "68\n",
      "Train Epoch: 1 [35328/70488 (50%)]\tLoss: 1.079225\n",
      "69\n",
      "Train Epoch: 1 [35840/70488 (51%)]\tLoss: 1.079931\n",
      "70\n",
      "Train Epoch: 1 [36352/70488 (51%)]\tLoss: 1.078347\n",
      "71\n",
      "Train Epoch: 1 [36864/70488 (52%)]\tLoss: 1.075187\n",
      "72\n",
      "Train Epoch: 1 [37376/70488 (53%)]\tLoss: 1.076211\n",
      "73\n",
      "Train Epoch: 1 [37888/70488 (54%)]\tLoss: 1.075920\n",
      "74\n",
      "Train Epoch: 1 [38400/70488 (54%)]\tLoss: 1.074926\n",
      "75\n",
      "Train Epoch: 1 [38912/70488 (55%)]\tLoss: 1.071866\n",
      "76\n",
      "Train Epoch: 1 [39424/70488 (56%)]\tLoss: 1.073736\n",
      "77\n",
      "Train Epoch: 1 [39936/70488 (57%)]\tLoss: 1.073384\n",
      "78\n",
      "Train Epoch: 1 [40448/70488 (57%)]\tLoss: 1.071114\n",
      "79\n",
      "Train Epoch: 1 [40960/70488 (58%)]\tLoss: 1.069874\n",
      "80\n",
      "Train Epoch: 1 [41472/70488 (59%)]\tLoss: 1.072626\n",
      "81\n",
      "Train Epoch: 1 [41984/70488 (59%)]\tLoss: 1.066069\n",
      "82\n",
      "Train Epoch: 1 [42496/70488 (60%)]\tLoss: 1.064179\n",
      "83\n",
      "Train Epoch: 1 [43008/70488 (61%)]\tLoss: 1.063608\n",
      "84\n",
      "Train Epoch: 1 [43520/70488 (62%)]\tLoss: 1.064806\n",
      "85\n",
      "Train Epoch: 1 [44032/70488 (62%)]\tLoss: 1.064801\n",
      "86\n",
      "Train Epoch: 1 [44544/70488 (63%)]\tLoss: 1.063474\n",
      "87\n",
      "Train Epoch: 1 [45056/70488 (64%)]\tLoss: 1.061538\n",
      "88\n",
      "Train Epoch: 1 [45568/70488 (64%)]\tLoss: 1.059895\n",
      "89\n",
      "Train Epoch: 1 [46080/70488 (65%)]\tLoss: 1.060946\n",
      "90\n",
      "Train Epoch: 1 [46592/70488 (66%)]\tLoss: 1.059420\n",
      "91\n",
      "Train Epoch: 1 [47104/70488 (67%)]\tLoss: 1.060491\n",
      "92\n",
      "Train Epoch: 1 [47616/70488 (67%)]\tLoss: 1.061039\n",
      "93\n",
      "Train Epoch: 1 [48128/70488 (68%)]\tLoss: 1.059049\n",
      "94\n",
      "Train Epoch: 1 [48640/70488 (69%)]\tLoss: 1.058101\n",
      "95\n",
      "Train Epoch: 1 [49152/70488 (70%)]\tLoss: 1.052808\n",
      "96\n",
      "Train Epoch: 1 [49664/70488 (70%)]\tLoss: 1.057429\n",
      "97\n",
      "Train Epoch: 1 [50176/70488 (71%)]\tLoss: 1.054178\n",
      "98\n",
      "Train Epoch: 1 [50688/70488 (72%)]\tLoss: 1.058121\n",
      "99\n",
      "Train Epoch: 1 [51200/70488 (72%)]\tLoss: 1.053271\n",
      "100\n",
      "Train Epoch: 1 [51712/70488 (73%)]\tLoss: 1.055926\n",
      "101\n",
      "Train Epoch: 1 [52224/70488 (74%)]\tLoss: 1.053142\n",
      "102\n",
      "Train Epoch: 1 [52736/70488 (75%)]\tLoss: 1.048660\n",
      "103\n",
      "Train Epoch: 1 [53248/70488 (75%)]\tLoss: 1.055860\n",
      "104\n",
      "Train Epoch: 1 [53760/70488 (76%)]\tLoss: 1.051979\n",
      "105\n",
      "Train Epoch: 1 [54272/70488 (77%)]\tLoss: 1.046810\n",
      "106\n",
      "Train Epoch: 1 [54784/70488 (78%)]\tLoss: 1.048098\n",
      "107\n",
      "Train Epoch: 1 [55296/70488 (78%)]\tLoss: 1.045295\n",
      "108\n",
      "Train Epoch: 1 [55808/70488 (79%)]\tLoss: 1.050010\n",
      "109\n",
      "Train Epoch: 1 [56320/70488 (80%)]\tLoss: 1.048474\n",
      "110\n",
      "Train Epoch: 1 [56832/70488 (80%)]\tLoss: 1.048244\n",
      "111\n",
      "Train Epoch: 1 [57344/70488 (81%)]\tLoss: 1.042635\n",
      "112\n",
      "Train Epoch: 1 [57856/70488 (82%)]\tLoss: 1.043126\n",
      "113\n",
      "Train Epoch: 1 [58368/70488 (83%)]\tLoss: 1.039727\n",
      "114\n",
      "Train Epoch: 1 [58880/70488 (83%)]\tLoss: 1.044065\n",
      "115\n",
      "Train Epoch: 1 [59392/70488 (84%)]\tLoss: 1.039865\n",
      "116\n",
      "Train Epoch: 1 [59904/70488 (85%)]\tLoss: 1.043537\n",
      "117\n",
      "Train Epoch: 1 [60416/70488 (86%)]\tLoss: 1.039523\n",
      "118\n",
      "Train Epoch: 1 [60928/70488 (86%)]\tLoss: 1.038698\n",
      "119\n",
      "Train Epoch: 1 [61440/70488 (87%)]\tLoss: 1.036471\n",
      "120\n",
      "Train Epoch: 1 [61952/70488 (88%)]\tLoss: 1.035202\n",
      "121\n",
      "Train Epoch: 1 [62464/70488 (88%)]\tLoss: 1.036756\n",
      "122\n",
      "Train Epoch: 1 [62976/70488 (89%)]\tLoss: 1.036093\n",
      "123\n",
      "Train Epoch: 1 [63488/70488 (90%)]\tLoss: 1.036487\n",
      "124\n",
      "Train Epoch: 1 [64000/70488 (91%)]\tLoss: 1.031974\n",
      "125\n",
      "Train Epoch: 1 [64512/70488 (91%)]\tLoss: 1.031379\n",
      "126\n",
      "Train Epoch: 1 [65024/70488 (92%)]\tLoss: 1.034719\n",
      "127\n",
      "Train Epoch: 1 [65536/70488 (93%)]\tLoss: 1.030455\n",
      "128\n",
      "Train Epoch: 1 [66048/70488 (93%)]\tLoss: 1.028923\n",
      "129\n",
      "Train Epoch: 1 [66560/70488 (94%)]\tLoss: 1.033386\n",
      "130\n",
      "Train Epoch: 1 [67072/70488 (95%)]\tLoss: 1.029702\n",
      "131\n",
      "Train Epoch: 1 [67584/70488 (96%)]\tLoss: 1.033435\n",
      "132\n",
      "Train Epoch: 1 [68096/70488 (96%)]\tLoss: 1.027321\n",
      "133\n",
      "Train Epoch: 1 [68608/70488 (97%)]\tLoss: 1.030325\n",
      "134\n",
      "Train Epoch: 1 [69120/70488 (98%)]\tLoss: 1.027849\n",
      "135\n",
      "Train Epoch: 1 [69632/70488 (99%)]\tLoss: 1.032900\n",
      "136\n",
      "Train Epoch: 1 [47128/70488 (99%)]\tLoss: 1.028566\n",
      "137\n",
      "====> Epoch: 1 Average loss: 0.00434167\n",
      "====> Test set loss: 31.30219269\n",
      "Train Epoch: 2 [0/70488 (0%)]\tLoss: 1.026924\n",
      "148\n",
      "Train Epoch: 2 [512/70488 (1%)]\tLoss: 1.025511\n",
      "149\n",
      "Train Epoch: 2 [1024/70488 (1%)]\tLoss: 1.023739\n",
      "150\n",
      "Train Epoch: 2 [1536/70488 (2%)]\tLoss: 1.024358\n",
      "151\n",
      "Train Epoch: 2 [2048/70488 (3%)]\tLoss: 1.019546\n",
      "152\n",
      "Train Epoch: 2 [2560/70488 (4%)]\tLoss: 1.021738\n",
      "153\n",
      "Train Epoch: 2 [3072/70488 (4%)]\tLoss: 1.023388\n",
      "154\n",
      "Train Epoch: 2 [3584/70488 (5%)]\tLoss: 1.025909\n",
      "155\n",
      "Train Epoch: 2 [4096/70488 (6%)]\tLoss: 1.019918\n",
      "156\n",
      "Train Epoch: 2 [4608/70488 (7%)]\tLoss: 1.016956\n",
      "157\n",
      "Train Epoch: 2 [5120/70488 (7%)]\tLoss: 1.020106\n",
      "158\n",
      "Train Epoch: 2 [5632/70488 (8%)]\tLoss: 1.015734\n",
      "159\n",
      "Train Epoch: 2 [6144/70488 (9%)]\tLoss: 1.021461\n",
      "160\n",
      "Train Epoch: 2 [6656/70488 (9%)]\tLoss: 1.017556\n",
      "161\n",
      "Train Epoch: 2 [7168/70488 (10%)]\tLoss: 1.019315\n",
      "162\n",
      "Train Epoch: 2 [7680/70488 (11%)]\tLoss: 1.014005\n",
      "163\n",
      "Train Epoch: 2 [8192/70488 (12%)]\tLoss: 1.018213\n",
      "164\n",
      "Train Epoch: 2 [8704/70488 (12%)]\tLoss: 1.016614\n",
      "165\n",
      "Train Epoch: 2 [9216/70488 (13%)]\tLoss: 1.015166\n",
      "166\n",
      "Train Epoch: 2 [9728/70488 (14%)]\tLoss: 1.014356\n",
      "167\n",
      "Train Epoch: 2 [10240/70488 (14%)]\tLoss: 1.015351\n",
      "168\n",
      "Train Epoch: 2 [10752/70488 (15%)]\tLoss: 1.016241\n",
      "169\n",
      "Train Epoch: 2 [11264/70488 (16%)]\tLoss: 1.010839\n",
      "170\n",
      "Train Epoch: 2 [11776/70488 (17%)]\tLoss: 1.013685\n",
      "171\n",
      "Train Epoch: 2 [12288/70488 (17%)]\tLoss: 1.009404\n",
      "172\n",
      "Train Epoch: 2 [12800/70488 (18%)]\tLoss: 1.016732\n",
      "173\n",
      "Train Epoch: 2 [13312/70488 (19%)]\tLoss: 1.011423\n",
      "174\n",
      "Train Epoch: 2 [13824/70488 (20%)]\tLoss: 1.010116\n",
      "175\n",
      "Train Epoch: 2 [14336/70488 (20%)]\tLoss: 1.008349\n",
      "176\n",
      "Train Epoch: 2 [14848/70488 (21%)]\tLoss: 1.009024\n",
      "177\n",
      "Train Epoch: 2 [15360/70488 (22%)]\tLoss: 1.009212\n",
      "178\n",
      "Train Epoch: 2 [15872/70488 (22%)]\tLoss: 1.010042\n",
      "179\n",
      "Train Epoch: 2 [16384/70488 (23%)]\tLoss: 1.009114\n",
      "180\n",
      "Train Epoch: 2 [16896/70488 (24%)]\tLoss: 1.010776\n",
      "181\n",
      "Train Epoch: 2 [17408/70488 (25%)]\tLoss: 1.009601\n",
      "182\n",
      "Train Epoch: 2 [17920/70488 (25%)]\tLoss: 1.007993\n",
      "183\n",
      "Train Epoch: 2 [18432/70488 (26%)]\tLoss: 1.006885\n",
      "184\n",
      "Train Epoch: 2 [18944/70488 (27%)]\tLoss: 1.007012\n",
      "185\n",
      "Train Epoch: 2 [19456/70488 (28%)]\tLoss: 1.004933\n",
      "186\n",
      "Train Epoch: 2 [19968/70488 (28%)]\tLoss: 1.011689\n",
      "187\n",
      "Train Epoch: 2 [20480/70488 (29%)]\tLoss: 1.008588\n",
      "188\n",
      "Train Epoch: 2 [20992/70488 (30%)]\tLoss: 1.004703\n",
      "189\n",
      "Train Epoch: 2 [21504/70488 (30%)]\tLoss: 1.004854\n",
      "190\n",
      "Train Epoch: 2 [22016/70488 (31%)]\tLoss: 1.001670\n",
      "191\n",
      "Train Epoch: 2 [22528/70488 (32%)]\tLoss: 0.999473\n",
      "192\n",
      "Train Epoch: 2 [23040/70488 (33%)]\tLoss: 1.003125\n",
      "193\n",
      "Train Epoch: 2 [23552/70488 (33%)]\tLoss: 1.005278\n",
      "194\n",
      "Train Epoch: 2 [24064/70488 (34%)]\tLoss: 1.004742\n",
      "195\n",
      "Train Epoch: 2 [24576/70488 (35%)]\tLoss: 1.003353\n",
      "196\n",
      "Train Epoch: 2 [25088/70488 (36%)]\tLoss: 0.997794\n",
      "197\n",
      "Train Epoch: 2 [25600/70488 (36%)]\tLoss: 1.003138\n",
      "198\n",
      "Train Epoch: 2 [26112/70488 (37%)]\tLoss: 0.999924\n",
      "199\n",
      "Train Epoch: 2 [26624/70488 (38%)]\tLoss: 1.004259\n",
      "200\n",
      "Train Epoch: 2 [27136/70488 (38%)]\tLoss: 1.002223\n",
      "201\n",
      "Train Epoch: 2 [27648/70488 (39%)]\tLoss: 1.000960\n",
      "202\n",
      "Train Epoch: 2 [28160/70488 (40%)]\tLoss: 0.997794\n",
      "203\n",
      "Train Epoch: 2 [28672/70488 (41%)]\tLoss: 1.000918\n",
      "204\n",
      "Train Epoch: 2 [29184/70488 (41%)]\tLoss: 1.001729\n",
      "205\n",
      "Train Epoch: 2 [29696/70488 (42%)]\tLoss: 0.999125\n",
      "206\n",
      "Train Epoch: 2 [30208/70488 (43%)]\tLoss: 0.998855\n",
      "207\n",
      "Train Epoch: 2 [30720/70488 (43%)]\tLoss: 0.997747\n",
      "208\n",
      "Train Epoch: 2 [31232/70488 (44%)]\tLoss: 0.999172\n",
      "209\n",
      "Train Epoch: 2 [31744/70488 (45%)]\tLoss: 0.997397\n",
      "210\n",
      "Train Epoch: 2 [32256/70488 (46%)]\tLoss: 0.999016\n",
      "211\n",
      "Train Epoch: 2 [32768/70488 (46%)]\tLoss: 1.001889\n",
      "212\n",
      "Train Epoch: 2 [33280/70488 (47%)]\tLoss: 0.998880\n",
      "213\n",
      "Train Epoch: 2 [33792/70488 (48%)]\tLoss: 0.994428\n",
      "214\n",
      "Train Epoch: 2 [34304/70488 (49%)]\tLoss: 0.995262\n",
      "215\n",
      "Train Epoch: 2 [34816/70488 (49%)]\tLoss: 0.995288\n",
      "216\n",
      "Train Epoch: 2 [35328/70488 (50%)]\tLoss: 0.994746\n",
      "217\n",
      "Train Epoch: 2 [35840/70488 (51%)]\tLoss: 0.996675\n",
      "218\n",
      "Train Epoch: 2 [36352/70488 (51%)]\tLoss: 1.001022\n",
      "219\n",
      "Train Epoch: 2 [36864/70488 (52%)]\tLoss: 0.993970\n",
      "220\n",
      "Train Epoch: 2 [37376/70488 (53%)]\tLoss: 0.994574\n",
      "221\n",
      "Train Epoch: 2 [37888/70488 (54%)]\tLoss: 0.990900\n",
      "222\n",
      "Train Epoch: 2 [38400/70488 (54%)]\tLoss: 0.996351\n",
      "223\n",
      "Train Epoch: 2 [38912/70488 (55%)]\tLoss: 0.994834\n",
      "224\n",
      "Train Epoch: 2 [39424/70488 (56%)]\tLoss: 0.994210\n",
      "225\n",
      "Train Epoch: 2 [39936/70488 (57%)]\tLoss: 0.992967\n",
      "226\n",
      "Train Epoch: 2 [40448/70488 (57%)]\tLoss: 0.993551\n",
      "227\n",
      "Train Epoch: 2 [40960/70488 (58%)]\tLoss: 0.990380\n",
      "228\n",
      "Train Epoch: 2 [41472/70488 (59%)]\tLoss: 0.992247\n",
      "229\n",
      "Train Epoch: 2 [41984/70488 (59%)]\tLoss: 0.987730\n",
      "230\n",
      "Train Epoch: 2 [42496/70488 (60%)]\tLoss: 0.996243\n",
      "231\n",
      "Train Epoch: 2 [43008/70488 (61%)]\tLoss: 0.991981\n",
      "232\n",
      "Train Epoch: 2 [43520/70488 (62%)]\tLoss: 0.988815\n",
      "233\n",
      "Train Epoch: 2 [44032/70488 (62%)]\tLoss: 0.993652\n",
      "234\n",
      "Train Epoch: 2 [44544/70488 (63%)]\tLoss: 0.989906\n",
      "235\n",
      "Train Epoch: 2 [45056/70488 (64%)]\tLoss: 0.987330\n",
      "236\n",
      "Train Epoch: 2 [45568/70488 (64%)]\tLoss: 0.987098\n",
      "237\n",
      "Train Epoch: 2 [46080/70488 (65%)]\tLoss: 0.986776\n",
      "238\n",
      "Train Epoch: 2 [46592/70488 (66%)]\tLoss: 0.990771\n",
      "239\n",
      "Train Epoch: 2 [47104/70488 (67%)]\tLoss: 0.991907\n",
      "240\n",
      "Train Epoch: 2 [47616/70488 (67%)]\tLoss: 0.989399\n",
      "241\n",
      "Train Epoch: 2 [48128/70488 (68%)]\tLoss: 0.993594\n",
      "242\n",
      "Train Epoch: 2 [48640/70488 (69%)]\tLoss: 0.990196\n",
      "243\n",
      "Train Epoch: 2 [49152/70488 (70%)]\tLoss: 0.987110\n",
      "244\n",
      "Train Epoch: 2 [49664/70488 (70%)]\tLoss: 0.992269\n",
      "245\n",
      "Train Epoch: 2 [50176/70488 (71%)]\tLoss: 0.987762\n",
      "246\n",
      "Train Epoch: 2 [50688/70488 (72%)]\tLoss: 0.988377\n",
      "247\n",
      "Train Epoch: 2 [51200/70488 (72%)]\tLoss: 0.985764\n",
      "248\n",
      "Train Epoch: 2 [51712/70488 (73%)]\tLoss: 0.986579\n",
      "249\n",
      "Train Epoch: 2 [52224/70488 (74%)]\tLoss: 0.987173\n",
      "250\n",
      "Train Epoch: 2 [52736/70488 (75%)]\tLoss: 0.988221\n",
      "251\n",
      "Train Epoch: 2 [53248/70488 (75%)]\tLoss: 0.993973\n",
      "252\n",
      "Train Epoch: 2 [53760/70488 (76%)]\tLoss: 0.989944\n",
      "253\n",
      "Train Epoch: 2 [54272/70488 (77%)]\tLoss: 0.988470\n",
      "254\n",
      "Train Epoch: 2 [54784/70488 (78%)]\tLoss: 0.990400\n",
      "255\n",
      "Train Epoch: 2 [55296/70488 (78%)]\tLoss: 0.984340\n",
      "256\n",
      "Train Epoch: 2 [55808/70488 (79%)]\tLoss: 0.984859\n",
      "257\n",
      "Train Epoch: 2 [56320/70488 (80%)]\tLoss: 0.994503\n",
      "258\n",
      "Train Epoch: 2 [56832/70488 (80%)]\tLoss: 0.982728\n",
      "259\n",
      "Train Epoch: 2 [57344/70488 (81%)]\tLoss: 0.982097\n",
      "260\n",
      "Train Epoch: 2 [57856/70488 (82%)]\tLoss: 0.987937\n",
      "261\n",
      "Train Epoch: 2 [58368/70488 (83%)]\tLoss: 0.989393\n",
      "262\n",
      "Train Epoch: 2 [58880/70488 (83%)]\tLoss: 0.980744\n",
      "263\n",
      "Train Epoch: 2 [59392/70488 (84%)]\tLoss: 0.983841\n",
      "264\n",
      "Train Epoch: 2 [59904/70488 (85%)]\tLoss: 0.983703\n",
      "265\n",
      "Train Epoch: 2 [60416/70488 (86%)]\tLoss: 0.985450\n",
      "266\n",
      "Train Epoch: 2 [60928/70488 (86%)]\tLoss: 0.985122\n",
      "267\n",
      "Train Epoch: 2 [61440/70488 (87%)]\tLoss: 0.981170\n",
      "268\n",
      "Train Epoch: 2 [61952/70488 (88%)]\tLoss: 0.983966\n",
      "269\n",
      "Train Epoch: 2 [62464/70488 (88%)]\tLoss: 0.985392\n",
      "270\n",
      "Train Epoch: 2 [62976/70488 (89%)]\tLoss: 0.981206\n",
      "271\n",
      "Train Epoch: 2 [63488/70488 (90%)]\tLoss: 0.988087\n",
      "272\n",
      "Train Epoch: 2 [64000/70488 (91%)]\tLoss: 0.982074\n",
      "273\n",
      "Train Epoch: 2 [64512/70488 (91%)]\tLoss: 0.982619\n",
      "274\n",
      "Train Epoch: 2 [65024/70488 (92%)]\tLoss: 0.982935\n",
      "275\n",
      "Train Epoch: 2 [65536/70488 (93%)]\tLoss: 0.981628\n",
      "276\n",
      "Train Epoch: 2 [66048/70488 (93%)]\tLoss: 0.982985\n",
      "277\n",
      "Train Epoch: 2 [66560/70488 (94%)]\tLoss: 0.988088\n",
      "278\n",
      "Train Epoch: 2 [67072/70488 (95%)]\tLoss: 0.984172\n",
      "279\n",
      "Train Epoch: 2 [67584/70488 (96%)]\tLoss: 0.980007\n",
      "280\n",
      "Train Epoch: 2 [68096/70488 (96%)]\tLoss: 0.980673\n",
      "281\n",
      "Train Epoch: 2 [68608/70488 (97%)]\tLoss: 0.984603\n",
      "282\n",
      "Train Epoch: 2 [69120/70488 (98%)]\tLoss: 0.985662\n",
      "283\n",
      "Train Epoch: 2 [69632/70488 (99%)]\tLoss: 0.983135\n",
      "284\n",
      "Train Epoch: 2 [47128/70488 (99%)]\tLoss: 0.981353\n",
      "285\n",
      "====> Epoch: 2 Average loss: 0.00391025\n",
      "====> Test set loss: 30.52481627\n",
      "Train Epoch: 3 [0/70488 (0%)]\tLoss: 0.977472\n",
      "296\n",
      "Train Epoch: 3 [512/70488 (1%)]\tLoss: 0.977768\n",
      "297\n",
      "Train Epoch: 3 [1024/70488 (1%)]\tLoss: 0.978941\n",
      "298\n",
      "Train Epoch: 3 [1536/70488 (2%)]\tLoss: 0.978717\n",
      "299\n",
      "Train Epoch: 3 [2048/70488 (3%)]\tLoss: 0.977036\n",
      "300\n",
      "Train Epoch: 3 [2560/70488 (4%)]\tLoss: 0.981365\n",
      "301\n",
      "Train Epoch: 3 [3072/70488 (4%)]\tLoss: 0.978912\n",
      "302\n",
      "Train Epoch: 3 [3584/70488 (5%)]\tLoss: 0.978910\n",
      "303\n",
      "Train Epoch: 3 [4096/70488 (6%)]\tLoss: 0.979124\n",
      "304\n",
      "Train Epoch: 3 [4608/70488 (7%)]\tLoss: 0.979589\n",
      "305\n",
      "Train Epoch: 3 [5120/70488 (7%)]\tLoss: 0.977929\n",
      "306\n",
      "Train Epoch: 3 [5632/70488 (8%)]\tLoss: 0.974796\n",
      "307\n",
      "Train Epoch: 3 [6144/70488 (9%)]\tLoss: 0.977700\n",
      "308\n",
      "Train Epoch: 3 [6656/70488 (9%)]\tLoss: 0.981652\n",
      "309\n",
      "Train Epoch: 3 [7168/70488 (10%)]\tLoss: 0.979737\n",
      "310\n",
      "Train Epoch: 3 [7680/70488 (11%)]\tLoss: 0.978660\n",
      "311\n",
      "Train Epoch: 3 [8192/70488 (12%)]\tLoss: 0.974392\n",
      "312\n",
      "Train Epoch: 3 [8704/70488 (12%)]\tLoss: 0.977293\n",
      "313\n",
      "Train Epoch: 3 [9216/70488 (13%)]\tLoss: 0.975989\n",
      "314\n",
      "Train Epoch: 3 [9728/70488 (14%)]\tLoss: 0.975048\n",
      "315\n",
      "Train Epoch: 3 [10240/70488 (14%)]\tLoss: 0.975014\n",
      "316\n",
      "Train Epoch: 3 [10752/70488 (15%)]\tLoss: 0.975026\n",
      "317\n",
      "Train Epoch: 3 [11264/70488 (16%)]\tLoss: 0.982926\n",
      "318\n",
      "Train Epoch: 3 [11776/70488 (17%)]\tLoss: 0.977899\n",
      "319\n",
      "Train Epoch: 3 [12288/70488 (17%)]\tLoss: 0.974258\n",
      "320\n",
      "Train Epoch: 3 [12800/70488 (18%)]\tLoss: 0.979616\n",
      "321\n",
      "Train Epoch: 3 [13312/70488 (19%)]\tLoss: 0.980755\n",
      "322\n",
      "Train Epoch: 3 [13824/70488 (20%)]\tLoss: 0.976382\n",
      "323\n",
      "Train Epoch: 3 [14336/70488 (20%)]\tLoss: 0.978929\n",
      "324\n",
      "Train Epoch: 3 [14848/70488 (21%)]\tLoss: 0.976122\n",
      "325\n",
      "Train Epoch: 3 [15360/70488 (22%)]\tLoss: 0.975911\n",
      "326\n",
      "Train Epoch: 3 [15872/70488 (22%)]\tLoss: 0.974595\n",
      "327\n",
      "Train Epoch: 3 [16384/70488 (23%)]\tLoss: 0.975719\n",
      "328\n",
      "Train Epoch: 3 [16896/70488 (24%)]\tLoss: 0.977307\n",
      "329\n",
      "Train Epoch: 3 [17408/70488 (25%)]\tLoss: 0.976263\n",
      "330\n",
      "Train Epoch: 3 [17920/70488 (25%)]\tLoss: 0.976793\n",
      "331\n",
      "Train Epoch: 3 [18432/70488 (26%)]\tLoss: 0.974595\n",
      "332\n",
      "Train Epoch: 3 [18944/70488 (27%)]\tLoss: 0.975735\n",
      "333\n",
      "Train Epoch: 3 [19456/70488 (28%)]\tLoss: 0.982519\n",
      "334\n",
      "Train Epoch: 3 [19968/70488 (28%)]\tLoss: 0.977881\n",
      "335\n",
      "Train Epoch: 3 [20480/70488 (29%)]\tLoss: 0.974795\n",
      "336\n",
      "Train Epoch: 3 [20992/70488 (30%)]\tLoss: 0.975399\n",
      "337\n",
      "Train Epoch: 3 [21504/70488 (30%)]\tLoss: 0.974528\n",
      "338\n",
      "Train Epoch: 3 [22016/70488 (31%)]\tLoss: 0.978679\n",
      "339\n",
      "Train Epoch: 3 [22528/70488 (32%)]\tLoss: 0.973477\n",
      "340\n",
      "Train Epoch: 3 [23040/70488 (33%)]\tLoss: 0.971136\n",
      "341\n",
      "Train Epoch: 3 [23552/70488 (33%)]\tLoss: 0.972245\n",
      "342\n",
      "Train Epoch: 3 [24064/70488 (34%)]\tLoss: 0.970805\n",
      "343\n",
      "Train Epoch: 3 [24576/70488 (35%)]\tLoss: 0.972951\n",
      "344\n",
      "Train Epoch: 3 [25088/70488 (36%)]\tLoss: 0.974835\n",
      "345\n",
      "Train Epoch: 3 [25600/70488 (36%)]\tLoss: 0.971362\n",
      "346\n",
      "Train Epoch: 3 [26112/70488 (37%)]\tLoss: 0.975855\n",
      "347\n",
      "Train Epoch: 3 [26624/70488 (38%)]\tLoss: 0.972589\n",
      "348\n",
      "Train Epoch: 3 [27136/70488 (38%)]\tLoss: 0.972619\n",
      "349\n",
      "Train Epoch: 3 [27648/70488 (39%)]\tLoss: 0.972494\n",
      "350\n",
      "Train Epoch: 3 [28160/70488 (40%)]\tLoss: 0.972319\n",
      "351\n",
      "Train Epoch: 3 [28672/70488 (41%)]\tLoss: 0.973155\n",
      "352\n",
      "Train Epoch: 3 [29184/70488 (41%)]\tLoss: 0.972823\n",
      "353\n",
      "Train Epoch: 3 [29696/70488 (42%)]\tLoss: 0.969348\n",
      "354\n",
      "Train Epoch: 3 [30208/70488 (43%)]\tLoss: 0.972251\n",
      "355\n",
      "Train Epoch: 3 [30720/70488 (43%)]\tLoss: 0.972734\n",
      "356\n",
      "Train Epoch: 3 [31232/70488 (44%)]\tLoss: 0.970880\n",
      "357\n",
      "Train Epoch: 3 [31744/70488 (45%)]\tLoss: 0.972774\n",
      "358\n",
      "Train Epoch: 3 [32256/70488 (46%)]\tLoss: 0.971936\n",
      "359\n",
      "Train Epoch: 3 [32768/70488 (46%)]\tLoss: 0.973905\n",
      "360\n",
      "Train Epoch: 3 [33280/70488 (47%)]\tLoss: 0.970009\n",
      "361\n",
      "Train Epoch: 3 [33792/70488 (48%)]\tLoss: 0.973141\n",
      "362\n",
      "Train Epoch: 3 [34304/70488 (49%)]\tLoss: 0.974941\n",
      "363\n",
      "Train Epoch: 3 [34816/70488 (49%)]\tLoss: 0.972509\n",
      "364\n",
      "Train Epoch: 3 [35328/70488 (50%)]\tLoss: 0.971321\n",
      "365\n",
      "Train Epoch: 3 [35840/70488 (51%)]\tLoss: 0.969305\n",
      "366\n",
      "Train Epoch: 3 [36352/70488 (51%)]\tLoss: 0.972390\n",
      "367\n",
      "Train Epoch: 3 [36864/70488 (52%)]\tLoss: 0.968305\n",
      "368\n",
      "Train Epoch: 3 [37376/70488 (53%)]\tLoss: 0.968316\n",
      "369\n",
      "Train Epoch: 3 [37888/70488 (54%)]\tLoss: 0.970287\n",
      "370\n",
      "Train Epoch: 3 [38400/70488 (54%)]\tLoss: 0.969822\n",
      "371\n",
      "Train Epoch: 3 [38912/70488 (55%)]\tLoss: 0.970720\n",
      "372\n",
      "Train Epoch: 3 [39424/70488 (56%)]\tLoss: 0.968420\n",
      "373\n",
      "Train Epoch: 3 [39936/70488 (57%)]\tLoss: 0.973304\n",
      "374\n",
      "Train Epoch: 3 [40448/70488 (57%)]\tLoss: 0.969513\n",
      "375\n",
      "Train Epoch: 3 [40960/70488 (58%)]\tLoss: 0.970599\n",
      "376\n",
      "Train Epoch: 3 [41472/70488 (59%)]\tLoss: 0.971346\n",
      "377\n",
      "Train Epoch: 3 [41984/70488 (59%)]\tLoss: 0.968598\n",
      "378\n",
      "Train Epoch: 3 [42496/70488 (60%)]\tLoss: 0.970523\n",
      "379\n",
      "Train Epoch: 3 [43008/70488 (61%)]\tLoss: 0.972155\n",
      "380\n",
      "Train Epoch: 3 [43520/70488 (62%)]\tLoss: 0.971753\n",
      "381\n",
      "Train Epoch: 3 [44032/70488 (62%)]\tLoss: 0.971080\n",
      "382\n",
      "Train Epoch: 3 [44544/70488 (63%)]\tLoss: 0.977624\n",
      "383\n",
      "Train Epoch: 3 [45056/70488 (64%)]\tLoss: 0.969823\n",
      "384\n",
      "Train Epoch: 3 [45568/70488 (64%)]\tLoss: 0.969901\n",
      "385\n",
      "Train Epoch: 3 [46080/70488 (65%)]\tLoss: 0.973036\n",
      "386\n",
      "Train Epoch: 3 [46592/70488 (66%)]\tLoss: 0.970032\n",
      "387\n",
      "Train Epoch: 3 [47104/70488 (67%)]\tLoss: 0.971353\n",
      "388\n",
      "Train Epoch: 3 [47616/70488 (67%)]\tLoss: 0.967693\n",
      "389\n",
      "Train Epoch: 3 [48128/70488 (68%)]\tLoss: 0.969694\n",
      "390\n",
      "Train Epoch: 3 [48640/70488 (69%)]\tLoss: 0.967804\n",
      "391\n",
      "Train Epoch: 3 [49152/70488 (70%)]\tLoss: 0.968805\n",
      "392\n",
      "Train Epoch: 3 [49664/70488 (70%)]\tLoss: 0.966364\n",
      "393\n",
      "Train Epoch: 3 [50176/70488 (71%)]\tLoss: 0.966802\n",
      "394\n",
      "Train Epoch: 3 [50688/70488 (72%)]\tLoss: 0.969074\n",
      "395\n",
      "Train Epoch: 3 [51200/70488 (72%)]\tLoss: 0.966099\n",
      "396\n",
      "Train Epoch: 3 [51712/70488 (73%)]\tLoss: 0.964746\n",
      "397\n",
      "Train Epoch: 3 [52224/70488 (74%)]\tLoss: 0.967897\n",
      "398\n",
      "Train Epoch: 3 [52736/70488 (75%)]\tLoss: 0.965984\n",
      "399\n",
      "Train Epoch: 3 [53248/70488 (75%)]\tLoss: 0.969258\n",
      "400\n",
      "Train Epoch: 3 [53760/70488 (76%)]\tLoss: 0.965946\n",
      "401\n",
      "Train Epoch: 3 [54272/70488 (77%)]\tLoss: 0.974182\n",
      "402\n",
      "Train Epoch: 3 [54784/70488 (78%)]\tLoss: 0.970315\n",
      "403\n",
      "Train Epoch: 3 [55296/70488 (78%)]\tLoss: 0.969051\n",
      "404\n",
      "Train Epoch: 3 [55808/70488 (79%)]\tLoss: 0.971151\n",
      "405\n",
      "Train Epoch: 3 [56320/70488 (80%)]\tLoss: 0.965157\n",
      "406\n",
      "Train Epoch: 3 [56832/70488 (80%)]\tLoss: 0.963543\n",
      "407\n",
      "Train Epoch: 3 [57344/70488 (81%)]\tLoss: 0.970012\n",
      "408\n",
      "Train Epoch: 3 [57856/70488 (82%)]\tLoss: 0.967901\n",
      "409\n",
      "Train Epoch: 3 [58368/70488 (83%)]\tLoss: 0.965886\n",
      "410\n",
      "Train Epoch: 3 [58880/70488 (83%)]\tLoss: 0.967913\n",
      "411\n",
      "Train Epoch: 3 [59392/70488 (84%)]\tLoss: 0.965079\n",
      "412\n",
      "Train Epoch: 3 [59904/70488 (85%)]\tLoss: 0.962731\n",
      "413\n",
      "Train Epoch: 3 [60416/70488 (86%)]\tLoss: 0.965207\n",
      "414\n",
      "Train Epoch: 3 [60928/70488 (86%)]\tLoss: 0.963329\n",
      "415\n",
      "Train Epoch: 3 [61440/70488 (87%)]\tLoss: 0.968761\n",
      "416\n",
      "Train Epoch: 3 [61952/70488 (88%)]\tLoss: 0.963358\n",
      "417\n",
      "Train Epoch: 3 [62464/70488 (88%)]\tLoss: 0.971909\n",
      "418\n",
      "Train Epoch: 3 [62976/70488 (89%)]\tLoss: 0.965661\n",
      "419\n",
      "Train Epoch: 3 [63488/70488 (90%)]\tLoss: 0.966556\n",
      "420\n",
      "Train Epoch: 3 [64000/70488 (91%)]\tLoss: 0.965813\n",
      "421\n",
      "Train Epoch: 3 [64512/70488 (91%)]\tLoss: 0.963768\n",
      "422\n",
      "Train Epoch: 3 [65024/70488 (92%)]\tLoss: 0.965723\n",
      "423\n",
      "Train Epoch: 3 [65536/70488 (93%)]\tLoss: 0.967802\n",
      "424\n",
      "Train Epoch: 3 [66048/70488 (93%)]\tLoss: 0.966347\n",
      "425\n",
      "Train Epoch: 3 [66560/70488 (94%)]\tLoss: 0.966699\n",
      "426\n",
      "Train Epoch: 3 [67072/70488 (95%)]\tLoss: 0.965634\n",
      "427\n",
      "Train Epoch: 3 [67584/70488 (96%)]\tLoss: 0.966391\n",
      "428\n",
      "Train Epoch: 3 [68096/70488 (96%)]\tLoss: 0.969814\n",
      "429\n",
      "Train Epoch: 3 [68608/70488 (97%)]\tLoss: 0.967409\n",
      "430\n",
      "Train Epoch: 3 [69120/70488 (98%)]\tLoss: 0.964849\n",
      "431\n",
      "Train Epoch: 3 [69632/70488 (99%)]\tLoss: 0.965383\n",
      "432\n",
      "Train Epoch: 3 [47128/70488 (99%)]\tLoss: 0.967014\n",
      "433\n",
      "====> Epoch: 3 Average loss: 0.00380598\n",
      "====> Test set loss: 30.40077734\n",
      "Train Epoch: 4 [0/70488 (0%)]\tLoss: 0.965147\n",
      "444\n",
      "Train Epoch: 4 [512/70488 (1%)]\tLoss: 0.966413\n",
      "445\n",
      "Train Epoch: 4 [1024/70488 (1%)]\tLoss: 0.962548\n",
      "446\n",
      "Train Epoch: 4 [1536/70488 (2%)]\tLoss: 0.961226\n",
      "447\n",
      "Train Epoch: 4 [2048/70488 (3%)]\tLoss: 0.964663\n",
      "448\n",
      "Train Epoch: 4 [2560/70488 (4%)]\tLoss: 0.961373\n",
      "449\n",
      "Train Epoch: 4 [3072/70488 (4%)]\tLoss: 0.962768\n",
      "450\n",
      "Train Epoch: 4 [3584/70488 (5%)]\tLoss: 0.957568\n",
      "451\n",
      "Train Epoch: 4 [4096/70488 (6%)]\tLoss: 0.962740\n",
      "452\n",
      "Train Epoch: 4 [4608/70488 (7%)]\tLoss: 0.966987\n",
      "453\n",
      "Train Epoch: 4 [5120/70488 (7%)]\tLoss: 0.959894\n",
      "454\n",
      "Train Epoch: 4 [5632/70488 (8%)]\tLoss: 0.963480\n",
      "455\n",
      "Train Epoch: 4 [6144/70488 (9%)]\tLoss: 0.958666\n",
      "456\n",
      "Train Epoch: 4 [6656/70488 (9%)]\tLoss: 0.967611\n",
      "457\n",
      "Train Epoch: 4 [7168/70488 (10%)]\tLoss: 0.962230\n",
      "458\n",
      "Train Epoch: 4 [7680/70488 (11%)]\tLoss: 0.960416\n",
      "459\n",
      "Train Epoch: 4 [8192/70488 (12%)]\tLoss: 0.962690\n",
      "460\n",
      "Train Epoch: 4 [8704/70488 (12%)]\tLoss: 0.958614\n",
      "461\n",
      "Train Epoch: 4 [9216/70488 (13%)]\tLoss: 0.961530\n",
      "462\n",
      "Train Epoch: 4 [9728/70488 (14%)]\tLoss: 0.962971\n",
      "463\n",
      "Train Epoch: 4 [10240/70488 (14%)]\tLoss: 0.963954\n",
      "464\n",
      "Train Epoch: 4 [10752/70488 (15%)]\tLoss: 0.964178\n",
      "465\n",
      "Train Epoch: 4 [11264/70488 (16%)]\tLoss: 0.962524\n",
      "466\n",
      "Train Epoch: 4 [11776/70488 (17%)]\tLoss: 0.963495\n",
      "467\n",
      "Train Epoch: 4 [12288/70488 (17%)]\tLoss: 0.960874\n",
      "468\n",
      "Train Epoch: 4 [12800/70488 (18%)]\tLoss: 0.958090\n",
      "469\n",
      "Train Epoch: 4 [13312/70488 (19%)]\tLoss: 0.962594\n",
      "470\n",
      "Train Epoch: 4 [13824/70488 (20%)]\tLoss: 0.962629\n",
      "471\n",
      "Train Epoch: 4 [14336/70488 (20%)]\tLoss: 0.961981\n",
      "472\n",
      "Train Epoch: 4 [14848/70488 (21%)]\tLoss: 0.960380\n",
      "473\n",
      "Train Epoch: 4 [15360/70488 (22%)]\tLoss: 0.961036\n",
      "474\n",
      "Train Epoch: 4 [15872/70488 (22%)]\tLoss: 0.961837\n",
      "475\n",
      "Train Epoch: 4 [16384/70488 (23%)]\tLoss: 0.962084\n",
      "476\n",
      "Train Epoch: 4 [16896/70488 (24%)]\tLoss: 0.960845\n",
      "477\n",
      "Train Epoch: 4 [17408/70488 (25%)]\tLoss: 0.963096\n",
      "478\n",
      "Train Epoch: 4 [17920/70488 (25%)]\tLoss: 0.961922\n",
      "479\n",
      "Train Epoch: 4 [18432/70488 (26%)]\tLoss: 0.959696\n",
      "480\n",
      "Train Epoch: 4 [18944/70488 (27%)]\tLoss: 0.958569\n",
      "481\n",
      "Train Epoch: 4 [19456/70488 (28%)]\tLoss: 0.960421\n",
      "482\n",
      "Train Epoch: 4 [19968/70488 (28%)]\tLoss: 0.959117\n",
      "483\n",
      "Train Epoch: 4 [20480/70488 (29%)]\tLoss: 0.958787\n",
      "484\n",
      "Train Epoch: 4 [20992/70488 (30%)]\tLoss: 0.960399\n",
      "485\n",
      "Train Epoch: 4 [21504/70488 (30%)]\tLoss: 0.959110\n",
      "486\n",
      "Train Epoch: 4 [22016/70488 (31%)]\tLoss: 0.961676\n",
      "487\n",
      "Train Epoch: 4 [22528/70488 (32%)]\tLoss: 0.963019\n",
      "488\n",
      "Train Epoch: 4 [23040/70488 (33%)]\tLoss: 0.959980\n",
      "489\n",
      "Train Epoch: 4 [23552/70488 (33%)]\tLoss: 0.959913\n",
      "490\n",
      "Train Epoch: 4 [24064/70488 (34%)]\tLoss: 0.962598\n",
      "491\n",
      "Train Epoch: 4 [24576/70488 (35%)]\tLoss: 0.959302\n",
      "492\n",
      "Train Epoch: 4 [25088/70488 (36%)]\tLoss: 0.962182\n",
      "493\n",
      "Train Epoch: 4 [25600/70488 (36%)]\tLoss: 0.959084\n",
      "494\n",
      "Train Epoch: 4 [26112/70488 (37%)]\tLoss: 0.960315\n",
      "495\n",
      "Train Epoch: 4 [26624/70488 (38%)]\tLoss: 0.959794\n",
      "496\n",
      "Train Epoch: 4 [27136/70488 (38%)]\tLoss: 0.957468\n",
      "497\n",
      "Train Epoch: 4 [27648/70488 (39%)]\tLoss: 0.959207\n",
      "498\n",
      "Train Epoch: 4 [28160/70488 (40%)]\tLoss: 0.960514\n",
      "499\n",
      "Train Epoch: 4 [28672/70488 (41%)]\tLoss: 0.963074\n",
      "500\n",
      "Train Epoch: 4 [29184/70488 (41%)]\tLoss: 0.958818\n",
      "501\n",
      "Train Epoch: 4 [29696/70488 (42%)]\tLoss: 0.960213\n",
      "502\n",
      "Train Epoch: 4 [30208/70488 (43%)]\tLoss: 0.959770\n",
      "503\n",
      "Train Epoch: 4 [30720/70488 (43%)]\tLoss: 0.957968\n",
      "504\n",
      "Train Epoch: 4 [31232/70488 (44%)]\tLoss: 0.961697\n",
      "505\n",
      "Train Epoch: 4 [31744/70488 (45%)]\tLoss: 0.956948\n",
      "506\n",
      "Train Epoch: 4 [32256/70488 (46%)]\tLoss: 0.961239\n",
      "507\n",
      "Train Epoch: 4 [32768/70488 (46%)]\tLoss: 0.956100\n",
      "508\n",
      "Train Epoch: 4 [33280/70488 (47%)]\tLoss: 0.956040\n",
      "509\n",
      "Train Epoch: 4 [33792/70488 (48%)]\tLoss: 0.962763\n",
      "510\n",
      "Train Epoch: 4 [34304/70488 (49%)]\tLoss: 0.957758\n",
      "511\n",
      "Train Epoch: 4 [34816/70488 (49%)]\tLoss: 0.955732\n",
      "512\n",
      "Train Epoch: 4 [35328/70488 (50%)]\tLoss: 0.957902\n",
      "513\n",
      "Train Epoch: 4 [35840/70488 (51%)]\tLoss: 0.957735\n",
      "514\n",
      "Train Epoch: 4 [36352/70488 (51%)]\tLoss: 0.960240\n",
      "515\n",
      "Train Epoch: 4 [36864/70488 (52%)]\tLoss: 0.955632\n",
      "516\n",
      "Train Epoch: 4 [37376/70488 (53%)]\tLoss: 0.956363\n",
      "517\n",
      "Train Epoch: 4 [37888/70488 (54%)]\tLoss: 0.962433\n",
      "518\n",
      "Train Epoch: 4 [38400/70488 (54%)]\tLoss: 0.957034\n",
      "519\n",
      "Train Epoch: 4 [38912/70488 (55%)]\tLoss: 0.959670\n",
      "520\n",
      "Train Epoch: 4 [39424/70488 (56%)]\tLoss: 0.960652\n",
      "521\n",
      "Train Epoch: 4 [39936/70488 (57%)]\tLoss: 0.962429\n",
      "522\n",
      "Train Epoch: 4 [40448/70488 (57%)]\tLoss: 0.956365\n",
      "523\n",
      "Train Epoch: 4 [40960/70488 (58%)]\tLoss: 0.959345\n",
      "524\n",
      "Train Epoch: 4 [41472/70488 (59%)]\tLoss: 0.957158\n",
      "525\n",
      "Train Epoch: 4 [41984/70488 (59%)]\tLoss: 0.958005\n",
      "526\n",
      "Train Epoch: 4 [42496/70488 (60%)]\tLoss: 0.955210\n",
      "527\n",
      "Train Epoch: 4 [43008/70488 (61%)]\tLoss: 0.957478\n",
      "528\n",
      "Train Epoch: 4 [43520/70488 (62%)]\tLoss: 0.956566\n",
      "529\n",
      "Train Epoch: 4 [44032/70488 (62%)]\tLoss: 0.958130\n",
      "530\n",
      "Train Epoch: 4 [44544/70488 (63%)]\tLoss: 0.962152\n",
      "531\n",
      "Train Epoch: 4 [45056/70488 (64%)]\tLoss: 0.959856\n",
      "532\n",
      "Train Epoch: 4 [45568/70488 (64%)]\tLoss: 0.959327\n",
      "533\n",
      "Train Epoch: 4 [46080/70488 (65%)]\tLoss: 0.963278\n",
      "534\n",
      "Train Epoch: 4 [46592/70488 (66%)]\tLoss: 0.958465\n",
      "535\n",
      "Train Epoch: 4 [47104/70488 (67%)]\tLoss: 0.957298\n",
      "536\n",
      "Train Epoch: 4 [47616/70488 (67%)]\tLoss: 0.959091\n",
      "537\n",
      "Train Epoch: 4 [48128/70488 (68%)]\tLoss: 0.959894\n",
      "538\n",
      "Train Epoch: 4 [48640/70488 (69%)]\tLoss: 0.959460\n",
      "539\n",
      "Train Epoch: 4 [49152/70488 (70%)]\tLoss: 0.956433\n",
      "540\n",
      "Train Epoch: 4 [49664/70488 (70%)]\tLoss: 0.959808\n",
      "541\n",
      "Train Epoch: 4 [50176/70488 (71%)]\tLoss: 0.959664\n",
      "542\n",
      "Train Epoch: 4 [50688/70488 (72%)]\tLoss: 0.958445\n",
      "543\n",
      "Train Epoch: 4 [51200/70488 (72%)]\tLoss: 0.958093\n",
      "544\n",
      "Train Epoch: 4 [51712/70488 (73%)]\tLoss: 0.959303\n",
      "545\n",
      "Train Epoch: 4 [52224/70488 (74%)]\tLoss: 0.957193\n",
      "546\n",
      "Train Epoch: 4 [52736/70488 (75%)]\tLoss: 0.957976\n",
      "547\n",
      "Train Epoch: 4 [53248/70488 (75%)]\tLoss: 0.955705\n",
      "548\n",
      "Train Epoch: 4 [53760/70488 (76%)]\tLoss: 0.958474\n",
      "549\n",
      "Train Epoch: 4 [54272/70488 (77%)]\tLoss: 0.956250\n",
      "550\n",
      "Train Epoch: 4 [54784/70488 (78%)]\tLoss: 0.959436\n",
      "551\n",
      "Train Epoch: 4 [55296/70488 (78%)]\tLoss: 0.963475\n",
      "552\n",
      "Train Epoch: 4 [55808/70488 (79%)]\tLoss: 0.958962\n",
      "553\n",
      "Train Epoch: 4 [56320/70488 (80%)]\tLoss: 0.961214\n",
      "554\n",
      "Train Epoch: 4 [56832/70488 (80%)]\tLoss: 0.956657\n",
      "555\n",
      "Train Epoch: 4 [57344/70488 (81%)]\tLoss: 0.958486\n",
      "556\n",
      "Train Epoch: 4 [57856/70488 (82%)]\tLoss: 0.957141\n",
      "557\n",
      "Train Epoch: 4 [58368/70488 (83%)]\tLoss: 0.958158\n",
      "558\n",
      "Train Epoch: 4 [58880/70488 (83%)]\tLoss: 0.957432\n",
      "559\n",
      "Train Epoch: 4 [59392/70488 (84%)]\tLoss: 0.959353\n",
      "560\n",
      "Train Epoch: 4 [59904/70488 (85%)]\tLoss: 0.953498\n",
      "561\n",
      "Train Epoch: 4 [60416/70488 (86%)]\tLoss: 0.959485\n",
      "562\n",
      "Train Epoch: 4 [60928/70488 (86%)]\tLoss: 0.953992\n",
      "563\n",
      "Train Epoch: 4 [61440/70488 (87%)]\tLoss: 0.959364\n",
      "564\n",
      "Train Epoch: 4 [61952/70488 (88%)]\tLoss: 0.954571\n",
      "565\n",
      "Train Epoch: 4 [62464/70488 (88%)]\tLoss: 0.955232\n",
      "566\n",
      "Train Epoch: 4 [62976/70488 (89%)]\tLoss: 0.960497\n",
      "567\n",
      "Train Epoch: 4 [63488/70488 (90%)]\tLoss: 0.955046\n",
      "568\n",
      "Train Epoch: 4 [64000/70488 (91%)]\tLoss: 0.957297\n",
      "569\n",
      "Train Epoch: 4 [64512/70488 (91%)]\tLoss: 0.957058\n",
      "570\n",
      "Train Epoch: 4 [65024/70488 (92%)]\tLoss: 0.951238\n",
      "571\n",
      "Train Epoch: 4 [65536/70488 (93%)]\tLoss: 0.953958\n",
      "572\n",
      "Train Epoch: 4 [66048/70488 (93%)]\tLoss: 0.955756\n",
      "573\n",
      "Train Epoch: 4 [66560/70488 (94%)]\tLoss: 0.961158\n",
      "574\n",
      "Train Epoch: 4 [67072/70488 (95%)]\tLoss: 0.953552\n",
      "575\n",
      "Train Epoch: 4 [67584/70488 (96%)]\tLoss: 0.959749\n",
      "576\n",
      "Train Epoch: 4 [68096/70488 (96%)]\tLoss: 0.954769\n",
      "577\n",
      "Train Epoch: 4 [68608/70488 (97%)]\tLoss: 0.954716\n",
      "578\n",
      "Train Epoch: 4 [69120/70488 (98%)]\tLoss: 0.952987\n",
      "579\n",
      "Train Epoch: 4 [69632/70488 (99%)]\tLoss: 0.952789\n",
      "580\n",
      "Train Epoch: 4 [47128/70488 (99%)]\tLoss: 0.954814\n",
      "581\n",
      "====> Epoch: 4 Average loss: 0.00375657\n",
      "====> Test set loss: 30.39287400\n",
      "Train Epoch: 5 [0/70488 (0%)]\tLoss: 0.952520\n",
      "592\n",
      "Train Epoch: 5 [512/70488 (1%)]\tLoss: 0.953136\n",
      "593\n",
      "Train Epoch: 5 [1024/70488 (1%)]\tLoss: 0.953321\n",
      "594\n",
      "Train Epoch: 5 [1536/70488 (2%)]\tLoss: 0.951563\n",
      "595\n",
      "Train Epoch: 5 [2048/70488 (3%)]\tLoss: 0.957582\n",
      "596\n",
      "Train Epoch: 5 [2560/70488 (4%)]\tLoss: 0.952909\n",
      "597\n",
      "Train Epoch: 5 [3072/70488 (4%)]\tLoss: 0.953434\n",
      "598\n",
      "Train Epoch: 5 [3584/70488 (5%)]\tLoss: 0.950135\n",
      "599\n",
      "Train Epoch: 5 [4096/70488 (6%)]\tLoss: 0.952926\n",
      "600\n",
      "Train Epoch: 5 [4608/70488 (7%)]\tLoss: 0.952694\n",
      "601\n",
      "Train Epoch: 5 [5120/70488 (7%)]\tLoss: 0.953340\n",
      "602\n",
      "Train Epoch: 5 [5632/70488 (8%)]\tLoss: 0.950965\n",
      "603\n",
      "Train Epoch: 5 [6144/70488 (9%)]\tLoss: 0.951798\n",
      "604\n",
      "Train Epoch: 5 [6656/70488 (9%)]\tLoss: 0.952902\n",
      "605\n",
      "Train Epoch: 5 [7168/70488 (10%)]\tLoss: 0.953054\n",
      "606\n",
      "Train Epoch: 5 [7680/70488 (11%)]\tLoss: 0.955240\n",
      "607\n",
      "Train Epoch: 5 [8192/70488 (12%)]\tLoss: 0.951909\n",
      "608\n",
      "Train Epoch: 5 [8704/70488 (12%)]\tLoss: 0.952071\n",
      "609\n",
      "Train Epoch: 5 [9216/70488 (13%)]\tLoss: 0.952885\n",
      "610\n",
      "Train Epoch: 5 [9728/70488 (14%)]\tLoss: 0.950500\n",
      "611\n",
      "Train Epoch: 5 [10240/70488 (14%)]\tLoss: 0.952549\n",
      "612\n",
      "Train Epoch: 5 [10752/70488 (15%)]\tLoss: 0.952698\n",
      "613\n",
      "Train Epoch: 5 [11264/70488 (16%)]\tLoss: 0.950702\n",
      "614\n",
      "Train Epoch: 5 [11776/70488 (17%)]\tLoss: 0.951420\n",
      "615\n",
      "Train Epoch: 5 [12288/70488 (17%)]\tLoss: 0.957935\n",
      "616\n",
      "Train Epoch: 5 [12800/70488 (18%)]\tLoss: 0.952055\n",
      "617\n",
      "Train Epoch: 5 [13312/70488 (19%)]\tLoss: 0.953278\n",
      "618\n",
      "Train Epoch: 5 [13824/70488 (20%)]\tLoss: 0.951736\n",
      "619\n",
      "Train Epoch: 5 [14336/70488 (20%)]\tLoss: 0.950624\n",
      "620\n",
      "Train Epoch: 5 [14848/70488 (21%)]\tLoss: 0.952518\n",
      "621\n",
      "Train Epoch: 5 [15360/70488 (22%)]\tLoss: 0.951341\n",
      "622\n",
      "Train Epoch: 5 [15872/70488 (22%)]\tLoss: 0.950495\n",
      "623\n",
      "Train Epoch: 5 [16384/70488 (23%)]\tLoss: 0.951487\n",
      "624\n",
      "Train Epoch: 5 [16896/70488 (24%)]\tLoss: 0.952358\n",
      "625\n",
      "Train Epoch: 5 [17408/70488 (25%)]\tLoss: 0.950333\n",
      "626\n",
      "Train Epoch: 5 [17920/70488 (25%)]\tLoss: 0.950311\n",
      "627\n",
      "Train Epoch: 5 [18432/70488 (26%)]\tLoss: 0.952692\n",
      "628\n",
      "Train Epoch: 5 [18944/70488 (27%)]\tLoss: 0.950703\n",
      "629\n",
      "Train Epoch: 5 [19456/70488 (28%)]\tLoss: 0.949032\n",
      "630\n",
      "Train Epoch: 5 [19968/70488 (28%)]\tLoss: 0.952701\n",
      "631\n",
      "Train Epoch: 5 [20480/70488 (29%)]\tLoss: 0.952680\n",
      "632\n",
      "Train Epoch: 5 [20992/70488 (30%)]\tLoss: 0.949901\n",
      "633\n",
      "Train Epoch: 5 [21504/70488 (30%)]\tLoss: 0.948568\n",
      "634\n",
      "Train Epoch: 5 [22016/70488 (31%)]\tLoss: 0.949377\n",
      "635\n",
      "Train Epoch: 5 [22528/70488 (32%)]\tLoss: 0.950210\n",
      "636\n",
      "Train Epoch: 5 [23040/70488 (33%)]\tLoss: 0.950861\n",
      "637\n",
      "Train Epoch: 5 [23552/70488 (33%)]\tLoss: 0.950808\n",
      "638\n",
      "Train Epoch: 5 [24064/70488 (34%)]\tLoss: 0.953074\n",
      "639\n",
      "Train Epoch: 5 [24576/70488 (35%)]\tLoss: 0.950961\n",
      "640\n",
      "Train Epoch: 5 [25088/70488 (36%)]\tLoss: 0.952222\n",
      "641\n",
      "Train Epoch: 5 [25600/70488 (36%)]\tLoss: 0.952443\n",
      "642\n",
      "Train Epoch: 5 [26112/70488 (37%)]\tLoss: 0.949586\n",
      "643\n",
      "Train Epoch: 5 [26624/70488 (38%)]\tLoss: 0.950029\n",
      "644\n",
      "Train Epoch: 5 [27136/70488 (38%)]\tLoss: 0.949216\n",
      "645\n",
      "Train Epoch: 5 [27648/70488 (39%)]\tLoss: 0.951097\n",
      "646\n",
      "Train Epoch: 5 [28160/70488 (40%)]\tLoss: 0.948562\n",
      "647\n",
      "Train Epoch: 5 [28672/70488 (41%)]\tLoss: 0.951756\n",
      "648\n",
      "Train Epoch: 5 [29184/70488 (41%)]\tLoss: 0.950095\n",
      "649\n",
      "Train Epoch: 5 [29696/70488 (42%)]\tLoss: 0.953347\n",
      "650\n",
      "Train Epoch: 5 [30208/70488 (43%)]\tLoss: 0.950035\n",
      "651\n",
      "Train Epoch: 5 [30720/70488 (43%)]\tLoss: 0.951429\n",
      "652\n",
      "Train Epoch: 5 [31232/70488 (44%)]\tLoss: 0.946176\n",
      "653\n",
      "Train Epoch: 5 [31744/70488 (45%)]\tLoss: 0.949870\n",
      "654\n",
      "Train Epoch: 5 [32256/70488 (46%)]\tLoss: 0.949790\n",
      "655\n",
      "Train Epoch: 5 [32768/70488 (46%)]\tLoss: 0.953563\n",
      "656\n",
      "Train Epoch: 5 [33280/70488 (47%)]\tLoss: 0.951123\n",
      "657\n",
      "Train Epoch: 5 [33792/70488 (48%)]\tLoss: 0.948656\n",
      "658\n",
      "Train Epoch: 5 [34304/70488 (49%)]\tLoss: 0.949852\n",
      "659\n",
      "Train Epoch: 5 [34816/70488 (49%)]\tLoss: 0.950622\n",
      "660\n",
      "Train Epoch: 5 [35328/70488 (50%)]\tLoss: 0.951425\n",
      "661\n",
      "Train Epoch: 5 [35840/70488 (51%)]\tLoss: 0.957096\n",
      "662\n",
      "Train Epoch: 5 [36352/70488 (51%)]\tLoss: 0.950070\n",
      "663\n",
      "Train Epoch: 5 [36864/70488 (52%)]\tLoss: 0.950769\n",
      "664\n",
      "Train Epoch: 5 [37376/70488 (53%)]\tLoss: 0.950299\n",
      "665\n",
      "Train Epoch: 5 [37888/70488 (54%)]\tLoss: 0.948404\n",
      "666\n",
      "Train Epoch: 5 [38400/70488 (54%)]\tLoss: 0.950352\n",
      "667\n",
      "Train Epoch: 5 [38912/70488 (55%)]\tLoss: 0.951719\n",
      "668\n",
      "Train Epoch: 5 [39424/70488 (56%)]\tLoss: 0.949908\n",
      "669\n",
      "Train Epoch: 5 [39936/70488 (57%)]\tLoss: 0.946679\n",
      "670\n",
      "Train Epoch: 5 [40448/70488 (57%)]\tLoss: 0.948099\n",
      "671\n",
      "Train Epoch: 5 [40960/70488 (58%)]\tLoss: 0.950015\n",
      "672\n",
      "Train Epoch: 5 [41472/70488 (59%)]\tLoss: 0.951640\n",
      "673\n",
      "Train Epoch: 5 [41984/70488 (59%)]\tLoss: 0.951139\n",
      "674\n",
      "Train Epoch: 5 [42496/70488 (60%)]\tLoss: 0.954899\n",
      "675\n",
      "Train Epoch: 5 [43008/70488 (61%)]\tLoss: 0.950286\n",
      "676\n",
      "Train Epoch: 5 [43520/70488 (62%)]\tLoss: 0.948207\n",
      "677\n",
      "Train Epoch: 5 [44032/70488 (62%)]\tLoss: 0.949622\n",
      "678\n",
      "Train Epoch: 5 [44544/70488 (63%)]\tLoss: 0.947657\n",
      "679\n",
      "Train Epoch: 5 [45056/70488 (64%)]\tLoss: 0.947371\n",
      "680\n",
      "Train Epoch: 5 [45568/70488 (64%)]\tLoss: 0.951958\n",
      "681\n",
      "Train Epoch: 5 [46080/70488 (65%)]\tLoss: 0.951535\n",
      "682\n",
      "Train Epoch: 5 [46592/70488 (66%)]\tLoss: 0.949041\n",
      "683\n",
      "Train Epoch: 5 [47104/70488 (67%)]\tLoss: 0.951044\n",
      "684\n",
      "Train Epoch: 5 [47616/70488 (67%)]\tLoss: 0.948091\n",
      "685\n",
      "Train Epoch: 5 [48128/70488 (68%)]\tLoss: 0.949785\n",
      "686\n",
      "Train Epoch: 5 [48640/70488 (69%)]\tLoss: 0.947790\n",
      "687\n",
      "Train Epoch: 5 [49152/70488 (70%)]\tLoss: 0.953028\n",
      "688\n",
      "Train Epoch: 5 [49664/70488 (70%)]\tLoss: 0.948422\n",
      "689\n",
      "Train Epoch: 5 [50176/70488 (71%)]\tLoss: 0.952685\n",
      "690\n",
      "Train Epoch: 5 [50688/70488 (72%)]\tLoss: 0.946488\n",
      "691\n",
      "Train Epoch: 5 [51200/70488 (72%)]\tLoss: 0.947448\n",
      "692\n",
      "Train Epoch: 5 [51712/70488 (73%)]\tLoss: 0.947097\n",
      "693\n",
      "Train Epoch: 5 [52224/70488 (74%)]\tLoss: 0.950165\n",
      "694\n",
      "Train Epoch: 5 [52736/70488 (75%)]\tLoss: 0.949605\n",
      "695\n",
      "Train Epoch: 5 [53248/70488 (75%)]\tLoss: 0.949897\n",
      "696\n",
      "Train Epoch: 5 [53760/70488 (76%)]\tLoss: 0.947623\n",
      "697\n",
      "Train Epoch: 5 [54272/70488 (77%)]\tLoss: 0.948686\n",
      "698\n",
      "Train Epoch: 5 [54784/70488 (78%)]\tLoss: 0.946721\n",
      "699\n",
      "Train Epoch: 5 [55296/70488 (78%)]\tLoss: 0.948030\n",
      "700\n",
      "Train Epoch: 5 [55808/70488 (79%)]\tLoss: 0.949870\n",
      "701\n",
      "Train Epoch: 5 [56320/70488 (80%)]\tLoss: 0.946504\n",
      "702\n",
      "Train Epoch: 5 [56832/70488 (80%)]\tLoss: 0.946679\n",
      "703\n",
      "Train Epoch: 5 [57344/70488 (81%)]\tLoss: 0.948753\n",
      "704\n",
      "Train Epoch: 5 [57856/70488 (82%)]\tLoss: 0.949001\n",
      "705\n",
      "Train Epoch: 5 [58368/70488 (83%)]\tLoss: 0.947561\n",
      "706\n",
      "Train Epoch: 5 [58880/70488 (83%)]\tLoss: 0.948237\n",
      "707\n",
      "Train Epoch: 5 [59392/70488 (84%)]\tLoss: 0.948402\n",
      "708\n",
      "Train Epoch: 5 [59904/70488 (85%)]\tLoss: 0.950070\n",
      "709\n",
      "Train Epoch: 5 [60416/70488 (86%)]\tLoss: 0.947684\n",
      "710\n",
      "Train Epoch: 5 [60928/70488 (86%)]\tLoss: 0.949357\n",
      "711\n",
      "Train Epoch: 5 [61440/70488 (87%)]\tLoss: 0.949875\n",
      "712\n",
      "Train Epoch: 5 [61952/70488 (88%)]\tLoss: 0.946541\n",
      "713\n",
      "Train Epoch: 5 [62464/70488 (88%)]\tLoss: 0.948138\n",
      "714\n",
      "Train Epoch: 5 [62976/70488 (89%)]\tLoss: 0.951701\n",
      "715\n",
      "Train Epoch: 5 [63488/70488 (90%)]\tLoss: 0.947011\n",
      "716\n",
      "Train Epoch: 5 [64000/70488 (91%)]\tLoss: 0.947397\n",
      "717\n",
      "Train Epoch: 5 [64512/70488 (91%)]\tLoss: 0.951993\n",
      "718\n",
      "Train Epoch: 5 [65024/70488 (92%)]\tLoss: 0.947553\n",
      "719\n",
      "Train Epoch: 5 [65536/70488 (93%)]\tLoss: 0.947092\n",
      "720\n",
      "Train Epoch: 5 [66048/70488 (93%)]\tLoss: 0.946318\n",
      "721\n",
      "Train Epoch: 5 [66560/70488 (94%)]\tLoss: 0.945568\n",
      "722\n",
      "Train Epoch: 5 [67072/70488 (95%)]\tLoss: 0.949015\n",
      "723\n",
      "Train Epoch: 5 [67584/70488 (96%)]\tLoss: 0.946972\n",
      "724\n",
      "Train Epoch: 5 [68096/70488 (96%)]\tLoss: 0.954276\n",
      "725\n",
      "Train Epoch: 5 [68608/70488 (97%)]\tLoss: 0.946485\n",
      "726\n",
      "Train Epoch: 5 [69120/70488 (98%)]\tLoss: 0.950357\n",
      "727\n",
      "Train Epoch: 5 [69632/70488 (99%)]\tLoss: 0.946057\n",
      "728\n",
      "Train Epoch: 5 [47128/70488 (99%)]\tLoss: 0.947738\n",
      "729\n",
      "====> Epoch: 5 Average loss: 0.00372122\n",
      "====> Test set loss: 30.35425329\n",
      "Train Epoch: 6 [0/70488 (0%)]\tLoss: 0.945749\n",
      "740\n",
      "Train Epoch: 6 [512/70488 (1%)]\tLoss: 0.950516\n",
      "741\n",
      "Train Epoch: 6 [1024/70488 (1%)]\tLoss: 0.947274\n",
      "742\n",
      "Train Epoch: 6 [1536/70488 (2%)]\tLoss: 0.946733\n",
      "743\n",
      "Train Epoch: 6 [2048/70488 (3%)]\tLoss: 0.950982\n",
      "744\n",
      "Train Epoch: 6 [2560/70488 (4%)]\tLoss: 0.945891\n",
      "745\n",
      "Train Epoch: 6 [3072/70488 (4%)]\tLoss: 0.945039\n",
      "746\n",
      "Train Epoch: 6 [3584/70488 (5%)]\tLoss: 0.946575\n",
      "747\n",
      "Train Epoch: 6 [4096/70488 (6%)]\tLoss: 0.946015\n",
      "748\n",
      "Train Epoch: 6 [4608/70488 (7%)]\tLoss: 0.943731\n",
      "749\n",
      "Train Epoch: 6 [5120/70488 (7%)]\tLoss: 0.944226\n",
      "750\n",
      "Train Epoch: 6 [5632/70488 (8%)]\tLoss: 0.944016\n",
      "751\n",
      "Train Epoch: 6 [6144/70488 (9%)]\tLoss: 0.946572\n",
      "752\n",
      "Train Epoch: 6 [6656/70488 (9%)]\tLoss: 0.945226\n",
      "753\n",
      "Train Epoch: 6 [7168/70488 (10%)]\tLoss: 0.946318\n",
      "754\n",
      "Train Epoch: 6 [7680/70488 (11%)]\tLoss: 0.947022\n",
      "755\n",
      "Train Epoch: 6 [8192/70488 (12%)]\tLoss: 0.945409\n",
      "756\n",
      "Train Epoch: 6 [8704/70488 (12%)]\tLoss: 0.946669\n",
      "757\n",
      "Train Epoch: 6 [9216/70488 (13%)]\tLoss: 0.947119\n",
      "758\n",
      "Train Epoch: 6 [9728/70488 (14%)]\tLoss: 0.944364\n",
      "759\n",
      "Train Epoch: 6 [10240/70488 (14%)]\tLoss: 0.944458\n",
      "760\n",
      "Train Epoch: 6 [10752/70488 (15%)]\tLoss: 0.944836\n",
      "761\n",
      "Train Epoch: 6 [11264/70488 (16%)]\tLoss: 0.943113\n",
      "762\n",
      "Train Epoch: 6 [11776/70488 (17%)]\tLoss: 0.945601\n",
      "763\n",
      "Train Epoch: 6 [12288/70488 (17%)]\tLoss: 0.943331\n",
      "764\n",
      "Train Epoch: 6 [12800/70488 (18%)]\tLoss: 0.943696\n",
      "765\n",
      "Train Epoch: 6 [13312/70488 (19%)]\tLoss: 0.948783\n",
      "766\n",
      "Train Epoch: 6 [13824/70488 (20%)]\tLoss: 0.941314\n",
      "767\n",
      "Train Epoch: 6 [14336/70488 (20%)]\tLoss: 0.944005\n",
      "768\n",
      "Train Epoch: 6 [14848/70488 (21%)]\tLoss: 0.944898\n",
      "769\n",
      "Train Epoch: 6 [15360/70488 (22%)]\tLoss: 0.945536\n",
      "770\n",
      "Train Epoch: 6 [15872/70488 (22%)]\tLoss: 0.946798\n",
      "771\n",
      "Train Epoch: 6 [16384/70488 (23%)]\tLoss: 0.943462\n",
      "772\n",
      "Train Epoch: 6 [16896/70488 (24%)]\tLoss: 0.945131\n",
      "773\n",
      "Train Epoch: 6 [17408/70488 (25%)]\tLoss: 0.945613\n",
      "774\n",
      "Train Epoch: 6 [17920/70488 (25%)]\tLoss: 0.949071\n",
      "775\n",
      "Train Epoch: 6 [18432/70488 (26%)]\tLoss: 0.944539\n",
      "776\n",
      "Train Epoch: 6 [18944/70488 (27%)]\tLoss: 0.949597\n",
      "777\n",
      "Train Epoch: 6 [19456/70488 (28%)]\tLoss: 0.945846\n",
      "778\n",
      "Train Epoch: 6 [19968/70488 (28%)]\tLoss: 0.944753\n",
      "779\n",
      "Train Epoch: 6 [20480/70488 (29%)]\tLoss: 0.945486\n",
      "780\n",
      "Train Epoch: 6 [20992/70488 (30%)]\tLoss: 0.942694\n",
      "781\n",
      "Train Epoch: 6 [21504/70488 (30%)]\tLoss: 0.945489\n",
      "782\n",
      "Train Epoch: 6 [22016/70488 (31%)]\tLoss: 0.943752\n",
      "783\n",
      "Train Epoch: 6 [22528/70488 (32%)]\tLoss: 0.945264\n",
      "784\n",
      "Train Epoch: 6 [23040/70488 (33%)]\tLoss: 0.943436\n",
      "785\n",
      "Train Epoch: 6 [23552/70488 (33%)]\tLoss: 0.945636\n",
      "786\n",
      "Train Epoch: 6 [24064/70488 (34%)]\tLoss: 0.946656\n",
      "787\n",
      "Train Epoch: 6 [24576/70488 (35%)]\tLoss: 0.944882\n",
      "788\n",
      "Train Epoch: 6 [25088/70488 (36%)]\tLoss: 0.943848\n",
      "789\n",
      "Train Epoch: 6 [25600/70488 (36%)]\tLoss: 0.944503\n",
      "790\n",
      "Train Epoch: 6 [26112/70488 (37%)]\tLoss: 0.945445\n",
      "791\n",
      "Train Epoch: 6 [26624/70488 (38%)]\tLoss: 0.946187\n",
      "792\n",
      "Train Epoch: 6 [27136/70488 (38%)]\tLoss: 0.949471\n",
      "793\n",
      "Train Epoch: 6 [27648/70488 (39%)]\tLoss: 0.942308\n",
      "794\n",
      "Train Epoch: 6 [28160/70488 (40%)]\tLoss: 0.951084\n",
      "795\n",
      "Train Epoch: 6 [28672/70488 (41%)]\tLoss: 0.942374\n",
      "796\n",
      "Train Epoch: 6 [29184/70488 (41%)]\tLoss: 0.942495\n",
      "797\n",
      "Train Epoch: 6 [29696/70488 (42%)]\tLoss: 0.945188\n",
      "798\n",
      "Train Epoch: 6 [30208/70488 (43%)]\tLoss: 0.943495\n",
      "799\n",
      "Train Epoch: 6 [30720/70488 (43%)]\tLoss: 0.944576\n",
      "800\n",
      "Train Epoch: 6 [31232/70488 (44%)]\tLoss: 0.945385\n",
      "801\n",
      "Train Epoch: 6 [31744/70488 (45%)]\tLoss: 0.941607\n",
      "802\n",
      "Train Epoch: 6 [32256/70488 (46%)]\tLoss: 0.946363\n",
      "803\n",
      "Train Epoch: 6 [32768/70488 (46%)]\tLoss: 0.944063\n",
      "804\n",
      "Train Epoch: 6 [33280/70488 (47%)]\tLoss: 0.943351\n",
      "805\n",
      "Train Epoch: 6 [33792/70488 (48%)]\tLoss: 0.941688\n",
      "806\n",
      "Train Epoch: 6 [34304/70488 (49%)]\tLoss: 0.946864\n",
      "807\n",
      "Train Epoch: 6 [34816/70488 (49%)]\tLoss: 0.941381\n",
      "808\n",
      "Train Epoch: 6 [35328/70488 (50%)]\tLoss: 0.944725\n",
      "809\n",
      "Train Epoch: 6 [35840/70488 (51%)]\tLoss: 0.942213\n",
      "810\n",
      "Train Epoch: 6 [36352/70488 (51%)]\tLoss: 0.943019\n",
      "811\n",
      "Train Epoch: 6 [36864/70488 (52%)]\tLoss: 0.944692\n",
      "812\n",
      "Train Epoch: 6 [37376/70488 (53%)]\tLoss: 0.944097\n",
      "813\n",
      "Train Epoch: 6 [37888/70488 (54%)]\tLoss: 0.945835\n",
      "814\n",
      "Train Epoch: 6 [38400/70488 (54%)]\tLoss: 0.946844\n",
      "815\n",
      "Train Epoch: 6 [38912/70488 (55%)]\tLoss: 0.943629\n",
      "816\n",
      "Train Epoch: 6 [39424/70488 (56%)]\tLoss: 0.945022\n",
      "817\n",
      "Train Epoch: 6 [39936/70488 (57%)]\tLoss: 0.941891\n",
      "818\n",
      "Train Epoch: 6 [40448/70488 (57%)]\tLoss: 0.944980\n",
      "819\n",
      "Train Epoch: 6 [40960/70488 (58%)]\tLoss: 0.946093\n",
      "820\n",
      "Train Epoch: 6 [41472/70488 (59%)]\tLoss: 0.942608\n",
      "821\n",
      "Train Epoch: 6 [41984/70488 (59%)]\tLoss: 0.942366\n",
      "822\n",
      "Train Epoch: 6 [42496/70488 (60%)]\tLoss: 0.940741\n",
      "823\n",
      "Train Epoch: 6 [43008/70488 (61%)]\tLoss: 0.942790\n",
      "824\n",
      "Train Epoch: 6 [43520/70488 (62%)]\tLoss: 0.943370\n",
      "825\n",
      "Train Epoch: 6 [44032/70488 (62%)]\tLoss: 0.941739\n",
      "826\n",
      "Train Epoch: 6 [44544/70488 (63%)]\tLoss: 0.943352\n",
      "827\n",
      "Train Epoch: 6 [45056/70488 (64%)]\tLoss: 0.943635\n",
      "828\n",
      "Train Epoch: 6 [45568/70488 (64%)]\tLoss: 0.941531\n",
      "829\n",
      "Train Epoch: 6 [46080/70488 (65%)]\tLoss: 0.940700\n",
      "830\n",
      "Train Epoch: 6 [46592/70488 (66%)]\tLoss: 0.942186\n",
      "831\n",
      "Train Epoch: 6 [47104/70488 (67%)]\tLoss: 0.945354\n",
      "832\n",
      "Train Epoch: 6 [47616/70488 (67%)]\tLoss: 0.942225\n",
      "833\n",
      "Train Epoch: 6 [48128/70488 (68%)]\tLoss: 0.942285\n",
      "834\n",
      "Train Epoch: 6 [48640/70488 (69%)]\tLoss: 0.941828\n",
      "835\n",
      "Train Epoch: 6 [49152/70488 (70%)]\tLoss: 0.941860\n",
      "836\n",
      "Train Epoch: 6 [49664/70488 (70%)]\tLoss: 0.944313\n",
      "837\n",
      "Train Epoch: 6 [50176/70488 (71%)]\tLoss: 0.941595\n",
      "838\n",
      "Train Epoch: 6 [50688/70488 (72%)]\tLoss: 0.941729\n",
      "839\n",
      "Train Epoch: 6 [51200/70488 (72%)]\tLoss: 0.944964\n",
      "840\n",
      "Train Epoch: 6 [51712/70488 (73%)]\tLoss: 0.944674\n",
      "841\n",
      "Train Epoch: 6 [52224/70488 (74%)]\tLoss: 0.943547\n",
      "842\n",
      "Train Epoch: 6 [52736/70488 (75%)]\tLoss: 0.943295\n",
      "843\n",
      "Train Epoch: 6 [53248/70488 (75%)]\tLoss: 0.941805\n",
      "844\n",
      "Train Epoch: 6 [53760/70488 (76%)]\tLoss: 0.948633\n",
      "845\n",
      "Train Epoch: 6 [54272/70488 (77%)]\tLoss: 0.940451\n",
      "846\n",
      "Train Epoch: 6 [54784/70488 (78%)]\tLoss: 0.943008\n",
      "847\n",
      "Train Epoch: 6 [55296/70488 (78%)]\tLoss: 0.941588\n",
      "848\n",
      "Train Epoch: 6 [55808/70488 (79%)]\tLoss: 0.943541\n",
      "849\n",
      "Train Epoch: 6 [56320/70488 (80%)]\tLoss: 0.944375\n",
      "850\n",
      "Train Epoch: 6 [56832/70488 (80%)]\tLoss: 0.943406\n",
      "851\n",
      "Train Epoch: 6 [57344/70488 (81%)]\tLoss: 0.941236\n",
      "852\n",
      "Train Epoch: 6 [57856/70488 (82%)]\tLoss: 0.942464\n",
      "853\n",
      "Train Epoch: 6 [58368/70488 (83%)]\tLoss: 0.943936\n",
      "854\n",
      "Train Epoch: 6 [58880/70488 (83%)]\tLoss: 0.944827\n",
      "855\n",
      "Train Epoch: 6 [59392/70488 (84%)]\tLoss: 0.942868\n",
      "856\n",
      "Train Epoch: 6 [59904/70488 (85%)]\tLoss: 0.943612\n",
      "857\n",
      "Train Epoch: 6 [60416/70488 (86%)]\tLoss: 0.941361\n",
      "858\n",
      "Train Epoch: 6 [60928/70488 (86%)]\tLoss: 0.942595\n",
      "859\n",
      "Train Epoch: 6 [61440/70488 (87%)]\tLoss: 0.941678\n",
      "860\n",
      "Train Epoch: 6 [61952/70488 (88%)]\tLoss: 0.944106\n",
      "861\n",
      "Train Epoch: 6 [62464/70488 (88%)]\tLoss: 0.944204\n",
      "862\n",
      "Train Epoch: 6 [62976/70488 (89%)]\tLoss: 0.940072\n",
      "863\n",
      "Train Epoch: 6 [63488/70488 (90%)]\tLoss: 0.941629\n",
      "864\n",
      "Train Epoch: 6 [64000/70488 (91%)]\tLoss: 0.944245\n",
      "865\n",
      "Train Epoch: 6 [64512/70488 (91%)]\tLoss: 0.939835\n",
      "866\n",
      "Train Epoch: 6 [65024/70488 (92%)]\tLoss: 0.939350\n",
      "867\n",
      "Train Epoch: 6 [65536/70488 (93%)]\tLoss: 0.941085\n",
      "868\n",
      "Train Epoch: 6 [66048/70488 (93%)]\tLoss: 0.939150\n",
      "869\n",
      "Train Epoch: 6 [66560/70488 (94%)]\tLoss: 0.940624\n",
      "870\n",
      "Train Epoch: 6 [67072/70488 (95%)]\tLoss: 0.940711\n",
      "871\n",
      "Train Epoch: 6 [67584/70488 (96%)]\tLoss: 0.939804\n",
      "872\n",
      "Train Epoch: 6 [68096/70488 (96%)]\tLoss: 0.940431\n",
      "873\n",
      "Train Epoch: 6 [68608/70488 (97%)]\tLoss: 0.942602\n",
      "874\n",
      "Train Epoch: 6 [69120/70488 (98%)]\tLoss: 0.943899\n",
      "875\n",
      "Train Epoch: 6 [69632/70488 (99%)]\tLoss: 0.944239\n",
      "876\n",
      "Train Epoch: 6 [47128/70488 (99%)]\tLoss: 0.941255\n",
      "877\n",
      "====> Epoch: 6 Average loss: 0.00369649\n",
      "====> Test set loss: 30.30858350\n",
      "Train Epoch: 7 [0/70488 (0%)]\tLoss: 0.940206\n",
      "888\n",
      "Train Epoch: 7 [512/70488 (1%)]\tLoss: 0.939861\n",
      "889\n",
      "Train Epoch: 7 [1024/70488 (1%)]\tLoss: 0.940630\n",
      "890\n",
      "Train Epoch: 7 [1536/70488 (2%)]\tLoss: 0.940556\n",
      "891\n",
      "Train Epoch: 7 [2048/70488 (3%)]\tLoss: 0.939403\n",
      "892\n",
      "Train Epoch: 7 [2560/70488 (4%)]\tLoss: 0.940872\n",
      "893\n",
      "Train Epoch: 7 [3072/70488 (4%)]\tLoss: 0.941147\n",
      "894\n",
      "Train Epoch: 7 [3584/70488 (5%)]\tLoss: 0.941348\n",
      "895\n",
      "Train Epoch: 7 [4096/70488 (6%)]\tLoss: 0.937861\n",
      "896\n",
      "Train Epoch: 7 [4608/70488 (7%)]\tLoss: 0.938857\n",
      "897\n",
      "Train Epoch: 7 [5120/70488 (7%)]\tLoss: 0.938888\n",
      "898\n",
      "Train Epoch: 7 [5632/70488 (8%)]\tLoss: 0.939782\n",
      "899\n",
      "Train Epoch: 7 [6144/70488 (9%)]\tLoss: 0.940085\n",
      "900\n",
      "Train Epoch: 7 [6656/70488 (9%)]\tLoss: 0.938467\n",
      "901\n",
      "Train Epoch: 7 [7168/70488 (10%)]\tLoss: 0.943717\n",
      "902\n",
      "Train Epoch: 7 [7680/70488 (11%)]\tLoss: 0.938742\n",
      "903\n",
      "Train Epoch: 7 [8192/70488 (12%)]\tLoss: 0.939231\n",
      "904\n",
      "Train Epoch: 7 [8704/70488 (12%)]\tLoss: 0.937673\n",
      "905\n",
      "Train Epoch: 7 [9216/70488 (13%)]\tLoss: 0.937098\n",
      "906\n",
      "Train Epoch: 7 [9728/70488 (14%)]\tLoss: 0.940596\n",
      "907\n",
      "Train Epoch: 7 [10240/70488 (14%)]\tLoss: 0.939779\n",
      "908\n",
      "Train Epoch: 7 [10752/70488 (15%)]\tLoss: 0.939779\n",
      "909\n",
      "Train Epoch: 7 [11264/70488 (16%)]\tLoss: 0.939885\n",
      "910\n",
      "Train Epoch: 7 [11776/70488 (17%)]\tLoss: 0.935687\n",
      "911\n",
      "Train Epoch: 7 [12288/70488 (17%)]\tLoss: 0.938690\n",
      "912\n",
      "Train Epoch: 7 [12800/70488 (18%)]\tLoss: 0.939750\n",
      "913\n",
      "Train Epoch: 7 [13312/70488 (19%)]\tLoss: 0.938203\n",
      "914\n",
      "Train Epoch: 7 [13824/70488 (20%)]\tLoss: 0.941968\n",
      "915\n",
      "Train Epoch: 7 [14336/70488 (20%)]\tLoss: 0.938265\n",
      "916\n",
      "Train Epoch: 7 [14848/70488 (21%)]\tLoss: 0.941141\n",
      "917\n",
      "Train Epoch: 7 [15360/70488 (22%)]\tLoss: 0.936683\n",
      "918\n",
      "Train Epoch: 7 [15872/70488 (22%)]\tLoss: 0.943556\n",
      "919\n",
      "Train Epoch: 7 [16384/70488 (23%)]\tLoss: 0.938127\n",
      "920\n",
      "Train Epoch: 7 [16896/70488 (24%)]\tLoss: 0.939432\n",
      "921\n",
      "Train Epoch: 7 [17408/70488 (25%)]\tLoss: 0.940926\n",
      "922\n",
      "Train Epoch: 7 [17920/70488 (25%)]\tLoss: 0.938724\n",
      "923\n",
      "Train Epoch: 7 [18432/70488 (26%)]\tLoss: 0.939286\n",
      "924\n",
      "Train Epoch: 7 [18944/70488 (27%)]\tLoss: 0.938766\n",
      "925\n",
      "Train Epoch: 7 [19456/70488 (28%)]\tLoss: 0.939876\n",
      "926\n",
      "Train Epoch: 7 [19968/70488 (28%)]\tLoss: 0.939385\n",
      "927\n",
      "Train Epoch: 7 [20480/70488 (29%)]\tLoss: 0.937189\n",
      "928\n",
      "Train Epoch: 7 [20992/70488 (30%)]\tLoss: 0.939162\n",
      "929\n",
      "Train Epoch: 7 [21504/70488 (30%)]\tLoss: 0.936420\n",
      "930\n",
      "Train Epoch: 7 [22016/70488 (31%)]\tLoss: 0.939266\n",
      "931\n",
      "Train Epoch: 7 [22528/70488 (32%)]\tLoss: 0.936364\n",
      "932\n",
      "Train Epoch: 7 [23040/70488 (33%)]\tLoss: 0.940100\n",
      "933\n",
      "Train Epoch: 7 [23552/70488 (33%)]\tLoss: 0.946050\n",
      "934\n",
      "Train Epoch: 7 [24064/70488 (34%)]\tLoss: 0.938171\n",
      "935\n",
      "Train Epoch: 7 [24576/70488 (35%)]\tLoss: 0.938126\n",
      "936\n",
      "Train Epoch: 7 [25088/70488 (36%)]\tLoss: 0.936879\n",
      "937\n",
      "Train Epoch: 7 [25600/70488 (36%)]\tLoss: 0.937478\n",
      "938\n",
      "Train Epoch: 7 [26112/70488 (37%)]\tLoss: 0.937574\n",
      "939\n",
      "Train Epoch: 7 [26624/70488 (38%)]\tLoss: 0.938446\n",
      "940\n",
      "Train Epoch: 7 [27136/70488 (38%)]\tLoss: 0.939507\n",
      "941\n",
      "Train Epoch: 7 [27648/70488 (39%)]\tLoss: 0.938604\n",
      "942\n",
      "Train Epoch: 7 [28160/70488 (40%)]\tLoss: 0.941382\n",
      "943\n",
      "Train Epoch: 7 [28672/70488 (41%)]\tLoss: 0.936463\n",
      "944\n",
      "Train Epoch: 7 [29184/70488 (41%)]\tLoss: 0.937370\n",
      "945\n",
      "Train Epoch: 7 [29696/70488 (42%)]\tLoss: 0.942796\n",
      "946\n",
      "Train Epoch: 7 [30208/70488 (43%)]\tLoss: 0.938276\n",
      "947\n",
      "Train Epoch: 7 [30720/70488 (43%)]\tLoss: 0.939429\n",
      "948\n",
      "Train Epoch: 7 [31232/70488 (44%)]\tLoss: 0.935602\n",
      "949\n",
      "Train Epoch: 7 [31744/70488 (45%)]\tLoss: 0.935953\n",
      "950\n",
      "Train Epoch: 7 [32256/70488 (46%)]\tLoss: 0.937882\n",
      "951\n",
      "Train Epoch: 7 [32768/70488 (46%)]\tLoss: 0.934718\n",
      "952\n",
      "Train Epoch: 7 [33280/70488 (47%)]\tLoss: 0.936321\n",
      "953\n",
      "Train Epoch: 7 [33792/70488 (48%)]\tLoss: 0.940128\n",
      "954\n",
      "Train Epoch: 7 [34304/70488 (49%)]\tLoss: 0.935013\n",
      "955\n",
      "Train Epoch: 7 [34816/70488 (49%)]\tLoss: 0.938486\n",
      "956\n",
      "Train Epoch: 7 [35328/70488 (50%)]\tLoss: 0.939062\n",
      "957\n",
      "Train Epoch: 7 [35840/70488 (51%)]\tLoss: 0.936765\n",
      "958\n",
      "Train Epoch: 7 [36352/70488 (51%)]\tLoss: 0.936942\n",
      "959\n",
      "Train Epoch: 7 [36864/70488 (52%)]\tLoss: 0.935541\n",
      "960\n",
      "Train Epoch: 7 [37376/70488 (53%)]\tLoss: 0.938994\n",
      "961\n",
      "Train Epoch: 7 [37888/70488 (54%)]\tLoss: 0.940757\n",
      "962\n",
      "Train Epoch: 7 [38400/70488 (54%)]\tLoss: 0.939481\n",
      "963\n",
      "Train Epoch: 7 [38912/70488 (55%)]\tLoss: 0.937448\n",
      "964\n",
      "Train Epoch: 7 [39424/70488 (56%)]\tLoss: 0.938337\n",
      "965\n",
      "Train Epoch: 7 [39936/70488 (57%)]\tLoss: 0.937082\n",
      "966\n",
      "Train Epoch: 7 [40448/70488 (57%)]\tLoss: 0.935544\n",
      "967\n",
      "Train Epoch: 7 [40960/70488 (58%)]\tLoss: 0.937773\n",
      "968\n",
      "Train Epoch: 7 [41472/70488 (59%)]\tLoss: 0.938665\n",
      "969\n",
      "Train Epoch: 7 [41984/70488 (59%)]\tLoss: 0.942216\n",
      "970\n",
      "Train Epoch: 7 [42496/70488 (60%)]\tLoss: 0.937919\n",
      "971\n",
      "Train Epoch: 7 [43008/70488 (61%)]\tLoss: 0.934505\n",
      "972\n",
      "Train Epoch: 7 [43520/70488 (62%)]\tLoss: 0.938540\n",
      "973\n",
      "Train Epoch: 7 [44032/70488 (62%)]\tLoss: 0.938314\n",
      "974\n",
      "Train Epoch: 7 [44544/70488 (63%)]\tLoss: 0.934958\n",
      "975\n",
      "Train Epoch: 7 [45056/70488 (64%)]\tLoss: 0.937025\n",
      "976\n",
      "Train Epoch: 7 [45568/70488 (64%)]\tLoss: 0.936315\n",
      "977\n",
      "Train Epoch: 7 [46080/70488 (65%)]\tLoss: 0.940026\n",
      "978\n",
      "Train Epoch: 7 [46592/70488 (66%)]\tLoss: 0.934851\n",
      "979\n",
      "Train Epoch: 7 [47104/70488 (67%)]\tLoss: 0.935775\n",
      "980\n",
      "Train Epoch: 7 [47616/70488 (67%)]\tLoss: 0.935542\n",
      "981\n",
      "Train Epoch: 7 [48128/70488 (68%)]\tLoss: 0.939794\n",
      "982\n",
      "Train Epoch: 7 [48640/70488 (69%)]\tLoss: 0.936376\n",
      "983\n",
      "Train Epoch: 7 [49152/70488 (70%)]\tLoss: 0.937354\n",
      "984\n",
      "Train Epoch: 7 [49664/70488 (70%)]\tLoss: 0.934597\n",
      "985\n",
      "Train Epoch: 7 [50176/70488 (71%)]\tLoss: 0.936541\n",
      "986\n",
      "Train Epoch: 7 [50688/70488 (72%)]\tLoss: 0.937517\n",
      "987\n",
      "Train Epoch: 7 [51200/70488 (72%)]\tLoss: 0.940821\n",
      "988\n",
      "Train Epoch: 7 [51712/70488 (73%)]\tLoss: 0.937194\n",
      "989\n",
      "Train Epoch: 7 [52224/70488 (74%)]\tLoss: 0.935486\n",
      "990\n",
      "Train Epoch: 7 [52736/70488 (75%)]\tLoss: 0.935455\n",
      "991\n",
      "Train Epoch: 7 [53248/70488 (75%)]\tLoss: 0.938481\n",
      "992\n",
      "Train Epoch: 7 [53760/70488 (76%)]\tLoss: 0.937270\n",
      "993\n",
      "Train Epoch: 7 [54272/70488 (77%)]\tLoss: 0.935132\n",
      "994\n",
      "Train Epoch: 7 [54784/70488 (78%)]\tLoss: 0.939796\n",
      "995\n",
      "Train Epoch: 7 [55296/70488 (78%)]\tLoss: 0.937133\n",
      "996\n",
      "Train Epoch: 7 [55808/70488 (79%)]\tLoss: 0.935975\n",
      "997\n",
      "Train Epoch: 7 [56320/70488 (80%)]\tLoss: 0.934935\n",
      "998\n",
      "Train Epoch: 7 [56832/70488 (80%)]\tLoss: 0.934542\n",
      "999\n",
      "Train Epoch: 7 [57344/70488 (81%)]\tLoss: 0.937502\n",
      "1000\n",
      "Train Epoch: 7 [57856/70488 (82%)]\tLoss: 0.935866\n",
      "1001\n",
      "Train Epoch: 7 [58368/70488 (83%)]\tLoss: 0.935571\n",
      "1002\n",
      "Train Epoch: 7 [58880/70488 (83%)]\tLoss: 0.937476\n",
      "1003\n",
      "Train Epoch: 7 [59392/70488 (84%)]\tLoss: 0.940434\n",
      "1004\n",
      "Train Epoch: 7 [59904/70488 (85%)]\tLoss: 0.934845\n",
      "1005\n",
      "Train Epoch: 7 [60416/70488 (86%)]\tLoss: 0.936899\n",
      "1006\n",
      "Train Epoch: 7 [60928/70488 (86%)]\tLoss: 0.936125\n",
      "1007\n",
      "Train Epoch: 7 [61440/70488 (87%)]\tLoss: 0.935256\n",
      "1008\n",
      "Train Epoch: 7 [61952/70488 (88%)]\tLoss: 0.933486\n",
      "1009\n",
      "Train Epoch: 7 [62464/70488 (88%)]\tLoss: 0.936329\n",
      "1010\n",
      "Train Epoch: 7 [62976/70488 (89%)]\tLoss: 0.934897\n",
      "1011\n",
      "Train Epoch: 7 [63488/70488 (90%)]\tLoss: 0.935459\n",
      "1012\n",
      "Train Epoch: 7 [64000/70488 (91%)]\tLoss: 0.938304\n",
      "1013\n",
      "Train Epoch: 7 [64512/70488 (91%)]\tLoss: 0.937294\n",
      "1014\n",
      "Train Epoch: 7 [65024/70488 (92%)]\tLoss: 0.937145\n",
      "1015\n",
      "Train Epoch: 7 [65536/70488 (93%)]\tLoss: 0.939434\n",
      "1016\n",
      "Train Epoch: 7 [66048/70488 (93%)]\tLoss: 0.937023\n",
      "1017\n",
      "Train Epoch: 7 [66560/70488 (94%)]\tLoss: 0.934328\n",
      "1018\n",
      "Train Epoch: 7 [67072/70488 (95%)]\tLoss: 0.936812\n",
      "1019\n",
      "Train Epoch: 7 [67584/70488 (96%)]\tLoss: 0.937299\n",
      "1020\n",
      "Train Epoch: 7 [68096/70488 (96%)]\tLoss: 0.935786\n",
      "1021\n",
      "Train Epoch: 7 [68608/70488 (97%)]\tLoss: 0.935738\n",
      "1022\n",
      "Train Epoch: 7 [69120/70488 (98%)]\tLoss: 0.938722\n",
      "1023\n",
      "Train Epoch: 7 [69632/70488 (99%)]\tLoss: 0.931654\n",
      "1024\n",
      "Train Epoch: 7 [47128/70488 (99%)]\tLoss: 0.936714\n",
      "1025\n",
      "====> Epoch: 7 Average loss: 0.00367272\n",
      "====> Test set loss: 30.32185650\n",
      "Train Epoch: 8 [0/70488 (0%)]\tLoss: 0.935552\n",
      "1036\n",
      "Train Epoch: 8 [512/70488 (1%)]\tLoss: 0.935087\n",
      "1037\n",
      "Train Epoch: 8 [1024/70488 (1%)]\tLoss: 0.933175\n",
      "1038\n",
      "Train Epoch: 8 [1536/70488 (2%)]\tLoss: 0.937926\n",
      "1039\n",
      "Train Epoch: 8 [2048/70488 (3%)]\tLoss: 0.932938\n",
      "1040\n",
      "Train Epoch: 8 [2560/70488 (4%)]\tLoss: 0.936812\n",
      "1041\n",
      "Train Epoch: 8 [3072/70488 (4%)]\tLoss: 0.933665\n",
      "1042\n",
      "Train Epoch: 8 [3584/70488 (5%)]\tLoss: 0.935177\n",
      "1043\n",
      "Train Epoch: 8 [4096/70488 (6%)]\tLoss: 0.933622\n",
      "1044\n",
      "Train Epoch: 8 [4608/70488 (7%)]\tLoss: 0.933505\n",
      "1045\n",
      "Train Epoch: 8 [5120/70488 (7%)]\tLoss: 0.935108\n",
      "1046\n",
      "Train Epoch: 8 [5632/70488 (8%)]\tLoss: 0.935487\n",
      "1047\n",
      "Train Epoch: 8 [6144/70488 (9%)]\tLoss: 0.932168\n",
      "1048\n",
      "Train Epoch: 8 [6656/70488 (9%)]\tLoss: 0.934531\n",
      "1049\n",
      "Train Epoch: 8 [7168/70488 (10%)]\tLoss: 0.933022\n",
      "1050\n",
      "Train Epoch: 8 [7680/70488 (11%)]\tLoss: 0.932500\n",
      "1051\n",
      "Train Epoch: 8 [8192/70488 (12%)]\tLoss: 0.935720\n",
      "1052\n",
      "Train Epoch: 8 [8704/70488 (12%)]\tLoss: 0.933859\n",
      "1053\n",
      "Train Epoch: 8 [9216/70488 (13%)]\tLoss: 0.935156\n",
      "1054\n",
      "Train Epoch: 8 [9728/70488 (14%)]\tLoss: 0.931511\n",
      "1055\n",
      "Train Epoch: 8 [10240/70488 (14%)]\tLoss: 0.934492\n",
      "1056\n",
      "Train Epoch: 8 [10752/70488 (15%)]\tLoss: 0.937254\n",
      "1057\n",
      "Train Epoch: 8 [11264/70488 (16%)]\tLoss: 0.932172\n",
      "1058\n",
      "Train Epoch: 8 [11776/70488 (17%)]\tLoss: 0.933996\n",
      "1059\n",
      "Train Epoch: 8 [12288/70488 (17%)]\tLoss: 0.932605\n",
      "1060\n",
      "Train Epoch: 8 [12800/70488 (18%)]\tLoss: 0.935527\n",
      "1061\n",
      "Train Epoch: 8 [13312/70488 (19%)]\tLoss: 0.935821\n",
      "1062\n",
      "Train Epoch: 8 [13824/70488 (20%)]\tLoss: 0.931183\n",
      "1063\n",
      "Train Epoch: 8 [14336/70488 (20%)]\tLoss: 0.934762\n",
      "1064\n",
      "Train Epoch: 8 [14848/70488 (21%)]\tLoss: 0.935093\n",
      "1065\n",
      "Train Epoch: 8 [15360/70488 (22%)]\tLoss: 0.933415\n",
      "1066\n",
      "Train Epoch: 8 [15872/70488 (22%)]\tLoss: 0.935107\n",
      "1067\n",
      "Train Epoch: 8 [16384/70488 (23%)]\tLoss: 0.932973\n",
      "1068\n",
      "Train Epoch: 8 [16896/70488 (24%)]\tLoss: 0.932737\n",
      "1069\n",
      "Train Epoch: 8 [17408/70488 (25%)]\tLoss: 0.934515\n",
      "1070\n",
      "Train Epoch: 8 [17920/70488 (25%)]\tLoss: 0.932623\n",
      "1071\n",
      "Train Epoch: 8 [18432/70488 (26%)]\tLoss: 0.931009\n",
      "1072\n",
      "Train Epoch: 8 [18944/70488 (27%)]\tLoss: 0.932479\n",
      "1073\n",
      "Train Epoch: 8 [19456/70488 (28%)]\tLoss: 0.936769\n",
      "1074\n",
      "Train Epoch: 8 [19968/70488 (28%)]\tLoss: 0.932490\n",
      "1075\n",
      "Train Epoch: 8 [20480/70488 (29%)]\tLoss: 0.932646\n",
      "1076\n",
      "Train Epoch: 8 [20992/70488 (30%)]\tLoss: 0.931721\n",
      "1077\n",
      "Train Epoch: 8 [21504/70488 (30%)]\tLoss: 0.936189\n",
      "1078\n",
      "Train Epoch: 8 [22016/70488 (31%)]\tLoss: 0.933841\n",
      "1079\n",
      "Train Epoch: 8 [22528/70488 (32%)]\tLoss: 0.934197\n",
      "1080\n",
      "Train Epoch: 8 [23040/70488 (33%)]\tLoss: 0.933276\n",
      "1081\n",
      "Train Epoch: 8 [23552/70488 (33%)]\tLoss: 0.933659\n",
      "1082\n",
      "Train Epoch: 8 [24064/70488 (34%)]\tLoss: 0.935333\n",
      "1083\n",
      "Train Epoch: 8 [24576/70488 (35%)]\tLoss: 0.934177\n",
      "1084\n",
      "Train Epoch: 8 [25088/70488 (36%)]\tLoss: 0.934128\n",
      "1085\n",
      "Train Epoch: 8 [25600/70488 (36%)]\tLoss: 0.933396\n",
      "1086\n",
      "Train Epoch: 8 [26112/70488 (37%)]\tLoss: 0.931481\n",
      "1087\n",
      "Train Epoch: 8 [26624/70488 (38%)]\tLoss: 0.931006\n",
      "1088\n",
      "Train Epoch: 8 [27136/70488 (38%)]\tLoss: 0.933836\n",
      "1089\n",
      "Train Epoch: 8 [27648/70488 (39%)]\tLoss: 0.933793\n",
      "1090\n",
      "Train Epoch: 8 [28160/70488 (40%)]\tLoss: 0.932183\n",
      "1091\n",
      "Train Epoch: 8 [28672/70488 (41%)]\tLoss: 0.935908\n",
      "1092\n",
      "Train Epoch: 8 [29184/70488 (41%)]\tLoss: 0.933714\n",
      "1093\n",
      "Train Epoch: 8 [29696/70488 (42%)]\tLoss: 0.932444\n",
      "1094\n",
      "Train Epoch: 8 [30208/70488 (43%)]\tLoss: 0.933350\n",
      "1095\n",
      "Train Epoch: 8 [30720/70488 (43%)]\tLoss: 0.933318\n",
      "1096\n",
      "Train Epoch: 8 [31232/70488 (44%)]\tLoss: 0.930437\n",
      "1097\n",
      "Train Epoch: 8 [31744/70488 (45%)]\tLoss: 0.934238\n",
      "1098\n",
      "Train Epoch: 8 [32256/70488 (46%)]\tLoss: 0.935358\n",
      "1099\n",
      "Train Epoch: 8 [32768/70488 (46%)]\tLoss: 0.936099\n",
      "1100\n",
      "Train Epoch: 8 [33280/70488 (47%)]\tLoss: 0.933740\n",
      "1101\n",
      "Train Epoch: 8 [33792/70488 (48%)]\tLoss: 0.933036\n",
      "1102\n",
      "Train Epoch: 8 [34304/70488 (49%)]\tLoss: 0.933891\n",
      "1103\n",
      "Train Epoch: 8 [34816/70488 (49%)]\tLoss: 0.933154\n",
      "1104\n",
      "Train Epoch: 8 [35328/70488 (50%)]\tLoss: 0.936708\n",
      "1105\n",
      "Train Epoch: 8 [35840/70488 (51%)]\tLoss: 0.933922\n",
      "1106\n",
      "Train Epoch: 8 [36352/70488 (51%)]\tLoss: 0.933543\n",
      "1107\n",
      "Train Epoch: 8 [36864/70488 (52%)]\tLoss: 0.934036\n",
      "1108\n",
      "Train Epoch: 8 [37376/70488 (53%)]\tLoss: 0.935458\n",
      "1109\n",
      "Train Epoch: 8 [37888/70488 (54%)]\tLoss: 0.932480\n",
      "1110\n",
      "Train Epoch: 8 [38400/70488 (54%)]\tLoss: 0.933807\n",
      "1111\n",
      "Train Epoch: 8 [38912/70488 (55%)]\tLoss: 0.933151\n",
      "1112\n",
      "Train Epoch: 8 [39424/70488 (56%)]\tLoss: 0.932932\n",
      "1113\n",
      "Train Epoch: 8 [39936/70488 (57%)]\tLoss: 0.933534\n",
      "1114\n",
      "Train Epoch: 8 [40448/70488 (57%)]\tLoss: 0.933322\n",
      "1115\n",
      "Train Epoch: 8 [40960/70488 (58%)]\tLoss: 0.933119\n",
      "1116\n",
      "Train Epoch: 8 [41472/70488 (59%)]\tLoss: 0.932701\n",
      "1117\n",
      "Train Epoch: 8 [41984/70488 (59%)]\tLoss: 0.932282\n",
      "1118\n",
      "Train Epoch: 8 [42496/70488 (60%)]\tLoss: 0.929495\n",
      "1119\n",
      "Train Epoch: 8 [43008/70488 (61%)]\tLoss: 0.933703\n",
      "1120\n",
      "Train Epoch: 8 [43520/70488 (62%)]\tLoss: 0.932507\n",
      "1121\n",
      "Train Epoch: 8 [44032/70488 (62%)]\tLoss: 0.931128\n",
      "1122\n",
      "Train Epoch: 8 [44544/70488 (63%)]\tLoss: 0.932147\n",
      "1123\n",
      "Train Epoch: 8 [45056/70488 (64%)]\tLoss: 0.932595\n",
      "1124\n",
      "Train Epoch: 8 [45568/70488 (64%)]\tLoss: 0.933366\n",
      "1125\n",
      "Train Epoch: 8 [46080/70488 (65%)]\tLoss: 0.934043\n",
      "1126\n",
      "Train Epoch: 8 [46592/70488 (66%)]\tLoss: 0.938950\n",
      "1127\n",
      "Train Epoch: 8 [47104/70488 (67%)]\tLoss: 0.934770\n",
      "1128\n",
      "Train Epoch: 8 [47616/70488 (67%)]\tLoss: 0.934612\n",
      "1129\n",
      "Train Epoch: 8 [48128/70488 (68%)]\tLoss: 0.934229\n",
      "1130\n",
      "Train Epoch: 8 [48640/70488 (69%)]\tLoss: 0.933749\n",
      "1131\n",
      "Train Epoch: 8 [49152/70488 (70%)]\tLoss: 0.936217\n",
      "1132\n",
      "Train Epoch: 8 [49664/70488 (70%)]\tLoss: 0.933177\n",
      "1133\n",
      "Train Epoch: 8 [50176/70488 (71%)]\tLoss: 0.929658\n",
      "1134\n",
      "Train Epoch: 8 [50688/70488 (72%)]\tLoss: 0.931299\n",
      "1135\n",
      "Train Epoch: 8 [51200/70488 (72%)]\tLoss: 0.933096\n",
      "1136\n",
      "Train Epoch: 8 [51712/70488 (73%)]\tLoss: 0.932923\n",
      "1137\n",
      "Train Epoch: 8 [52224/70488 (74%)]\tLoss: 0.931094\n",
      "1138\n",
      "Train Epoch: 8 [52736/70488 (75%)]\tLoss: 0.932253\n",
      "1139\n",
      "Train Epoch: 8 [53248/70488 (75%)]\tLoss: 0.932603\n",
      "1140\n",
      "Train Epoch: 8 [53760/70488 (76%)]\tLoss: 0.937583\n",
      "1141\n",
      "Train Epoch: 8 [54272/70488 (77%)]\tLoss: 0.931429\n",
      "1142\n",
      "Train Epoch: 8 [54784/70488 (78%)]\tLoss: 0.932898\n",
      "1143\n",
      "Train Epoch: 8 [55296/70488 (78%)]\tLoss: 0.933806\n",
      "1144\n",
      "Train Epoch: 8 [55808/70488 (79%)]\tLoss: 0.930391\n",
      "1145\n",
      "Train Epoch: 8 [56320/70488 (80%)]\tLoss: 0.932775\n",
      "1146\n",
      "Train Epoch: 8 [56832/70488 (80%)]\tLoss: 0.929345\n",
      "1147\n",
      "Train Epoch: 8 [57344/70488 (81%)]\tLoss: 0.931857\n",
      "1148\n",
      "Train Epoch: 8 [57856/70488 (82%)]\tLoss: 0.933338\n",
      "1149\n",
      "Train Epoch: 8 [58368/70488 (83%)]\tLoss: 0.932585\n",
      "1150\n",
      "Train Epoch: 8 [58880/70488 (83%)]\tLoss: 0.932835\n",
      "1151\n",
      "Train Epoch: 8 [59392/70488 (84%)]\tLoss: 0.928985\n",
      "1152\n",
      "Train Epoch: 8 [59904/70488 (85%)]\tLoss: 0.931118\n",
      "1153\n",
      "Train Epoch: 8 [60416/70488 (86%)]\tLoss: 0.933374\n",
      "1154\n",
      "Train Epoch: 8 [60928/70488 (86%)]\tLoss: 0.930619\n",
      "1155\n",
      "Train Epoch: 8 [61440/70488 (87%)]\tLoss: 0.930850\n",
      "1156\n",
      "Train Epoch: 8 [61952/70488 (88%)]\tLoss: 0.932224\n",
      "1157\n",
      "Train Epoch: 8 [62464/70488 (88%)]\tLoss: 0.937453\n",
      "1158\n",
      "Train Epoch: 8 [62976/70488 (89%)]\tLoss: 0.930632\n",
      "1159\n",
      "Train Epoch: 8 [63488/70488 (90%)]\tLoss: 0.931102\n",
      "1160\n",
      "Train Epoch: 8 [64000/70488 (91%)]\tLoss: 0.932083\n",
      "1161\n",
      "Train Epoch: 8 [64512/70488 (91%)]\tLoss: 0.930231\n",
      "1162\n",
      "Train Epoch: 8 [65024/70488 (92%)]\tLoss: 0.933294\n",
      "1163\n",
      "Train Epoch: 8 [65536/70488 (93%)]\tLoss: 0.932488\n",
      "1164\n",
      "Train Epoch: 8 [66048/70488 (93%)]\tLoss: 0.933733\n",
      "1165\n",
      "Train Epoch: 8 [66560/70488 (94%)]\tLoss: 0.940796\n",
      "1166\n",
      "Train Epoch: 8 [67072/70488 (95%)]\tLoss: 0.934076\n",
      "1167\n",
      "Train Epoch: 8 [67584/70488 (96%)]\tLoss: 0.930520\n",
      "1168\n",
      "Train Epoch: 8 [68096/70488 (96%)]\tLoss: 0.931780\n",
      "1169\n",
      "Train Epoch: 8 [68608/70488 (97%)]\tLoss: 0.933041\n",
      "1170\n",
      "Train Epoch: 8 [69120/70488 (98%)]\tLoss: 0.932214\n",
      "1171\n",
      "Train Epoch: 8 [69632/70488 (99%)]\tLoss: 0.930821\n",
      "1172\n",
      "Train Epoch: 8 [47128/70488 (99%)]\tLoss: 0.935401\n",
      "1173\n",
      "====> Epoch: 8 Average loss: 0.00365485\n",
      "====> Test set loss: 30.30041742\n",
      "Train Epoch: 9 [0/70488 (0%)]\tLoss: 0.931391\n",
      "1184\n",
      "Train Epoch: 9 [512/70488 (1%)]\tLoss: 0.927628\n",
      "1185\n",
      "Train Epoch: 9 [1024/70488 (1%)]\tLoss: 0.932662\n",
      "1186\n",
      "Train Epoch: 9 [1536/70488 (2%)]\tLoss: 0.930993\n",
      "1187\n",
      "Train Epoch: 9 [2048/70488 (3%)]\tLoss: 0.930813\n",
      "1188\n",
      "Train Epoch: 9 [2560/70488 (4%)]\tLoss: 0.929411\n",
      "1189\n",
      "Train Epoch: 9 [3072/70488 (4%)]\tLoss: 0.931144\n",
      "1190\n",
      "Train Epoch: 9 [3584/70488 (5%)]\tLoss: 0.930619\n",
      "1191\n",
      "Train Epoch: 9 [4096/70488 (6%)]\tLoss: 0.931470\n",
      "1192\n",
      "Train Epoch: 9 [4608/70488 (7%)]\tLoss: 0.931066\n",
      "1193\n",
      "Train Epoch: 9 [5120/70488 (7%)]\tLoss: 0.933945\n",
      "1194\n",
      "Train Epoch: 9 [5632/70488 (8%)]\tLoss: 0.930920\n",
      "1195\n",
      "Train Epoch: 9 [6144/70488 (9%)]\tLoss: 0.931213\n",
      "1196\n",
      "Train Epoch: 9 [6656/70488 (9%)]\tLoss: 0.929159\n",
      "1197\n",
      "Train Epoch: 9 [7168/70488 (10%)]\tLoss: 0.929374\n",
      "1198\n",
      "Train Epoch: 9 [7680/70488 (11%)]\tLoss: 0.930225\n",
      "1199\n",
      "Train Epoch: 9 [8192/70488 (12%)]\tLoss: 0.930459\n",
      "1200\n",
      "Train Epoch: 9 [8704/70488 (12%)]\tLoss: 0.932768\n",
      "1201\n",
      "Train Epoch: 9 [9216/70488 (13%)]\tLoss: 0.930393\n",
      "1202\n",
      "Train Epoch: 9 [9728/70488 (14%)]\tLoss: 0.932419\n",
      "1203\n",
      "Train Epoch: 9 [10240/70488 (14%)]\tLoss: 0.931637\n",
      "1204\n",
      "Train Epoch: 9 [10752/70488 (15%)]\tLoss: 0.930063\n",
      "1205\n",
      "Train Epoch: 9 [11264/70488 (16%)]\tLoss: 0.928472\n",
      "1206\n",
      "Train Epoch: 9 [11776/70488 (17%)]\tLoss: 0.929190\n",
      "1207\n",
      "Train Epoch: 9 [12288/70488 (17%)]\tLoss: 0.928484\n",
      "1208\n",
      "Train Epoch: 9 [12800/70488 (18%)]\tLoss: 0.929179\n",
      "1209\n",
      "Train Epoch: 9 [13312/70488 (19%)]\tLoss: 0.927523\n",
      "1210\n",
      "Train Epoch: 9 [13824/70488 (20%)]\tLoss: 0.929465\n",
      "1211\n",
      "Train Epoch: 9 [14336/70488 (20%)]\tLoss: 0.932226\n",
      "1212\n",
      "Train Epoch: 9 [14848/70488 (21%)]\tLoss: 0.929910\n",
      "1213\n",
      "Train Epoch: 9 [15360/70488 (22%)]\tLoss: 0.928453\n",
      "1214\n",
      "Train Epoch: 9 [15872/70488 (22%)]\tLoss: 0.926773\n",
      "1215\n",
      "Train Epoch: 9 [16384/70488 (23%)]\tLoss: 0.929833\n",
      "1216\n",
      "Train Epoch: 9 [16896/70488 (24%)]\tLoss: 0.930788\n",
      "1217\n",
      "Train Epoch: 9 [17408/70488 (25%)]\tLoss: 0.925741\n",
      "1218\n",
      "Train Epoch: 9 [17920/70488 (25%)]\tLoss: 0.930209\n",
      "1219\n",
      "Train Epoch: 9 [18432/70488 (26%)]\tLoss: 0.933098\n",
      "1220\n",
      "Train Epoch: 9 [18944/70488 (27%)]\tLoss: 0.929695\n",
      "1221\n",
      "Train Epoch: 9 [19456/70488 (28%)]\tLoss: 0.930121\n",
      "1222\n",
      "Train Epoch: 9 [19968/70488 (28%)]\tLoss: 0.931202\n",
      "1223\n",
      "Train Epoch: 9 [20480/70488 (29%)]\tLoss: 0.929715\n",
      "1224\n",
      "Train Epoch: 9 [20992/70488 (30%)]\tLoss: 0.932549\n",
      "1225\n",
      "Train Epoch: 9 [21504/70488 (30%)]\tLoss: 0.928600\n",
      "1226\n",
      "Train Epoch: 9 [22016/70488 (31%)]\tLoss: 0.928139\n",
      "1227\n",
      "Train Epoch: 9 [22528/70488 (32%)]\tLoss: 0.930659\n",
      "1228\n",
      "Train Epoch: 9 [23040/70488 (33%)]\tLoss: 0.930202\n",
      "1229\n",
      "Train Epoch: 9 [23552/70488 (33%)]\tLoss: 0.932220\n",
      "1230\n",
      "Train Epoch: 9 [24064/70488 (34%)]\tLoss: 0.931424\n",
      "1231\n",
      "Train Epoch: 9 [24576/70488 (35%)]\tLoss: 0.928751\n",
      "1232\n",
      "Train Epoch: 9 [25088/70488 (36%)]\tLoss: 0.932735\n",
      "1233\n",
      "Train Epoch: 9 [25600/70488 (36%)]\tLoss: 0.931270\n",
      "1234\n",
      "Train Epoch: 9 [26112/70488 (37%)]\tLoss: 0.926907\n",
      "1235\n",
      "Train Epoch: 9 [26624/70488 (38%)]\tLoss: 0.931075\n",
      "1236\n",
      "Train Epoch: 9 [27136/70488 (38%)]\tLoss: 0.932613\n",
      "1237\n",
      "Train Epoch: 9 [27648/70488 (39%)]\tLoss: 0.931318\n",
      "1238\n",
      "Train Epoch: 9 [28160/70488 (40%)]\tLoss: 0.928011\n",
      "1239\n",
      "Train Epoch: 9 [28672/70488 (41%)]\tLoss: 0.934520\n",
      "1240\n",
      "Train Epoch: 9 [29184/70488 (41%)]\tLoss: 0.932053\n",
      "1241\n",
      "Train Epoch: 9 [29696/70488 (42%)]\tLoss: 0.930454\n",
      "1242\n",
      "Train Epoch: 9 [30208/70488 (43%)]\tLoss: 0.931382\n",
      "1243\n",
      "Train Epoch: 9 [30720/70488 (43%)]\tLoss: 0.927841\n",
      "1244\n",
      "Train Epoch: 9 [31232/70488 (44%)]\tLoss: 0.930984\n",
      "1245\n",
      "Train Epoch: 9 [31744/70488 (45%)]\tLoss: 0.929287\n",
      "1246\n",
      "Train Epoch: 9 [32256/70488 (46%)]\tLoss: 0.928126\n",
      "1247\n",
      "Train Epoch: 9 [32768/70488 (46%)]\tLoss: 0.930900\n",
      "1248\n",
      "Train Epoch: 9 [33280/70488 (47%)]\tLoss: 0.930947\n",
      "1249\n",
      "Train Epoch: 9 [33792/70488 (48%)]\tLoss: 0.930573\n",
      "1250\n",
      "Train Epoch: 9 [34304/70488 (49%)]\tLoss: 0.929768\n",
      "1251\n",
      "Train Epoch: 9 [34816/70488 (49%)]\tLoss: 0.930445\n",
      "1252\n",
      "Train Epoch: 9 [35328/70488 (50%)]\tLoss: 0.929211\n",
      "1253\n",
      "Train Epoch: 9 [35840/70488 (51%)]\tLoss: 0.929304\n",
      "1254\n",
      "Train Epoch: 9 [36352/70488 (51%)]\tLoss: 0.930177\n",
      "1255\n",
      "Train Epoch: 9 [36864/70488 (52%)]\tLoss: 0.929231\n",
      "1256\n",
      "Train Epoch: 9 [37376/70488 (53%)]\tLoss: 0.933822\n",
      "1257\n",
      "Train Epoch: 9 [37888/70488 (54%)]\tLoss: 0.931710\n",
      "1258\n",
      "Train Epoch: 9 [38400/70488 (54%)]\tLoss: 0.927488\n",
      "1259\n",
      "Train Epoch: 9 [38912/70488 (55%)]\tLoss: 0.930515\n",
      "1260\n",
      "Train Epoch: 9 [39424/70488 (56%)]\tLoss: 0.928579\n",
      "1261\n",
      "Train Epoch: 9 [39936/70488 (57%)]\tLoss: 0.930330\n",
      "1262\n",
      "Train Epoch: 9 [40448/70488 (57%)]\tLoss: 0.931102\n",
      "1263\n",
      "Train Epoch: 9 [40960/70488 (58%)]\tLoss: 0.929569\n",
      "1264\n",
      "Train Epoch: 9 [41472/70488 (59%)]\tLoss: 0.928725\n",
      "1265\n",
      "Train Epoch: 9 [41984/70488 (59%)]\tLoss: 0.928905\n",
      "1266\n",
      "Train Epoch: 9 [42496/70488 (60%)]\tLoss: 0.933699\n",
      "1267\n",
      "Train Epoch: 9 [43008/70488 (61%)]\tLoss: 0.931126\n",
      "1268\n",
      "Train Epoch: 9 [43520/70488 (62%)]\tLoss: 0.929834\n",
      "1269\n",
      "Train Epoch: 9 [44032/70488 (62%)]\tLoss: 0.929859\n",
      "1270\n",
      "Train Epoch: 9 [44544/70488 (63%)]\tLoss: 0.933598\n",
      "1271\n",
      "Train Epoch: 9 [45056/70488 (64%)]\tLoss: 0.925242\n",
      "1272\n",
      "Train Epoch: 9 [45568/70488 (64%)]\tLoss: 0.928282\n",
      "1273\n",
      "Train Epoch: 9 [46080/70488 (65%)]\tLoss: 0.929284\n",
      "1274\n",
      "Train Epoch: 9 [46592/70488 (66%)]\tLoss: 0.927174\n",
      "1275\n",
      "Train Epoch: 9 [47104/70488 (67%)]\tLoss: 0.930454\n",
      "1276\n",
      "Train Epoch: 9 [47616/70488 (67%)]\tLoss: 0.934012\n",
      "1277\n",
      "Train Epoch: 9 [48128/70488 (68%)]\tLoss: 0.928640\n",
      "1278\n",
      "Train Epoch: 9 [48640/70488 (69%)]\tLoss: 0.926762\n",
      "1279\n",
      "Train Epoch: 9 [49152/70488 (70%)]\tLoss: 0.930976\n",
      "1280\n",
      "Train Epoch: 9 [49664/70488 (70%)]\tLoss: 0.929311\n",
      "1281\n",
      "Train Epoch: 9 [50176/70488 (71%)]\tLoss: 0.930771\n",
      "1282\n",
      "Train Epoch: 9 [50688/70488 (72%)]\tLoss: 0.931472\n",
      "1283\n",
      "Train Epoch: 9 [51200/70488 (72%)]\tLoss: 0.929741\n",
      "1284\n",
      "Train Epoch: 9 [51712/70488 (73%)]\tLoss: 0.934193\n",
      "1285\n",
      "Train Epoch: 9 [52224/70488 (74%)]\tLoss: 0.928206\n",
      "1286\n",
      "Train Epoch: 9 [52736/70488 (75%)]\tLoss: 0.929553\n",
      "1287\n",
      "Train Epoch: 9 [53248/70488 (75%)]\tLoss: 0.929257\n",
      "1288\n",
      "Train Epoch: 9 [53760/70488 (76%)]\tLoss: 0.928631\n",
      "1289\n",
      "Train Epoch: 9 [54272/70488 (77%)]\tLoss: 0.930457\n",
      "1290\n",
      "Train Epoch: 9 [54784/70488 (78%)]\tLoss: 0.928581\n",
      "1291\n",
      "Train Epoch: 9 [55296/70488 (78%)]\tLoss: 0.934490\n",
      "1292\n",
      "Train Epoch: 9 [55808/70488 (79%)]\tLoss: 0.931274\n",
      "1293\n",
      "Train Epoch: 9 [56320/70488 (80%)]\tLoss: 0.930637\n",
      "1294\n",
      "Train Epoch: 9 [56832/70488 (80%)]\tLoss: 0.929491\n",
      "1295\n",
      "Train Epoch: 9 [57344/70488 (81%)]\tLoss: 0.931593\n",
      "1296\n",
      "Train Epoch: 9 [57856/70488 (82%)]\tLoss: 0.928123\n",
      "1297\n",
      "Train Epoch: 9 [58368/70488 (83%)]\tLoss: 0.928106\n",
      "1298\n",
      "Train Epoch: 9 [58880/70488 (83%)]\tLoss: 0.928455\n",
      "1299\n",
      "Train Epoch: 9 [59392/70488 (84%)]\tLoss: 0.930405\n",
      "1300\n",
      "Train Epoch: 9 [59904/70488 (85%)]\tLoss: 0.929503\n",
      "1301\n",
      "Train Epoch: 9 [60416/70488 (86%)]\tLoss: 0.930493\n",
      "1302\n",
      "Train Epoch: 9 [60928/70488 (86%)]\tLoss: 0.930174\n",
      "1303\n",
      "Train Epoch: 9 [61440/70488 (87%)]\tLoss: 0.927527\n",
      "1304\n",
      "Train Epoch: 9 [61952/70488 (88%)]\tLoss: 0.930064\n",
      "1305\n",
      "Train Epoch: 9 [62464/70488 (88%)]\tLoss: 0.933830\n",
      "1306\n",
      "Train Epoch: 9 [62976/70488 (89%)]\tLoss: 0.929497\n",
      "1307\n",
      "Train Epoch: 9 [63488/70488 (90%)]\tLoss: 0.928131\n",
      "1308\n",
      "Train Epoch: 9 [64000/70488 (91%)]\tLoss: 0.928539\n",
      "1309\n",
      "Train Epoch: 9 [64512/70488 (91%)]\tLoss: 0.931486\n",
      "1310\n",
      "Train Epoch: 9 [65024/70488 (92%)]\tLoss: 0.928886\n",
      "1311\n",
      "Train Epoch: 9 [65536/70488 (93%)]\tLoss: 0.928737\n",
      "1312\n",
      "Train Epoch: 9 [66048/70488 (93%)]\tLoss: 0.936498\n",
      "1313\n",
      "Train Epoch: 9 [66560/70488 (94%)]\tLoss: 0.931144\n",
      "1314\n",
      "Train Epoch: 9 [67072/70488 (95%)]\tLoss: 0.928579\n",
      "1315\n",
      "Train Epoch: 9 [67584/70488 (96%)]\tLoss: 0.929710\n",
      "1316\n",
      "Train Epoch: 9 [68096/70488 (96%)]\tLoss: 0.928403\n",
      "1317\n",
      "Train Epoch: 9 [68608/70488 (97%)]\tLoss: 0.930169\n",
      "1318\n",
      "Train Epoch: 9 [69120/70488 (98%)]\tLoss: 0.927293\n",
      "1319\n",
      "Train Epoch: 9 [69632/70488 (99%)]\tLoss: 0.930101\n",
      "1320\n",
      "Train Epoch: 9 [47128/70488 (99%)]\tLoss: 0.930919\n",
      "1321\n",
      "====> Epoch: 9 Average loss: 0.00364214\n",
      "====> Test set loss: 30.32377267\n",
      "Train Epoch: 10 [0/70488 (0%)]\tLoss: 0.929617\n",
      "1332\n",
      "Train Epoch: 10 [512/70488 (1%)]\tLoss: 0.925440\n",
      "1333\n",
      "Train Epoch: 10 [1024/70488 (1%)]\tLoss: 0.930303\n",
      "1334\n",
      "Train Epoch: 10 [1536/70488 (2%)]\tLoss: 0.926526\n",
      "1335\n",
      "Train Epoch: 10 [2048/70488 (3%)]\tLoss: 0.931176\n",
      "1336\n",
      "Train Epoch: 10 [2560/70488 (4%)]\tLoss: 0.928127\n",
      "1337\n",
      "Train Epoch: 10 [3072/70488 (4%)]\tLoss: 0.928381\n",
      "1338\n",
      "Train Epoch: 10 [3584/70488 (5%)]\tLoss: 0.931197\n",
      "1339\n",
      "Train Epoch: 10 [4096/70488 (6%)]\tLoss: 0.927021\n",
      "1340\n",
      "Train Epoch: 10 [4608/70488 (7%)]\tLoss: 0.928501\n",
      "1341\n",
      "Train Epoch: 10 [5120/70488 (7%)]\tLoss: 0.927862\n",
      "1342\n",
      "Train Epoch: 10 [5632/70488 (8%)]\tLoss: 0.925992\n",
      "1343\n",
      "Train Epoch: 10 [6144/70488 (9%)]\tLoss: 0.929822\n",
      "1344\n",
      "Train Epoch: 10 [6656/70488 (9%)]\tLoss: 0.927804\n",
      "1345\n",
      "Train Epoch: 10 [7168/70488 (10%)]\tLoss: 0.928242\n",
      "1346\n",
      "Train Epoch: 10 [7680/70488 (11%)]\tLoss: 0.928708\n",
      "1347\n",
      "Train Epoch: 10 [8192/70488 (12%)]\tLoss: 0.926145\n",
      "1348\n",
      "Train Epoch: 10 [8704/70488 (12%)]\tLoss: 0.926606\n",
      "1349\n",
      "Train Epoch: 10 [9216/70488 (13%)]\tLoss: 0.927988\n",
      "1350\n",
      "Train Epoch: 10 [9728/70488 (14%)]\tLoss: 0.928701\n",
      "1351\n",
      "Train Epoch: 10 [10240/70488 (14%)]\tLoss: 0.930340\n",
      "1352\n",
      "Train Epoch: 10 [10752/70488 (15%)]\tLoss: 0.926838\n",
      "1353\n",
      "Train Epoch: 10 [11264/70488 (16%)]\tLoss: 0.932046\n",
      "1354\n",
      "Train Epoch: 10 [11776/70488 (17%)]\tLoss: 0.928137\n",
      "1355\n",
      "Train Epoch: 10 [12288/70488 (17%)]\tLoss: 0.927274\n",
      "1356\n",
      "Train Epoch: 10 [12800/70488 (18%)]\tLoss: 0.926546\n",
      "1357\n",
      "Train Epoch: 10 [13312/70488 (19%)]\tLoss: 0.929233\n",
      "1358\n",
      "Train Epoch: 10 [13824/70488 (20%)]\tLoss: 0.928196\n",
      "1359\n",
      "Train Epoch: 10 [14336/70488 (20%)]\tLoss: 0.926881\n",
      "1360\n",
      "Train Epoch: 10 [14848/70488 (21%)]\tLoss: 0.924800\n",
      "1361\n",
      "Train Epoch: 10 [15360/70488 (22%)]\tLoss: 0.926932\n",
      "1362\n",
      "Train Epoch: 10 [15872/70488 (22%)]\tLoss: 0.929994\n",
      "1363\n",
      "Train Epoch: 10 [16384/70488 (23%)]\tLoss: 0.928211\n",
      "1364\n",
      "Train Epoch: 10 [16896/70488 (24%)]\tLoss: 0.929228\n",
      "1365\n",
      "Train Epoch: 10 [17408/70488 (25%)]\tLoss: 0.926014\n",
      "1366\n",
      "Train Epoch: 10 [17920/70488 (25%)]\tLoss: 0.926463\n",
      "1367\n",
      "Train Epoch: 10 [18432/70488 (26%)]\tLoss: 0.926693\n",
      "1368\n",
      "Train Epoch: 10 [18944/70488 (27%)]\tLoss: 0.928600\n",
      "1369\n",
      "Train Epoch: 10 [19456/70488 (28%)]\tLoss: 0.928397\n",
      "1370\n",
      "Train Epoch: 10 [19968/70488 (28%)]\tLoss: 0.927244\n",
      "1371\n",
      "Train Epoch: 10 [20480/70488 (29%)]\tLoss: 0.927965\n",
      "1372\n",
      "Train Epoch: 10 [20992/70488 (30%)]\tLoss: 0.928569\n",
      "1373\n",
      "Train Epoch: 10 [21504/70488 (30%)]\tLoss: 0.925547\n",
      "1374\n",
      "Train Epoch: 10 [22016/70488 (31%)]\tLoss: 0.926705\n",
      "1375\n",
      "Train Epoch: 10 [22528/70488 (32%)]\tLoss: 0.927349\n",
      "1376\n",
      "Train Epoch: 10 [23040/70488 (33%)]\tLoss: 0.929736\n",
      "1377\n",
      "Train Epoch: 10 [23552/70488 (33%)]\tLoss: 0.926191\n",
      "1378\n",
      "Train Epoch: 10 [24064/70488 (34%)]\tLoss: 0.928028\n",
      "1379\n",
      "Train Epoch: 10 [24576/70488 (35%)]\tLoss: 0.927388\n",
      "1380\n",
      "Train Epoch: 10 [25088/70488 (36%)]\tLoss: 0.929174\n",
      "1381\n",
      "Train Epoch: 10 [25600/70488 (36%)]\tLoss: 0.928049\n",
      "1382\n",
      "Train Epoch: 10 [26112/70488 (37%)]\tLoss: 0.923979\n",
      "1383\n",
      "Train Epoch: 10 [26624/70488 (38%)]\tLoss: 0.931135\n",
      "1384\n",
      "Train Epoch: 10 [27136/70488 (38%)]\tLoss: 0.926427\n",
      "1385\n",
      "Train Epoch: 10 [27648/70488 (39%)]\tLoss: 0.925889\n",
      "1386\n",
      "Train Epoch: 10 [28160/70488 (40%)]\tLoss: 0.926127\n",
      "1387\n",
      "Train Epoch: 10 [28672/70488 (41%)]\tLoss: 0.927912\n",
      "1388\n",
      "Train Epoch: 10 [29184/70488 (41%)]\tLoss: 0.926009\n",
      "1389\n",
      "Train Epoch: 10 [29696/70488 (42%)]\tLoss: 0.927323\n",
      "1390\n",
      "Train Epoch: 10 [30208/70488 (43%)]\tLoss: 0.929376\n",
      "1391\n",
      "Train Epoch: 10 [30720/70488 (43%)]\tLoss: 0.928483\n",
      "1392\n",
      "Train Epoch: 10 [31232/70488 (44%)]\tLoss: 0.928695\n",
      "1393\n",
      "Train Epoch: 10 [31744/70488 (45%)]\tLoss: 0.926997\n",
      "1394\n",
      "Train Epoch: 10 [32256/70488 (46%)]\tLoss: 0.928459\n",
      "1395\n",
      "Train Epoch: 10 [32768/70488 (46%)]\tLoss: 0.925750\n",
      "1396\n",
      "Train Epoch: 10 [33280/70488 (47%)]\tLoss: 0.926138\n",
      "1397\n",
      "Train Epoch: 10 [33792/70488 (48%)]\tLoss: 0.925564\n",
      "1398\n",
      "Train Epoch: 10 [34304/70488 (49%)]\tLoss: 0.927317\n",
      "1399\n",
      "Train Epoch: 10 [34816/70488 (49%)]\tLoss: 0.928258\n",
      "1400\n",
      "Train Epoch: 10 [35328/70488 (50%)]\tLoss: 0.926036\n",
      "1401\n",
      "Train Epoch: 10 [35840/70488 (51%)]\tLoss: 0.931255\n",
      "1402\n",
      "Train Epoch: 10 [36352/70488 (51%)]\tLoss: 0.927111\n",
      "1403\n",
      "Train Epoch: 10 [36864/70488 (52%)]\tLoss: 0.926239\n",
      "1404\n",
      "Train Epoch: 10 [37376/70488 (53%)]\tLoss: 0.929050\n",
      "1405\n",
      "Train Epoch: 10 [37888/70488 (54%)]\tLoss: 0.927077\n",
      "1406\n",
      "Train Epoch: 10 [38400/70488 (54%)]\tLoss: 0.927973\n",
      "1407\n",
      "Train Epoch: 10 [38912/70488 (55%)]\tLoss: 0.926816\n",
      "1408\n",
      "Train Epoch: 10 [39424/70488 (56%)]\tLoss: 0.926733\n",
      "1409\n",
      "Train Epoch: 10 [39936/70488 (57%)]\tLoss: 0.928957\n",
      "1410\n",
      "Train Epoch: 10 [40448/70488 (57%)]\tLoss: 0.925168\n",
      "1411\n",
      "Train Epoch: 10 [40960/70488 (58%)]\tLoss: 0.928334\n",
      "1412\n",
      "Train Epoch: 10 [41472/70488 (59%)]\tLoss: 0.928327\n",
      "1413\n",
      "Train Epoch: 10 [41984/70488 (59%)]\tLoss: 0.927404\n",
      "1414\n",
      "Train Epoch: 10 [42496/70488 (60%)]\tLoss: 0.928460\n",
      "1415\n",
      "Train Epoch: 10 [43008/70488 (61%)]\tLoss: 0.928398\n",
      "1416\n",
      "Train Epoch: 10 [43520/70488 (62%)]\tLoss: 0.926833\n",
      "1417\n",
      "Train Epoch: 10 [44032/70488 (62%)]\tLoss: 0.926948\n",
      "1418\n",
      "Train Epoch: 10 [44544/70488 (63%)]\tLoss: 0.927460\n",
      "1419\n",
      "Train Epoch: 10 [45056/70488 (64%)]\tLoss: 0.929048\n",
      "1420\n",
      "Train Epoch: 10 [45568/70488 (64%)]\tLoss: 0.926770\n",
      "1421\n",
      "Train Epoch: 10 [46080/70488 (65%)]\tLoss: 0.931519\n",
      "1422\n",
      "Train Epoch: 10 [46592/70488 (66%)]\tLoss: 0.926428\n",
      "1423\n",
      "Train Epoch: 10 [47104/70488 (67%)]\tLoss: 0.928017\n",
      "1424\n",
      "Train Epoch: 10 [47616/70488 (67%)]\tLoss: 0.925993\n",
      "1425\n",
      "Train Epoch: 10 [48128/70488 (68%)]\tLoss: 0.927376\n",
      "1426\n",
      "Train Epoch: 10 [48640/70488 (69%)]\tLoss: 0.926507\n",
      "1427\n",
      "Train Epoch: 10 [49152/70488 (70%)]\tLoss: 0.926879\n",
      "1428\n",
      "Train Epoch: 10 [49664/70488 (70%)]\tLoss: 0.927299\n",
      "1429\n",
      "Train Epoch: 10 [50176/70488 (71%)]\tLoss: 0.926808\n",
      "1430\n",
      "Train Epoch: 10 [50688/70488 (72%)]\tLoss: 0.927514\n",
      "1431\n",
      "Train Epoch: 10 [51200/70488 (72%)]\tLoss: 0.931766\n",
      "1432\n",
      "Train Epoch: 10 [51712/70488 (73%)]\tLoss: 0.924117\n",
      "1433\n",
      "Train Epoch: 10 [52224/70488 (74%)]\tLoss: 0.926596\n",
      "1434\n",
      "Train Epoch: 10 [52736/70488 (75%)]\tLoss: 0.929531\n",
      "1435\n",
      "Train Epoch: 10 [53248/70488 (75%)]\tLoss: 0.926564\n",
      "1436\n",
      "Train Epoch: 10 [53760/70488 (76%)]\tLoss: 0.926619\n",
      "1437\n",
      "Train Epoch: 10 [54272/70488 (77%)]\tLoss: 0.927316\n",
      "1438\n",
      "Train Epoch: 10 [54784/70488 (78%)]\tLoss: 0.928288\n",
      "1439\n",
      "Train Epoch: 10 [55296/70488 (78%)]\tLoss: 0.926368\n",
      "1440\n",
      "Train Epoch: 10 [55808/70488 (79%)]\tLoss: 0.929525\n",
      "1441\n",
      "Train Epoch: 10 [56320/70488 (80%)]\tLoss: 0.927904\n",
      "1442\n",
      "Train Epoch: 10 [56832/70488 (80%)]\tLoss: 0.926671\n",
      "1443\n",
      "Train Epoch: 10 [57344/70488 (81%)]\tLoss: 0.928269\n",
      "1444\n",
      "Train Epoch: 10 [57856/70488 (82%)]\tLoss: 0.925049\n",
      "1445\n",
      "Train Epoch: 10 [58368/70488 (83%)]\tLoss: 0.931339\n",
      "1446\n",
      "Train Epoch: 10 [58880/70488 (83%)]\tLoss: 0.926878\n",
      "1447\n",
      "Train Epoch: 10 [59392/70488 (84%)]\tLoss: 0.929515\n",
      "1448\n",
      "Train Epoch: 10 [59904/70488 (85%)]\tLoss: 0.925271\n",
      "1449\n",
      "Train Epoch: 10 [60416/70488 (86%)]\tLoss: 0.925842\n",
      "1450\n",
      "Train Epoch: 10 [60928/70488 (86%)]\tLoss: 0.928971\n",
      "1451\n",
      "Train Epoch: 10 [61440/70488 (87%)]\tLoss: 0.928668\n",
      "1452\n",
      "Train Epoch: 10 [61952/70488 (88%)]\tLoss: 0.925575\n",
      "1453\n",
      "Train Epoch: 10 [62464/70488 (88%)]\tLoss: 0.928660\n",
      "1454\n",
      "Train Epoch: 10 [62976/70488 (89%)]\tLoss: 0.927579\n",
      "1455\n",
      "Train Epoch: 10 [63488/70488 (90%)]\tLoss: 0.925970\n",
      "1456\n",
      "Train Epoch: 10 [64000/70488 (91%)]\tLoss: 0.923869\n",
      "1457\n",
      "Train Epoch: 10 [64512/70488 (91%)]\tLoss: 0.927413\n",
      "1458\n",
      "Train Epoch: 10 [65024/70488 (92%)]\tLoss: 0.928974\n",
      "1459\n",
      "Train Epoch: 10 [65536/70488 (93%)]\tLoss: 0.926662\n",
      "1460\n",
      "Train Epoch: 10 [66048/70488 (93%)]\tLoss: 0.925825\n",
      "1461\n",
      "Train Epoch: 10 [66560/70488 (94%)]\tLoss: 0.925446\n",
      "1462\n",
      "Train Epoch: 10 [67072/70488 (95%)]\tLoss: 0.927865\n",
      "1463\n",
      "Train Epoch: 10 [67584/70488 (96%)]\tLoss: 0.929376\n",
      "1464\n",
      "Train Epoch: 10 [68096/70488 (96%)]\tLoss: 0.928073\n",
      "1465\n",
      "Train Epoch: 10 [68608/70488 (97%)]\tLoss: 0.927931\n",
      "1466\n",
      "Train Epoch: 10 [69120/70488 (98%)]\tLoss: 0.926306\n",
      "1467\n",
      "Train Epoch: 10 [69632/70488 (99%)]\tLoss: 0.924204\n",
      "1468\n",
      "Train Epoch: 10 [47128/70488 (99%)]\tLoss: 0.927997\n",
      "1469\n",
      "====> Epoch: 10 Average loss: 0.00363212\n",
      "====> Test set loss: 30.31151009\n",
      "Train Epoch: 11 [0/70488 (0%)]\tLoss: 0.928472\n",
      "1480\n",
      "Train Epoch: 11 [512/70488 (1%)]\tLoss: 0.926429\n",
      "1481\n",
      "Train Epoch: 11 [1024/70488 (1%)]\tLoss: 0.924797\n",
      "1482\n",
      "Train Epoch: 11 [1536/70488 (2%)]\tLoss: 0.923521\n",
      "1483\n",
      "Train Epoch: 11 [2048/70488 (3%)]\tLoss: 0.923186\n",
      "1484\n",
      "Train Epoch: 11 [2560/70488 (4%)]\tLoss: 0.925379\n",
      "1485\n",
      "Train Epoch: 11 [3072/70488 (4%)]\tLoss: 0.928820\n",
      "1486\n",
      "Train Epoch: 11 [3584/70488 (5%)]\tLoss: 0.925418\n",
      "1487\n",
      "Train Epoch: 11 [4096/70488 (6%)]\tLoss: 0.925262\n",
      "1488\n",
      "Train Epoch: 11 [4608/70488 (7%)]\tLoss: 0.925431\n",
      "1489\n",
      "Train Epoch: 11 [5120/70488 (7%)]\tLoss: 0.923506\n",
      "1490\n",
      "Train Epoch: 11 [5632/70488 (8%)]\tLoss: 0.924616\n",
      "1491\n",
      "Train Epoch: 11 [6144/70488 (9%)]\tLoss: 0.926827\n",
      "1492\n",
      "Train Epoch: 11 [6656/70488 (9%)]\tLoss: 0.929398\n",
      "1493\n",
      "Train Epoch: 11 [7168/70488 (10%)]\tLoss: 0.925467\n",
      "1494\n",
      "Train Epoch: 11 [7680/70488 (11%)]\tLoss: 0.926797\n",
      "1495\n",
      "Train Epoch: 11 [8192/70488 (12%)]\tLoss: 0.926617\n",
      "1496\n",
      "Train Epoch: 11 [8704/70488 (12%)]\tLoss: 0.924328\n",
      "1497\n",
      "Train Epoch: 11 [9216/70488 (13%)]\tLoss: 0.923416\n",
      "1498\n",
      "Train Epoch: 11 [9728/70488 (14%)]\tLoss: 0.924970\n",
      "1499\n",
      "Train Epoch: 11 [10240/70488 (14%)]\tLoss: 0.924856\n",
      "1500\n",
      "Train Epoch: 11 [10752/70488 (15%)]\tLoss: 0.925121\n",
      "1501\n",
      "Train Epoch: 11 [11264/70488 (16%)]\tLoss: 0.927173\n",
      "1502\n",
      "Train Epoch: 11 [11776/70488 (17%)]\tLoss: 0.927114\n",
      "1503\n",
      "Train Epoch: 11 [12288/70488 (17%)]\tLoss: 0.923282\n",
      "1504\n",
      "Train Epoch: 11 [12800/70488 (18%)]\tLoss: 0.926185\n",
      "1505\n",
      "Train Epoch: 11 [13312/70488 (19%)]\tLoss: 0.926153\n",
      "1506\n",
      "Train Epoch: 11 [13824/70488 (20%)]\tLoss: 0.924544\n",
      "1507\n",
      "Train Epoch: 11 [14336/70488 (20%)]\tLoss: 0.925683\n",
      "1508\n",
      "Train Epoch: 11 [14848/70488 (21%)]\tLoss: 0.924282\n",
      "1509\n",
      "Train Epoch: 11 [15360/70488 (22%)]\tLoss: 0.927299\n",
      "1510\n",
      "Train Epoch: 11 [15872/70488 (22%)]\tLoss: 0.928299\n",
      "1511\n",
      "Train Epoch: 11 [16384/70488 (23%)]\tLoss: 0.926654\n",
      "1512\n",
      "Train Epoch: 11 [16896/70488 (24%)]\tLoss: 0.926197\n",
      "1513\n",
      "Train Epoch: 11 [17408/70488 (25%)]\tLoss: 0.928152\n",
      "1514\n",
      "Train Epoch: 11 [17920/70488 (25%)]\tLoss: 0.927383\n",
      "1515\n",
      "Train Epoch: 11 [18432/70488 (26%)]\tLoss: 0.924141\n",
      "1516\n",
      "Train Epoch: 11 [18944/70488 (27%)]\tLoss: 0.925775\n",
      "1517\n",
      "Train Epoch: 11 [19456/70488 (28%)]\tLoss: 0.924862\n",
      "1518\n",
      "Train Epoch: 11 [19968/70488 (28%)]\tLoss: 0.926997\n",
      "1519\n",
      "Train Epoch: 11 [20480/70488 (29%)]\tLoss: 0.924803\n",
      "1520\n",
      "Train Epoch: 11 [20992/70488 (30%)]\tLoss: 0.924754\n",
      "1521\n",
      "Train Epoch: 11 [21504/70488 (30%)]\tLoss: 0.925208\n",
      "1522\n",
      "Train Epoch: 11 [22016/70488 (31%)]\tLoss: 0.925135\n",
      "1523\n",
      "Train Epoch: 11 [22528/70488 (32%)]\tLoss: 0.924683\n",
      "1524\n",
      "Train Epoch: 11 [23040/70488 (33%)]\tLoss: 0.928283\n",
      "1525\n",
      "Train Epoch: 11 [23552/70488 (33%)]\tLoss: 0.924380\n",
      "1526\n",
      "Train Epoch: 11 [24064/70488 (34%)]\tLoss: 0.924204\n",
      "1527\n",
      "Train Epoch: 11 [24576/70488 (35%)]\tLoss: 0.927533\n",
      "1528\n",
      "Train Epoch: 11 [25088/70488 (36%)]\tLoss: 0.926057\n",
      "1529\n",
      "Train Epoch: 11 [25600/70488 (36%)]\tLoss: 0.924265\n",
      "1530\n",
      "Train Epoch: 11 [26112/70488 (37%)]\tLoss: 0.927301\n",
      "1531\n",
      "Train Epoch: 11 [26624/70488 (38%)]\tLoss: 0.927822\n",
      "1532\n",
      "Train Epoch: 11 [27136/70488 (38%)]\tLoss: 0.925204\n",
      "1533\n",
      "Train Epoch: 11 [27648/70488 (39%)]\tLoss: 0.922654\n",
      "1534\n",
      "Train Epoch: 11 [28160/70488 (40%)]\tLoss: 0.926245\n",
      "1535\n",
      "Train Epoch: 11 [28672/70488 (41%)]\tLoss: 0.924216\n",
      "1536\n",
      "Train Epoch: 11 [29184/70488 (41%)]\tLoss: 0.924538\n",
      "1537\n",
      "Train Epoch: 11 [29696/70488 (42%)]\tLoss: 0.924523\n",
      "1538\n",
      "Train Epoch: 11 [30208/70488 (43%)]\tLoss: 0.924524\n",
      "1539\n",
      "Train Epoch: 11 [30720/70488 (43%)]\tLoss: 0.923549\n",
      "1540\n",
      "Train Epoch: 11 [31232/70488 (44%)]\tLoss: 0.927340\n",
      "1541\n",
      "Train Epoch: 11 [31744/70488 (45%)]\tLoss: 0.926691\n",
      "1542\n",
      "Train Epoch: 11 [32256/70488 (46%)]\tLoss: 0.925999\n",
      "1543\n",
      "Train Epoch: 11 [32768/70488 (46%)]\tLoss: 0.921610\n",
      "1544\n",
      "Train Epoch: 11 [33280/70488 (47%)]\tLoss: 0.927518\n",
      "1545\n",
      "Train Epoch: 11 [33792/70488 (48%)]\tLoss: 0.924101\n",
      "1546\n",
      "Train Epoch: 11 [34304/70488 (49%)]\tLoss: 0.924881\n",
      "1547\n",
      "Train Epoch: 11 [34816/70488 (49%)]\tLoss: 0.924455\n",
      "1548\n",
      "Train Epoch: 11 [35328/70488 (50%)]\tLoss: 0.924953\n",
      "1549\n",
      "Train Epoch: 11 [35840/70488 (51%)]\tLoss: 0.923791\n",
      "1550\n",
      "Train Epoch: 11 [36352/70488 (51%)]\tLoss: 0.926349\n",
      "1551\n",
      "Train Epoch: 11 [36864/70488 (52%)]\tLoss: 0.923967\n",
      "1552\n",
      "Train Epoch: 11 [37376/70488 (53%)]\tLoss: 0.923924\n",
      "1553\n",
      "Train Epoch: 11 [37888/70488 (54%)]\tLoss: 0.925855\n",
      "1554\n",
      "Train Epoch: 11 [38400/70488 (54%)]\tLoss: 0.924699\n",
      "1555\n",
      "Train Epoch: 11 [38912/70488 (55%)]\tLoss: 0.925849\n",
      "1556\n",
      "Train Epoch: 11 [39424/70488 (56%)]\tLoss: 0.924360\n",
      "1557\n",
      "Train Epoch: 11 [39936/70488 (57%)]\tLoss: 0.923831\n",
      "1558\n",
      "Train Epoch: 11 [40448/70488 (57%)]\tLoss: 0.923668\n",
      "1559\n",
      "Train Epoch: 11 [40960/70488 (58%)]\tLoss: 0.927349\n",
      "1560\n",
      "Train Epoch: 11 [41472/70488 (59%)]\tLoss: 0.926562\n",
      "1561\n",
      "Train Epoch: 11 [41984/70488 (59%)]\tLoss: 0.926032\n",
      "1562\n",
      "Train Epoch: 11 [42496/70488 (60%)]\tLoss: 0.931505\n",
      "1563\n",
      "Train Epoch: 11 [43008/70488 (61%)]\tLoss: 0.923068\n",
      "1564\n",
      "Train Epoch: 11 [43520/70488 (62%)]\tLoss: 0.923572\n",
      "1565\n",
      "Train Epoch: 11 [44032/70488 (62%)]\tLoss: 0.925201\n",
      "1566\n",
      "Train Epoch: 11 [44544/70488 (63%)]\tLoss: 0.925630\n",
      "1567\n",
      "Train Epoch: 11 [45056/70488 (64%)]\tLoss: 0.925371\n",
      "1568\n",
      "Train Epoch: 11 [45568/70488 (64%)]\tLoss: 0.924513\n",
      "1569\n",
      "Train Epoch: 11 [46080/70488 (65%)]\tLoss: 0.926336\n",
      "1570\n",
      "Train Epoch: 11 [46592/70488 (66%)]\tLoss: 0.923137\n",
      "1571\n",
      "Train Epoch: 11 [47104/70488 (67%)]\tLoss: 0.925369\n",
      "1572\n",
      "Train Epoch: 11 [47616/70488 (67%)]\tLoss: 0.926899\n",
      "1573\n",
      "Train Epoch: 11 [48128/70488 (68%)]\tLoss: 0.925890\n",
      "1574\n",
      "Train Epoch: 11 [48640/70488 (69%)]\tLoss: 0.925358\n",
      "1575\n",
      "Train Epoch: 11 [49152/70488 (70%)]\tLoss: 0.924849\n",
      "1576\n",
      "Train Epoch: 11 [49664/70488 (70%)]\tLoss: 0.923305\n",
      "1577\n",
      "Train Epoch: 11 [50176/70488 (71%)]\tLoss: 0.927664\n",
      "1578\n",
      "Train Epoch: 11 [50688/70488 (72%)]\tLoss: 0.925056\n",
      "1579\n",
      "Train Epoch: 11 [51200/70488 (72%)]\tLoss: 0.925575\n",
      "1580\n",
      "Train Epoch: 11 [51712/70488 (73%)]\tLoss: 0.922921\n",
      "1581\n",
      "Train Epoch: 11 [52224/70488 (74%)]\tLoss: 0.926181\n",
      "1582\n",
      "Train Epoch: 11 [52736/70488 (75%)]\tLoss: 0.926763\n",
      "1583\n",
      "Train Epoch: 11 [53248/70488 (75%)]\tLoss: 0.925415\n",
      "1584\n",
      "Train Epoch: 11 [53760/70488 (76%)]\tLoss: 0.925414\n",
      "1585\n",
      "Train Epoch: 11 [54272/70488 (77%)]\tLoss: 0.925063\n",
      "1586\n",
      "Train Epoch: 11 [54784/70488 (78%)]\tLoss: 0.925492\n",
      "1587\n",
      "Train Epoch: 11 [55296/70488 (78%)]\tLoss: 0.924623\n",
      "1588\n",
      "Train Epoch: 11 [55808/70488 (79%)]\tLoss: 0.926763\n",
      "1589\n",
      "Train Epoch: 11 [56320/70488 (80%)]\tLoss: 0.925177\n",
      "1590\n",
      "Train Epoch: 11 [56832/70488 (80%)]\tLoss: 0.926805\n",
      "1591\n",
      "Train Epoch: 11 [57344/70488 (81%)]\tLoss: 0.923123\n",
      "1592\n",
      "Train Epoch: 11 [57856/70488 (82%)]\tLoss: 0.925604\n",
      "1593\n",
      "Train Epoch: 11 [58368/70488 (83%)]\tLoss: 0.922720\n",
      "1594\n",
      "Train Epoch: 11 [58880/70488 (83%)]\tLoss: 0.925485\n",
      "1595\n",
      "Train Epoch: 11 [59392/70488 (84%)]\tLoss: 0.924570\n",
      "1596\n",
      "Train Epoch: 11 [59904/70488 (85%)]\tLoss: 0.924327\n",
      "1597\n",
      "Train Epoch: 11 [60416/70488 (86%)]\tLoss: 0.925462\n",
      "1598\n",
      "Train Epoch: 11 [60928/70488 (86%)]\tLoss: 0.927139\n",
      "1599\n",
      "Train Epoch: 11 [61440/70488 (87%)]\tLoss: 0.926020\n",
      "1600\n",
      "Train Epoch: 11 [61952/70488 (88%)]\tLoss: 0.925339\n",
      "1601\n",
      "Train Epoch: 11 [62464/70488 (88%)]\tLoss: 0.923369\n",
      "1602\n",
      "Train Epoch: 11 [62976/70488 (89%)]\tLoss: 0.926504\n",
      "1603\n",
      "Train Epoch: 11 [63488/70488 (90%)]\tLoss: 0.924051\n",
      "1604\n",
      "Train Epoch: 11 [64000/70488 (91%)]\tLoss: 0.924550\n",
      "1605\n",
      "Train Epoch: 11 [64512/70488 (91%)]\tLoss: 0.927298\n",
      "1606\n",
      "Train Epoch: 11 [65024/70488 (92%)]\tLoss: 0.924691\n",
      "1607\n",
      "Train Epoch: 11 [65536/70488 (93%)]\tLoss: 0.922725\n",
      "1608\n",
      "Train Epoch: 11 [66048/70488 (93%)]\tLoss: 0.929163\n",
      "1609\n",
      "Train Epoch: 11 [66560/70488 (94%)]\tLoss: 0.927405\n",
      "1610\n",
      "Train Epoch: 11 [67072/70488 (95%)]\tLoss: 0.925795\n",
      "1611\n",
      "Train Epoch: 11 [67584/70488 (96%)]\tLoss: 0.923598\n",
      "1612\n",
      "Train Epoch: 11 [68096/70488 (96%)]\tLoss: 0.924931\n",
      "1613\n",
      "Train Epoch: 11 [68608/70488 (97%)]\tLoss: 0.922670\n",
      "1614\n",
      "Train Epoch: 11 [69120/70488 (98%)]\tLoss: 0.923513\n",
      "1615\n",
      "Train Epoch: 11 [69632/70488 (99%)]\tLoss: 0.922615\n",
      "1616\n",
      "Train Epoch: 11 [47128/70488 (99%)]\tLoss: 0.926789\n",
      "1617\n",
      "====> Epoch: 11 Average loss: 0.00362345\n",
      "====> Test set loss: 30.32377601\n",
      "Train Epoch: 12 [0/70488 (0%)]\tLoss: 0.928749\n",
      "1628\n",
      "Train Epoch: 12 [512/70488 (1%)]\tLoss: 0.925746\n",
      "1629\n",
      "Train Epoch: 12 [1024/70488 (1%)]\tLoss: 0.924122\n",
      "1630\n",
      "Train Epoch: 12 [1536/70488 (2%)]\tLoss: 0.924325\n",
      "1631\n",
      "Train Epoch: 12 [2048/70488 (3%)]\tLoss: 0.921238\n",
      "1632\n",
      "Train Epoch: 12 [2560/70488 (4%)]\tLoss: 0.922653\n",
      "1633\n",
      "Train Epoch: 12 [3072/70488 (4%)]\tLoss: 0.922461\n",
      "1634\n",
      "Train Epoch: 12 [3584/70488 (5%)]\tLoss: 0.921355\n",
      "1635\n",
      "Train Epoch: 12 [4096/70488 (6%)]\tLoss: 0.928453\n",
      "1636\n",
      "Train Epoch: 12 [4608/70488 (7%)]\tLoss: 0.924996\n",
      "1637\n",
      "Train Epoch: 12 [5120/70488 (7%)]\tLoss: 0.926114\n",
      "1638\n",
      "Train Epoch: 12 [5632/70488 (8%)]\tLoss: 0.926310\n",
      "1639\n",
      "Train Epoch: 12 [6144/70488 (9%)]\tLoss: 0.924029\n",
      "1640\n",
      "Train Epoch: 12 [6656/70488 (9%)]\tLoss: 0.923017\n",
      "1641\n",
      "Train Epoch: 12 [7168/70488 (10%)]\tLoss: 0.924043\n",
      "1642\n",
      "Train Epoch: 12 [7680/70488 (11%)]\tLoss: 0.926435\n",
      "1643\n",
      "Train Epoch: 12 [8192/70488 (12%)]\tLoss: 0.923572\n",
      "1644\n",
      "Train Epoch: 12 [8704/70488 (12%)]\tLoss: 0.924326\n",
      "1645\n",
      "Train Epoch: 12 [9216/70488 (13%)]\tLoss: 0.924543\n",
      "1646\n",
      "Train Epoch: 12 [9728/70488 (14%)]\tLoss: 0.922673\n",
      "1647\n",
      "Train Epoch: 12 [10240/70488 (14%)]\tLoss: 0.924929\n",
      "1648\n",
      "Train Epoch: 12 [10752/70488 (15%)]\tLoss: 0.924276\n",
      "1649\n",
      "Train Epoch: 12 [11264/70488 (16%)]\tLoss: 0.924296\n",
      "1650\n",
      "Train Epoch: 12 [11776/70488 (17%)]\tLoss: 0.922629\n",
      "1651\n",
      "Train Epoch: 12 [12288/70488 (17%)]\tLoss: 0.923004\n",
      "1652\n",
      "Train Epoch: 12 [12800/70488 (18%)]\tLoss: 0.923922\n",
      "1653\n",
      "Train Epoch: 12 [13312/70488 (19%)]\tLoss: 0.924153\n",
      "1654\n",
      "Train Epoch: 12 [13824/70488 (20%)]\tLoss: 0.921008\n",
      "1655\n",
      "Train Epoch: 12 [14336/70488 (20%)]\tLoss: 0.926278\n",
      "1656\n",
      "Train Epoch: 12 [14848/70488 (21%)]\tLoss: 0.925514\n",
      "1657\n",
      "Train Epoch: 12 [15360/70488 (22%)]\tLoss: 0.923220\n",
      "1658\n",
      "Train Epoch: 12 [15872/70488 (22%)]\tLoss: 0.924815\n",
      "1659\n",
      "Train Epoch: 12 [16384/70488 (23%)]\tLoss: 0.922719\n",
      "1660\n",
      "Train Epoch: 12 [16896/70488 (24%)]\tLoss: 0.924770\n",
      "1661\n",
      "Train Epoch: 12 [17408/70488 (25%)]\tLoss: 0.924820\n",
      "1662\n",
      "Train Epoch: 12 [17920/70488 (25%)]\tLoss: 0.922047\n",
      "1663\n",
      "Train Epoch: 12 [18432/70488 (26%)]\tLoss: 0.923492\n",
      "1664\n",
      "Train Epoch: 12 [18944/70488 (27%)]\tLoss: 0.923588\n",
      "1665\n",
      "Train Epoch: 12 [19456/70488 (28%)]\tLoss: 0.922035\n",
      "1666\n",
      "Train Epoch: 12 [19968/70488 (28%)]\tLoss: 0.921518\n",
      "1667\n",
      "Train Epoch: 12 [20480/70488 (29%)]\tLoss: 0.925216\n",
      "1668\n",
      "Train Epoch: 12 [20992/70488 (30%)]\tLoss: 0.922742\n",
      "1669\n",
      "Train Epoch: 12 [21504/70488 (30%)]\tLoss: 0.925228\n",
      "1670\n",
      "Train Epoch: 12 [22016/70488 (31%)]\tLoss: 0.922473\n",
      "1671\n",
      "Train Epoch: 12 [22528/70488 (32%)]\tLoss: 0.925228\n",
      "1672\n",
      "Train Epoch: 12 [23040/70488 (33%)]\tLoss: 0.920199\n",
      "1673\n",
      "Train Epoch: 12 [23552/70488 (33%)]\tLoss: 0.925959\n",
      "1674\n",
      "Train Epoch: 12 [24064/70488 (34%)]\tLoss: 0.926172\n",
      "1675\n",
      "Train Epoch: 12 [24576/70488 (35%)]\tLoss: 0.925146\n",
      "1676\n",
      "Train Epoch: 12 [25088/70488 (36%)]\tLoss: 0.922367\n",
      "1677\n",
      "Train Epoch: 12 [25600/70488 (36%)]\tLoss: 0.922812\n",
      "1678\n",
      "Train Epoch: 12 [26112/70488 (37%)]\tLoss: 0.924501\n",
      "1679\n",
      "Train Epoch: 12 [26624/70488 (38%)]\tLoss: 0.920177\n",
      "1680\n",
      "Train Epoch: 12 [27136/70488 (38%)]\tLoss: 0.928066\n",
      "1681\n",
      "Train Epoch: 12 [27648/70488 (39%)]\tLoss: 0.923079\n",
      "1682\n",
      "Train Epoch: 12 [28160/70488 (40%)]\tLoss: 0.923428\n",
      "1683\n",
      "Train Epoch: 12 [28672/70488 (41%)]\tLoss: 0.925963\n",
      "1684\n",
      "Train Epoch: 12 [29184/70488 (41%)]\tLoss: 0.922056\n",
      "1685\n",
      "Train Epoch: 12 [29696/70488 (42%)]\tLoss: 0.921368\n",
      "1686\n",
      "Train Epoch: 12 [30208/70488 (43%)]\tLoss: 0.926281\n",
      "1687\n",
      "Train Epoch: 12 [30720/70488 (43%)]\tLoss: 0.923803\n",
      "1688\n",
      "Train Epoch: 12 [31232/70488 (44%)]\tLoss: 0.926419\n",
      "1689\n",
      "Train Epoch: 12 [31744/70488 (45%)]\tLoss: 0.923427\n",
      "1690\n",
      "Train Epoch: 12 [32256/70488 (46%)]\tLoss: 0.924638\n",
      "1691\n",
      "Train Epoch: 12 [32768/70488 (46%)]\tLoss: 0.921459\n",
      "1692\n",
      "Train Epoch: 12 [33280/70488 (47%)]\tLoss: 0.922550\n",
      "1693\n",
      "Train Epoch: 12 [33792/70488 (48%)]\tLoss: 0.925776\n",
      "1694\n",
      "Train Epoch: 12 [34304/70488 (49%)]\tLoss: 0.924386\n",
      "1695\n",
      "Train Epoch: 12 [34816/70488 (49%)]\tLoss: 0.927757\n",
      "1696\n",
      "Train Epoch: 12 [35328/70488 (50%)]\tLoss: 0.923927\n",
      "1697\n",
      "Train Epoch: 12 [35840/70488 (51%)]\tLoss: 0.923050\n",
      "1698\n",
      "Train Epoch: 12 [36352/70488 (51%)]\tLoss: 0.925494\n",
      "1699\n",
      "Train Epoch: 12 [36864/70488 (52%)]\tLoss: 0.925436\n",
      "1700\n",
      "Train Epoch: 12 [37376/70488 (53%)]\tLoss: 0.922956\n",
      "1701\n",
      "Train Epoch: 12 [37888/70488 (54%)]\tLoss: 0.921947\n",
      "1702\n",
      "Train Epoch: 12 [38400/70488 (54%)]\tLoss: 0.925874\n",
      "1703\n",
      "Train Epoch: 12 [38912/70488 (55%)]\tLoss: 0.921619\n",
      "1704\n",
      "Train Epoch: 12 [39424/70488 (56%)]\tLoss: 0.921657\n",
      "1705\n",
      "Train Epoch: 12 [39936/70488 (57%)]\tLoss: 0.922414\n",
      "1706\n",
      "Train Epoch: 12 [40448/70488 (57%)]\tLoss: 0.926156\n",
      "1707\n",
      "Train Epoch: 12 [40960/70488 (58%)]\tLoss: 0.922366\n",
      "1708\n",
      "Train Epoch: 12 [41472/70488 (59%)]\tLoss: 0.923345\n",
      "1709\n",
      "Train Epoch: 12 [41984/70488 (59%)]\tLoss: 0.924209\n",
      "1710\n",
      "Train Epoch: 12 [42496/70488 (60%)]\tLoss: 0.923129\n",
      "1711\n",
      "Train Epoch: 12 [43008/70488 (61%)]\tLoss: 0.925128\n",
      "1712\n",
      "Train Epoch: 12 [43520/70488 (62%)]\tLoss: 0.923997\n",
      "1713\n",
      "Train Epoch: 12 [44032/70488 (62%)]\tLoss: 0.923729\n",
      "1714\n",
      "Train Epoch: 12 [44544/70488 (63%)]\tLoss: 0.922942\n",
      "1715\n",
      "Train Epoch: 12 [45056/70488 (64%)]\tLoss: 0.922356\n",
      "1716\n",
      "Train Epoch: 12 [45568/70488 (64%)]\tLoss: 0.925582\n",
      "1717\n",
      "Train Epoch: 12 [46080/70488 (65%)]\tLoss: 0.923565\n",
      "1718\n",
      "Train Epoch: 12 [46592/70488 (66%)]\tLoss: 0.922022\n",
      "1719\n",
      "Train Epoch: 12 [47104/70488 (67%)]\tLoss: 0.921635\n",
      "1720\n",
      "Train Epoch: 12 [47616/70488 (67%)]\tLoss: 0.923025\n",
      "1721\n",
      "Train Epoch: 12 [48128/70488 (68%)]\tLoss: 0.922612\n",
      "1722\n",
      "Train Epoch: 12 [48640/70488 (69%)]\tLoss: 0.922466\n",
      "1723\n",
      "Train Epoch: 12 [49152/70488 (70%)]\tLoss: 0.925072\n",
      "1724\n",
      "Train Epoch: 12 [49664/70488 (70%)]\tLoss: 0.921996\n",
      "1725\n",
      "Train Epoch: 12 [50176/70488 (71%)]\tLoss: 0.924545\n",
      "1726\n",
      "Train Epoch: 12 [50688/70488 (72%)]\tLoss: 0.926687\n",
      "1727\n",
      "Train Epoch: 12 [51200/70488 (72%)]\tLoss: 0.922083\n",
      "1728\n",
      "Train Epoch: 12 [51712/70488 (73%)]\tLoss: 0.921533\n",
      "1729\n",
      "Train Epoch: 12 [52224/70488 (74%)]\tLoss: 0.922326\n",
      "1730\n",
      "Train Epoch: 12 [52736/70488 (75%)]\tLoss: 0.923325\n",
      "1731\n",
      "Train Epoch: 12 [53248/70488 (75%)]\tLoss: 0.922679\n",
      "1732\n",
      "Train Epoch: 12 [53760/70488 (76%)]\tLoss: 0.925318\n",
      "1733\n",
      "Train Epoch: 12 [54272/70488 (77%)]\tLoss: 0.920069\n",
      "1734\n",
      "Train Epoch: 12 [54784/70488 (78%)]\tLoss: 0.925097\n",
      "1735\n",
      "Train Epoch: 12 [55296/70488 (78%)]\tLoss: 0.922401\n",
      "1736\n",
      "Train Epoch: 12 [55808/70488 (79%)]\tLoss: 0.921395\n",
      "1737\n",
      "Train Epoch: 12 [56320/70488 (80%)]\tLoss: 0.923891\n",
      "1738\n",
      "Train Epoch: 12 [56832/70488 (80%)]\tLoss: 0.922371\n",
      "1739\n",
      "Train Epoch: 12 [57344/70488 (81%)]\tLoss: 0.924026\n",
      "1740\n",
      "Train Epoch: 12 [57856/70488 (82%)]\tLoss: 0.923607\n",
      "1741\n",
      "Train Epoch: 12 [58368/70488 (83%)]\tLoss: 0.922761\n",
      "1742\n",
      "Train Epoch: 12 [58880/70488 (83%)]\tLoss: 0.924410\n",
      "1743\n",
      "Train Epoch: 12 [59392/70488 (84%)]\tLoss: 0.923067\n",
      "1744\n",
      "Train Epoch: 12 [59904/70488 (85%)]\tLoss: 0.921164\n",
      "1745\n",
      "Train Epoch: 12 [60416/70488 (86%)]\tLoss: 0.924309\n",
      "1746\n",
      "Train Epoch: 12 [60928/70488 (86%)]\tLoss: 0.925652\n",
      "1747\n",
      "Train Epoch: 12 [61440/70488 (87%)]\tLoss: 0.924367\n",
      "1748\n",
      "Train Epoch: 12 [61952/70488 (88%)]\tLoss: 0.923796\n",
      "1749\n",
      "Train Epoch: 12 [62464/70488 (88%)]\tLoss: 0.922754\n",
      "1750\n",
      "Train Epoch: 12 [62976/70488 (89%)]\tLoss: 0.922146\n",
      "1751\n",
      "Train Epoch: 12 [63488/70488 (90%)]\tLoss: 0.923838\n",
      "1752\n",
      "Train Epoch: 12 [64000/70488 (91%)]\tLoss: 0.925515\n",
      "1753\n",
      "Train Epoch: 12 [64512/70488 (91%)]\tLoss: 0.922136\n",
      "1754\n",
      "Train Epoch: 12 [65024/70488 (92%)]\tLoss: 0.924491\n",
      "1755\n",
      "Train Epoch: 12 [65536/70488 (93%)]\tLoss: 0.923942\n",
      "1756\n",
      "Train Epoch: 12 [66048/70488 (93%)]\tLoss: 0.924958\n",
      "1757\n",
      "Train Epoch: 12 [66560/70488 (94%)]\tLoss: 0.924172\n",
      "1758\n",
      "Train Epoch: 12 [67072/70488 (95%)]\tLoss: 0.923299\n",
      "1759\n",
      "Train Epoch: 12 [67584/70488 (96%)]\tLoss: 0.922530\n",
      "1760\n",
      "Train Epoch: 12 [68096/70488 (96%)]\tLoss: 0.921309\n",
      "1761\n",
      "Train Epoch: 12 [68608/70488 (97%)]\tLoss: 0.926756\n",
      "1762\n",
      "Train Epoch: 12 [69120/70488 (98%)]\tLoss: 0.922746\n",
      "1763\n",
      "Train Epoch: 12 [69632/70488 (99%)]\tLoss: 0.921174\n",
      "1764\n",
      "Train Epoch: 12 [47128/70488 (99%)]\tLoss: 0.922116\n",
      "1765\n",
      "====> Epoch: 12 Average loss: 0.00361694\n",
      "====> Test set loss: 30.31351137\n",
      "Train Epoch: 13 [0/70488 (0%)]\tLoss: 0.922983\n",
      "1776\n",
      "Train Epoch: 13 [512/70488 (1%)]\tLoss: 0.923894\n",
      "1777\n",
      "Train Epoch: 13 [1024/70488 (1%)]\tLoss: 0.922272\n",
      "1778\n",
      "Train Epoch: 13 [1536/70488 (2%)]\tLoss: 0.922097\n",
      "1779\n",
      "Train Epoch: 13 [2048/70488 (3%)]\tLoss: 0.920470\n",
      "1780\n",
      "Train Epoch: 13 [2560/70488 (4%)]\tLoss: 0.921576\n",
      "1781\n",
      "Train Epoch: 13 [3072/70488 (4%)]\tLoss: 0.923378\n",
      "1782\n",
      "Train Epoch: 13 [3584/70488 (5%)]\tLoss: 0.921152\n",
      "1783\n",
      "Train Epoch: 13 [4096/70488 (6%)]\tLoss: 0.920797\n",
      "1784\n",
      "Train Epoch: 13 [4608/70488 (7%)]\tLoss: 0.919206\n",
      "1785\n",
      "Train Epoch: 13 [5120/70488 (7%)]\tLoss: 0.922235\n",
      "1786\n",
      "Train Epoch: 13 [5632/70488 (8%)]\tLoss: 0.922005\n",
      "1787\n",
      "Train Epoch: 13 [6144/70488 (9%)]\tLoss: 0.922900\n",
      "1788\n",
      "Train Epoch: 13 [6656/70488 (9%)]\tLoss: 0.921630\n",
      "1789\n",
      "Train Epoch: 13 [7168/70488 (10%)]\tLoss: 0.920711\n",
      "1790\n",
      "Train Epoch: 13 [7680/70488 (11%)]\tLoss: 0.920258\n",
      "1791\n",
      "Train Epoch: 13 [8192/70488 (12%)]\tLoss: 0.923287\n",
      "1792\n",
      "Train Epoch: 13 [8704/70488 (12%)]\tLoss: 0.924128\n",
      "1793\n",
      "Train Epoch: 13 [9216/70488 (13%)]\tLoss: 0.921473\n",
      "1794\n",
      "Train Epoch: 13 [9728/70488 (14%)]\tLoss: 0.921802\n",
      "1795\n",
      "Train Epoch: 13 [10240/70488 (14%)]\tLoss: 0.922306\n",
      "1796\n",
      "Train Epoch: 13 [10752/70488 (15%)]\tLoss: 0.921900\n",
      "1797\n",
      "Train Epoch: 13 [11264/70488 (16%)]\tLoss: 0.920449\n",
      "1798\n",
      "Train Epoch: 13 [11776/70488 (17%)]\tLoss: 0.922314\n",
      "1799\n",
      "Train Epoch: 13 [12288/70488 (17%)]\tLoss: 0.921048\n",
      "1800\n",
      "Train Epoch: 13 [12800/70488 (18%)]\tLoss: 0.920053\n",
      "1801\n",
      "Train Epoch: 13 [13312/70488 (19%)]\tLoss: 0.924637\n",
      "1802\n",
      "Train Epoch: 13 [13824/70488 (20%)]\tLoss: 0.922562\n",
      "1803\n",
      "Train Epoch: 13 [14336/70488 (20%)]\tLoss: 0.923462\n",
      "1804\n",
      "Train Epoch: 13 [14848/70488 (21%)]\tLoss: 0.921957\n",
      "1805\n",
      "Train Epoch: 13 [15360/70488 (22%)]\tLoss: 0.922920\n",
      "1806\n",
      "Train Epoch: 13 [15872/70488 (22%)]\tLoss: 0.922483\n",
      "1807\n",
      "Train Epoch: 13 [16384/70488 (23%)]\tLoss: 0.922176\n",
      "1808\n",
      "Train Epoch: 13 [16896/70488 (24%)]\tLoss: 0.921770\n",
      "1809\n",
      "Train Epoch: 13 [17408/70488 (25%)]\tLoss: 0.921827\n",
      "1810\n",
      "Train Epoch: 13 [17920/70488 (25%)]\tLoss: 0.925175\n",
      "1811\n",
      "Train Epoch: 13 [18432/70488 (26%)]\tLoss: 0.921311\n",
      "1812\n",
      "Train Epoch: 13 [18944/70488 (27%)]\tLoss: 0.917825\n",
      "1813\n",
      "Train Epoch: 13 [19456/70488 (28%)]\tLoss: 0.921972\n",
      "1814\n",
      "Train Epoch: 13 [19968/70488 (28%)]\tLoss: 0.921886\n",
      "1815\n",
      "Train Epoch: 13 [20480/70488 (29%)]\tLoss: 0.921051\n",
      "1816\n",
      "Train Epoch: 13 [20992/70488 (30%)]\tLoss: 0.923075\n",
      "1817\n",
      "Train Epoch: 13 [21504/70488 (30%)]\tLoss: 0.924165\n",
      "1818\n",
      "Train Epoch: 13 [22016/70488 (31%)]\tLoss: 0.920677\n",
      "1819\n",
      "Train Epoch: 13 [22528/70488 (32%)]\tLoss: 0.925523\n",
      "1820\n",
      "Train Epoch: 13 [23040/70488 (33%)]\tLoss: 0.920922\n",
      "1821\n",
      "Train Epoch: 13 [23552/70488 (33%)]\tLoss: 0.921474\n",
      "1822\n",
      "Train Epoch: 13 [24064/70488 (34%)]\tLoss: 0.924716\n",
      "1823\n",
      "Train Epoch: 13 [24576/70488 (35%)]\tLoss: 0.921571\n",
      "1824\n",
      "Train Epoch: 13 [25088/70488 (36%)]\tLoss: 0.919697\n",
      "1825\n",
      "Train Epoch: 13 [25600/70488 (36%)]\tLoss: 0.921069\n",
      "1826\n",
      "Train Epoch: 13 [26112/70488 (37%)]\tLoss: 0.923474\n",
      "1827\n",
      "Train Epoch: 13 [26624/70488 (38%)]\tLoss: 0.921927\n",
      "1828\n",
      "Train Epoch: 13 [27136/70488 (38%)]\tLoss: 0.919967\n",
      "1829\n",
      "Train Epoch: 13 [27648/70488 (39%)]\tLoss: 0.923799\n",
      "1830\n",
      "Train Epoch: 13 [28160/70488 (40%)]\tLoss: 0.923896\n",
      "1831\n",
      "Train Epoch: 13 [28672/70488 (41%)]\tLoss: 0.920391\n",
      "1832\n",
      "Train Epoch: 13 [29184/70488 (41%)]\tLoss: 0.922287\n",
      "1833\n",
      "Train Epoch: 13 [29696/70488 (42%)]\tLoss: 0.920797\n",
      "1834\n",
      "Train Epoch: 13 [30208/70488 (43%)]\tLoss: 0.919347\n",
      "1835\n",
      "Train Epoch: 13 [30720/70488 (43%)]\tLoss: 0.920846\n",
      "1836\n",
      "Train Epoch: 13 [31232/70488 (44%)]\tLoss: 0.922347\n",
      "1837\n",
      "Train Epoch: 13 [31744/70488 (45%)]\tLoss: 0.923380\n",
      "1838\n",
      "Train Epoch: 13 [32256/70488 (46%)]\tLoss: 0.921026\n",
      "1839\n",
      "Train Epoch: 13 [32768/70488 (46%)]\tLoss: 0.922020\n",
      "1840\n",
      "Train Epoch: 13 [33280/70488 (47%)]\tLoss: 0.923690\n",
      "1841\n",
      "Train Epoch: 13 [33792/70488 (48%)]\tLoss: 0.924906\n",
      "1842\n",
      "Train Epoch: 13 [34304/70488 (49%)]\tLoss: 0.921458\n",
      "1843\n",
      "Train Epoch: 13 [34816/70488 (49%)]\tLoss: 0.922912\n",
      "1844\n",
      "Train Epoch: 13 [35328/70488 (50%)]\tLoss: 0.920062\n",
      "1845\n",
      "Train Epoch: 13 [35840/70488 (51%)]\tLoss: 0.924568\n",
      "1846\n",
      "Train Epoch: 13 [36352/70488 (51%)]\tLoss: 0.921020\n",
      "1847\n",
      "Train Epoch: 13 [36864/70488 (52%)]\tLoss: 0.920353\n",
      "1848\n",
      "Train Epoch: 13 [37376/70488 (53%)]\tLoss: 0.921355\n",
      "1849\n",
      "Train Epoch: 13 [37888/70488 (54%)]\tLoss: 0.924118\n",
      "1850\n",
      "Train Epoch: 13 [38400/70488 (54%)]\tLoss: 0.922623\n",
      "1851\n",
      "Train Epoch: 13 [38912/70488 (55%)]\tLoss: 0.920232\n",
      "1852\n",
      "Train Epoch: 13 [39424/70488 (56%)]\tLoss: 0.922778\n",
      "1853\n",
      "Train Epoch: 13 [39936/70488 (57%)]\tLoss: 0.920953\n",
      "1854\n",
      "Train Epoch: 13 [40448/70488 (57%)]\tLoss: 0.919833\n",
      "1855\n",
      "Train Epoch: 13 [40960/70488 (58%)]\tLoss: 0.920501\n",
      "1856\n",
      "Train Epoch: 13 [41472/70488 (59%)]\tLoss: 0.921820\n",
      "1857\n",
      "Train Epoch: 13 [41984/70488 (59%)]\tLoss: 0.926993\n",
      "1858\n",
      "Train Epoch: 13 [42496/70488 (60%)]\tLoss: 0.921118\n",
      "1859\n",
      "Train Epoch: 13 [43008/70488 (61%)]\tLoss: 0.920517\n",
      "1860\n",
      "Train Epoch: 13 [43520/70488 (62%)]\tLoss: 0.921425\n",
      "1861\n",
      "Train Epoch: 13 [44032/70488 (62%)]\tLoss: 0.923782\n",
      "1862\n",
      "Train Epoch: 13 [44544/70488 (63%)]\tLoss: 0.921184\n",
      "1863\n",
      "Train Epoch: 13 [45056/70488 (64%)]\tLoss: 0.921507\n",
      "1864\n",
      "Train Epoch: 13 [45568/70488 (64%)]\tLoss: 0.920003\n",
      "1865\n",
      "Train Epoch: 13 [46080/70488 (65%)]\tLoss: 0.920069\n",
      "1866\n",
      "Train Epoch: 13 [46592/70488 (66%)]\tLoss: 0.924967\n",
      "1867\n",
      "Train Epoch: 13 [47104/70488 (67%)]\tLoss: 0.923102\n",
      "1868\n",
      "Train Epoch: 13 [47616/70488 (67%)]\tLoss: 0.923562\n",
      "1869\n",
      "Train Epoch: 13 [48128/70488 (68%)]\tLoss: 0.923602\n",
      "1870\n",
      "Train Epoch: 13 [48640/70488 (69%)]\tLoss: 0.921591\n",
      "1871\n",
      "Train Epoch: 13 [49152/70488 (70%)]\tLoss: 0.919262\n",
      "1872\n",
      "Train Epoch: 13 [49664/70488 (70%)]\tLoss: 0.923928\n",
      "1873\n",
      "Train Epoch: 13 [50176/70488 (71%)]\tLoss: 0.922809\n",
      "1874\n",
      "Train Epoch: 13 [50688/70488 (72%)]\tLoss: 0.923157\n",
      "1875\n",
      "Train Epoch: 13 [51200/70488 (72%)]\tLoss: 0.919488\n",
      "1876\n",
      "Train Epoch: 13 [51712/70488 (73%)]\tLoss: 0.921550\n",
      "1877\n",
      "Train Epoch: 13 [52224/70488 (74%)]\tLoss: 0.922383\n",
      "1878\n",
      "Train Epoch: 13 [52736/70488 (75%)]\tLoss: 0.925368\n",
      "1879\n",
      "Train Epoch: 13 [53248/70488 (75%)]\tLoss: 0.922900\n",
      "1880\n",
      "Train Epoch: 13 [53760/70488 (76%)]\tLoss: 0.919972\n",
      "1881\n",
      "Train Epoch: 13 [54272/70488 (77%)]\tLoss: 0.922636\n",
      "1882\n",
      "Train Epoch: 13 [54784/70488 (78%)]\tLoss: 0.922707\n",
      "1883\n",
      "Train Epoch: 13 [55296/70488 (78%)]\tLoss: 0.920953\n",
      "1884\n",
      "Train Epoch: 13 [55808/70488 (79%)]\tLoss: 0.919708\n",
      "1885\n",
      "Train Epoch: 13 [56320/70488 (80%)]\tLoss: 0.920765\n",
      "1886\n",
      "Train Epoch: 13 [56832/70488 (80%)]\tLoss: 0.920652\n",
      "1887\n",
      "Train Epoch: 13 [57344/70488 (81%)]\tLoss: 0.922447\n",
      "1888\n",
      "Train Epoch: 13 [57856/70488 (82%)]\tLoss: 0.920632\n",
      "1889\n",
      "Train Epoch: 13 [58368/70488 (83%)]\tLoss: 0.921108\n",
      "1890\n",
      "Train Epoch: 13 [58880/70488 (83%)]\tLoss: 0.922782\n",
      "1891\n",
      "Train Epoch: 13 [59392/70488 (84%)]\tLoss: 0.919759\n",
      "1892\n",
      "Train Epoch: 13 [59904/70488 (85%)]\tLoss: 0.921579\n",
      "1893\n",
      "Train Epoch: 13 [60416/70488 (86%)]\tLoss: 0.920464\n",
      "1894\n",
      "Train Epoch: 13 [60928/70488 (86%)]\tLoss: 0.920176\n",
      "1895\n",
      "Train Epoch: 13 [61440/70488 (87%)]\tLoss: 0.920583\n",
      "1896\n",
      "Train Epoch: 13 [61952/70488 (88%)]\tLoss: 0.920292\n",
      "1897\n",
      "Train Epoch: 13 [62464/70488 (88%)]\tLoss: 0.922100\n",
      "1898\n",
      "Train Epoch: 13 [62976/70488 (89%)]\tLoss: 0.921364\n",
      "1899\n",
      "Train Epoch: 13 [63488/70488 (90%)]\tLoss: 0.922206\n",
      "1900\n",
      "Train Epoch: 13 [64000/70488 (91%)]\tLoss: 0.919838\n",
      "1901\n",
      "Train Epoch: 13 [64512/70488 (91%)]\tLoss: 0.921442\n",
      "1902\n",
      "Train Epoch: 13 [65024/70488 (92%)]\tLoss: 0.921310\n",
      "1903\n",
      "Train Epoch: 13 [65536/70488 (93%)]\tLoss: 0.920326\n",
      "1904\n",
      "Train Epoch: 13 [66048/70488 (93%)]\tLoss: 0.920872\n",
      "1905\n",
      "Train Epoch: 13 [66560/70488 (94%)]\tLoss: 0.921373\n",
      "1906\n",
      "Train Epoch: 13 [67072/70488 (95%)]\tLoss: 0.920854\n",
      "1907\n",
      "Train Epoch: 13 [67584/70488 (96%)]\tLoss: 0.921698\n",
      "1908\n",
      "Train Epoch: 13 [68096/70488 (96%)]\tLoss: 0.920448\n",
      "1909\n",
      "Train Epoch: 13 [68608/70488 (97%)]\tLoss: 0.922966\n",
      "1910\n",
      "Train Epoch: 13 [69120/70488 (98%)]\tLoss: 0.922227\n",
      "1911\n",
      "Train Epoch: 13 [69632/70488 (99%)]\tLoss: 0.924083\n",
      "1912\n",
      "Train Epoch: 13 [47128/70488 (99%)]\tLoss: 0.920656\n",
      "1913\n",
      "====> Epoch: 13 Average loss: 0.00360958\n",
      "====> Test set loss: 30.36719561\n",
      "Train Epoch: 14 [0/70488 (0%)]\tLoss: 0.921688\n",
      "1924\n",
      "Train Epoch: 14 [512/70488 (1%)]\tLoss: 0.919966\n",
      "1925\n",
      "Train Epoch: 14 [1024/70488 (1%)]\tLoss: 0.919561\n",
      "1926\n",
      "Train Epoch: 14 [1536/70488 (2%)]\tLoss: 0.920327\n",
      "1927\n",
      "Train Epoch: 14 [2048/70488 (3%)]\tLoss: 0.917111\n",
      "1928\n",
      "Train Epoch: 14 [2560/70488 (4%)]\tLoss: 0.917983\n",
      "1929\n",
      "Train Epoch: 14 [3072/70488 (4%)]\tLoss: 0.919615\n",
      "1930\n",
      "Train Epoch: 14 [3584/70488 (5%)]\tLoss: 0.921209\n",
      "1931\n",
      "Train Epoch: 14 [4096/70488 (6%)]\tLoss: 0.920124\n",
      "1932\n",
      "Train Epoch: 14 [4608/70488 (7%)]\tLoss: 0.920963\n",
      "1933\n",
      "Train Epoch: 14 [5120/70488 (7%)]\tLoss: 0.920185\n",
      "1934\n",
      "Train Epoch: 14 [5632/70488 (8%)]\tLoss: 0.920543\n",
      "1935\n",
      "Train Epoch: 14 [6144/70488 (9%)]\tLoss: 0.919568\n",
      "1936\n",
      "Train Epoch: 14 [6656/70488 (9%)]\tLoss: 0.919695\n",
      "1937\n",
      "Train Epoch: 14 [7168/70488 (10%)]\tLoss: 0.922093\n",
      "1938\n",
      "Train Epoch: 14 [7680/70488 (11%)]\tLoss: 0.919127\n",
      "1939\n",
      "Train Epoch: 14 [8192/70488 (12%)]\tLoss: 0.920485\n",
      "1940\n",
      "Train Epoch: 14 [8704/70488 (12%)]\tLoss: 0.920059\n",
      "1941\n",
      "Train Epoch: 14 [9216/70488 (13%)]\tLoss: 0.921164\n",
      "1942\n",
      "Train Epoch: 14 [9728/70488 (14%)]\tLoss: 0.923317\n",
      "1943\n",
      "Train Epoch: 14 [10240/70488 (14%)]\tLoss: 0.918500\n",
      "1944\n",
      "Train Epoch: 14 [10752/70488 (15%)]\tLoss: 0.918361\n",
      "1945\n",
      "Train Epoch: 14 [11264/70488 (16%)]\tLoss: 0.920297\n",
      "1946\n",
      "Train Epoch: 14 [11776/70488 (17%)]\tLoss: 0.922127\n",
      "1947\n",
      "Train Epoch: 14 [12288/70488 (17%)]\tLoss: 0.919352\n",
      "1948\n",
      "Train Epoch: 14 [12800/70488 (18%)]\tLoss: 0.921165\n",
      "1949\n",
      "Train Epoch: 14 [13312/70488 (19%)]\tLoss: 0.916695\n",
      "1950\n",
      "Train Epoch: 14 [13824/70488 (20%)]\tLoss: 0.917037\n",
      "1951\n",
      "Train Epoch: 14 [14336/70488 (20%)]\tLoss: 0.919121\n",
      "1952\n",
      "Train Epoch: 14 [14848/70488 (21%)]\tLoss: 0.921222\n",
      "1953\n",
      "Train Epoch: 14 [15360/70488 (22%)]\tLoss: 0.919975\n",
      "1954\n",
      "Train Epoch: 14 [15872/70488 (22%)]\tLoss: 0.918371\n",
      "1955\n",
      "Train Epoch: 14 [16384/70488 (23%)]\tLoss: 0.920848\n",
      "1956\n",
      "Train Epoch: 14 [16896/70488 (24%)]\tLoss: 0.921942\n",
      "1957\n",
      "Train Epoch: 14 [17408/70488 (25%)]\tLoss: 0.921839\n",
      "1958\n",
      "Train Epoch: 14 [17920/70488 (25%)]\tLoss: 0.917834\n",
      "1959\n",
      "Train Epoch: 14 [18432/70488 (26%)]\tLoss: 0.918793\n",
      "1960\n",
      "Train Epoch: 14 [18944/70488 (27%)]\tLoss: 0.917053\n",
      "1961\n",
      "Train Epoch: 14 [19456/70488 (28%)]\tLoss: 0.917792\n",
      "1962\n",
      "Train Epoch: 14 [19968/70488 (28%)]\tLoss: 0.919835\n",
      "1963\n",
      "Train Epoch: 14 [20480/70488 (29%)]\tLoss: 0.920336\n",
      "1964\n",
      "Train Epoch: 14 [20992/70488 (30%)]\tLoss: 0.919899\n",
      "1965\n",
      "Train Epoch: 14 [21504/70488 (30%)]\tLoss: 0.920425\n",
      "1966\n",
      "Train Epoch: 14 [22016/70488 (31%)]\tLoss: 0.920910\n",
      "1967\n",
      "Train Epoch: 14 [22528/70488 (32%)]\tLoss: 0.919894\n",
      "1968\n",
      "Train Epoch: 14 [23040/70488 (33%)]\tLoss: 0.917284\n",
      "1969\n",
      "Train Epoch: 14 [23552/70488 (33%)]\tLoss: 0.921328\n",
      "1970\n",
      "Train Epoch: 14 [24064/70488 (34%)]\tLoss: 0.917568\n",
      "1971\n",
      "Train Epoch: 14 [24576/70488 (35%)]\tLoss: 0.918506\n",
      "1972\n",
      "Train Epoch: 14 [25088/70488 (36%)]\tLoss: 0.919575\n",
      "1973\n",
      "Train Epoch: 14 [25600/70488 (36%)]\tLoss: 0.921252\n",
      "1974\n",
      "Train Epoch: 14 [26112/70488 (37%)]\tLoss: 0.918881\n",
      "1975\n",
      "Train Epoch: 14 [26624/70488 (38%)]\tLoss: 0.918987\n",
      "1976\n",
      "Train Epoch: 14 [27136/70488 (38%)]\tLoss: 0.917205\n",
      "1977\n",
      "Train Epoch: 14 [27648/70488 (39%)]\tLoss: 0.919994\n",
      "1978\n",
      "Train Epoch: 14 [28160/70488 (40%)]\tLoss: 0.918528\n",
      "1979\n",
      "Train Epoch: 14 [28672/70488 (41%)]\tLoss: 0.918603\n",
      "1980\n",
      "Train Epoch: 14 [29184/70488 (41%)]\tLoss: 0.921304\n",
      "1981\n",
      "Train Epoch: 14 [29696/70488 (42%)]\tLoss: 0.917575\n",
      "1982\n",
      "Train Epoch: 14 [30208/70488 (43%)]\tLoss: 0.921360\n",
      "1983\n",
      "Train Epoch: 14 [30720/70488 (43%)]\tLoss: 0.920276\n",
      "1984\n",
      "Train Epoch: 14 [31232/70488 (44%)]\tLoss: 0.921118\n",
      "1985\n",
      "Train Epoch: 14 [31744/70488 (45%)]\tLoss: 0.918755\n",
      "1986\n",
      "Train Epoch: 14 [32256/70488 (46%)]\tLoss: 0.917638\n",
      "1987\n",
      "Train Epoch: 14 [32768/70488 (46%)]\tLoss: 0.921301\n",
      "1988\n",
      "Train Epoch: 14 [33280/70488 (47%)]\tLoss: 0.922313\n",
      "1989\n",
      "Train Epoch: 14 [33792/70488 (48%)]\tLoss: 0.919638\n",
      "1990\n",
      "Train Epoch: 14 [34304/70488 (49%)]\tLoss: 0.922247\n",
      "1991\n",
      "Train Epoch: 14 [34816/70488 (49%)]\tLoss: 0.921522\n",
      "1992\n",
      "Train Epoch: 14 [35328/70488 (50%)]\tLoss: 0.919504\n",
      "1993\n",
      "Train Epoch: 14 [35840/70488 (51%)]\tLoss: 0.917598\n",
      "1994\n",
      "Train Epoch: 14 [36352/70488 (51%)]\tLoss: 0.916649\n",
      "1995\n",
      "Train Epoch: 14 [36864/70488 (52%)]\tLoss: 0.917415\n",
      "1996\n",
      "Train Epoch: 14 [37376/70488 (53%)]\tLoss: 0.916807\n",
      "1997\n",
      "Train Epoch: 14 [37888/70488 (54%)]\tLoss: 0.920000\n",
      "1998\n",
      "Train Epoch: 14 [38400/70488 (54%)]\tLoss: 0.920356\n",
      "1999\n",
      "Train Epoch: 14 [38912/70488 (55%)]\tLoss: 0.918278\n",
      "2000\n",
      "Train Epoch: 14 [39424/70488 (56%)]\tLoss: 0.920231\n",
      "2001\n",
      "Train Epoch: 14 [39936/70488 (57%)]\tLoss: 0.916587\n",
      "2002\n",
      "Train Epoch: 14 [40448/70488 (57%)]\tLoss: 0.919782\n",
      "2003\n",
      "Train Epoch: 14 [40960/70488 (58%)]\tLoss: 0.917760\n",
      "2004\n",
      "Train Epoch: 14 [41472/70488 (59%)]\tLoss: 0.922465\n",
      "2005\n",
      "Train Epoch: 14 [41984/70488 (59%)]\tLoss: 0.920338\n",
      "2006\n",
      "Train Epoch: 14 [42496/70488 (60%)]\tLoss: 0.919724\n",
      "2007\n",
      "Train Epoch: 14 [43008/70488 (61%)]\tLoss: 0.918880\n",
      "2008\n",
      "Train Epoch: 14 [43520/70488 (62%)]\tLoss: 0.919619\n",
      "2009\n",
      "Train Epoch: 14 [44032/70488 (62%)]\tLoss: 0.921373\n",
      "2010\n",
      "Train Epoch: 14 [44544/70488 (63%)]\tLoss: 0.919035\n",
      "2011\n",
      "Train Epoch: 14 [45056/70488 (64%)]\tLoss: 0.918487\n",
      "2012\n",
      "Train Epoch: 14 [45568/70488 (64%)]\tLoss: 0.918486\n",
      "2013\n",
      "Train Epoch: 14 [46080/70488 (65%)]\tLoss: 0.918998\n",
      "2014\n",
      "Train Epoch: 14 [46592/70488 (66%)]\tLoss: 0.918467\n",
      "2015\n",
      "Train Epoch: 14 [47104/70488 (67%)]\tLoss: 0.917952\n",
      "2016\n",
      "Train Epoch: 14 [47616/70488 (67%)]\tLoss: 0.920939\n",
      "2017\n",
      "Train Epoch: 14 [48128/70488 (68%)]\tLoss: 0.918841\n",
      "2018\n",
      "Train Epoch: 14 [48640/70488 (69%)]\tLoss: 0.920012\n",
      "2019\n",
      "Train Epoch: 14 [49152/70488 (70%)]\tLoss: 0.919586\n",
      "2020\n",
      "Train Epoch: 14 [49664/70488 (70%)]\tLoss: 0.920441\n",
      "2021\n",
      "Train Epoch: 14 [50176/70488 (71%)]\tLoss: 0.921089\n",
      "2022\n",
      "Train Epoch: 14 [50688/70488 (72%)]\tLoss: 0.918196\n",
      "2023\n",
      "Train Epoch: 14 [51200/70488 (72%)]\tLoss: 0.918553\n",
      "2024\n",
      "Train Epoch: 14 [51712/70488 (73%)]\tLoss: 0.921588\n",
      "2025\n",
      "Train Epoch: 14 [52224/70488 (74%)]\tLoss: 0.920618\n",
      "2026\n",
      "Train Epoch: 14 [52736/70488 (75%)]\tLoss: 0.916142\n",
      "2027\n",
      "Train Epoch: 14 [53248/70488 (75%)]\tLoss: 0.918557\n",
      "2028\n",
      "Train Epoch: 14 [53760/70488 (76%)]\tLoss: 0.919244\n",
      "2029\n",
      "Train Epoch: 14 [54272/70488 (77%)]\tLoss: 0.920359\n",
      "2030\n",
      "Train Epoch: 14 [54784/70488 (78%)]\tLoss: 0.919724\n",
      "2031\n",
      "Train Epoch: 14 [55296/70488 (78%)]\tLoss: 0.920095\n",
      "2032\n",
      "Train Epoch: 14 [55808/70488 (79%)]\tLoss: 0.918421\n",
      "2033\n",
      "Train Epoch: 14 [56320/70488 (80%)]\tLoss: 0.918323\n",
      "2034\n",
      "Train Epoch: 14 [56832/70488 (80%)]\tLoss: 0.919519\n",
      "2035\n",
      "Train Epoch: 14 [57344/70488 (81%)]\tLoss: 0.921603\n",
      "2036\n",
      "Train Epoch: 14 [57856/70488 (82%)]\tLoss: 0.918652\n",
      "2037\n",
      "Train Epoch: 14 [58368/70488 (83%)]\tLoss: 0.919184\n",
      "2038\n",
      "Train Epoch: 14 [58880/70488 (83%)]\tLoss: 0.920776\n",
      "2039\n",
      "Train Epoch: 14 [59392/70488 (84%)]\tLoss: 0.918536\n",
      "2040\n",
      "Train Epoch: 14 [59904/70488 (85%)]\tLoss: 0.918688\n",
      "2041\n",
      "Train Epoch: 14 [60416/70488 (86%)]\tLoss: 0.919551\n",
      "2042\n",
      "Train Epoch: 14 [60928/70488 (86%)]\tLoss: 0.919960\n",
      "2043\n",
      "Train Epoch: 14 [61440/70488 (87%)]\tLoss: 0.917321\n",
      "2044\n",
      "Train Epoch: 14 [61952/70488 (88%)]\tLoss: 0.920400\n",
      "2045\n",
      "Train Epoch: 14 [62464/70488 (88%)]\tLoss: 0.918396\n",
      "2046\n",
      "Train Epoch: 14 [62976/70488 (89%)]\tLoss: 0.918047\n",
      "2047\n",
      "Train Epoch: 14 [63488/70488 (90%)]\tLoss: 0.921234\n",
      "2048\n",
      "Train Epoch: 14 [64000/70488 (91%)]\tLoss: 0.919737\n",
      "2049\n",
      "Train Epoch: 14 [64512/70488 (91%)]\tLoss: 0.917529\n",
      "2050\n",
      "Train Epoch: 14 [65024/70488 (92%)]\tLoss: 0.918690\n",
      "2051\n",
      "Train Epoch: 14 [65536/70488 (93%)]\tLoss: 0.918690\n",
      "2052\n",
      "Train Epoch: 14 [66048/70488 (93%)]\tLoss: 0.919849\n",
      "2053\n",
      "Train Epoch: 14 [66560/70488 (94%)]\tLoss: 0.918014\n",
      "2054\n",
      "Train Epoch: 14 [67072/70488 (95%)]\tLoss: 0.920443\n",
      "2055\n",
      "Train Epoch: 14 [67584/70488 (96%)]\tLoss: 0.920579\n",
      "2056\n",
      "Train Epoch: 14 [68096/70488 (96%)]\tLoss: 0.917210\n",
      "2057\n",
      "Train Epoch: 14 [68608/70488 (97%)]\tLoss: 0.918610\n",
      "2058\n",
      "Train Epoch: 14 [69120/70488 (98%)]\tLoss: 0.919705\n",
      "2059\n",
      "Train Epoch: 14 [69632/70488 (99%)]\tLoss: 0.920413\n",
      "2060\n",
      "Train Epoch: 14 [47128/70488 (99%)]\tLoss: 0.920306\n",
      "2061\n",
      "====> Epoch: 14 Average loss: 0.00360045\n",
      "====> Test set loss: 30.43650293\n",
      "Train Epoch: 15 [0/70488 (0%)]\tLoss: 0.917001\n",
      "2072\n",
      "Train Epoch: 15 [512/70488 (1%)]\tLoss: 0.919122\n",
      "2073\n",
      "Train Epoch: 15 [1024/70488 (1%)]\tLoss: 0.918275\n",
      "2074\n",
      "Train Epoch: 15 [1536/70488 (2%)]\tLoss: 0.919058\n",
      "2075\n",
      "Train Epoch: 15 [2048/70488 (3%)]\tLoss: 0.915421\n",
      "2076\n",
      "Train Epoch: 15 [2560/70488 (4%)]\tLoss: 0.919177\n",
      "2077\n",
      "Train Epoch: 15 [3072/70488 (4%)]\tLoss: 0.918679\n",
      "2078\n",
      "Train Epoch: 15 [3584/70488 (5%)]\tLoss: 0.920883\n",
      "2079\n",
      "Train Epoch: 15 [4096/70488 (6%)]\tLoss: 0.916929\n",
      "2080\n",
      "Train Epoch: 15 [4608/70488 (7%)]\tLoss: 0.918421\n",
      "2081\n",
      "Train Epoch: 15 [5120/70488 (7%)]\tLoss: 0.917843\n",
      "2082\n",
      "Train Epoch: 15 [5632/70488 (8%)]\tLoss: 0.918876\n",
      "2083\n",
      "Train Epoch: 15 [6144/70488 (9%)]\tLoss: 0.918375\n",
      "2084\n",
      "Train Epoch: 15 [6656/70488 (9%)]\tLoss: 0.916594\n",
      "2085\n",
      "Train Epoch: 15 [7168/70488 (10%)]\tLoss: 0.918309\n",
      "2086\n",
      "Train Epoch: 15 [7680/70488 (11%)]\tLoss: 0.918440\n",
      "2087\n",
      "Train Epoch: 15 [8192/70488 (12%)]\tLoss: 0.916906\n",
      "2088\n",
      "Train Epoch: 15 [8704/70488 (12%)]\tLoss: 0.919246\n",
      "2089\n",
      "Train Epoch: 15 [9216/70488 (13%)]\tLoss: 0.916210\n",
      "2090\n",
      "Train Epoch: 15 [9728/70488 (14%)]\tLoss: 0.918141\n",
      "2091\n",
      "Train Epoch: 15 [10240/70488 (14%)]\tLoss: 0.918284\n",
      "2092\n",
      "Train Epoch: 15 [10752/70488 (15%)]\tLoss: 0.918281\n",
      "2093\n",
      "Train Epoch: 15 [11264/70488 (16%)]\tLoss: 0.918283\n",
      "2094\n",
      "Train Epoch: 15 [11776/70488 (17%)]\tLoss: 0.915845\n",
      "2095\n",
      "Train Epoch: 15 [12288/70488 (17%)]\tLoss: 0.915517\n",
      "2096\n",
      "Train Epoch: 15 [12800/70488 (18%)]\tLoss: 0.917184\n",
      "2097\n",
      "Train Epoch: 15 [13312/70488 (19%)]\tLoss: 0.918053\n",
      "2098\n",
      "Train Epoch: 15 [13824/70488 (20%)]\tLoss: 0.916930\n",
      "2099\n",
      "Train Epoch: 15 [14336/70488 (20%)]\tLoss: 0.917612\n",
      "2100\n",
      "Train Epoch: 15 [14848/70488 (21%)]\tLoss: 0.917830\n",
      "2101\n",
      "Train Epoch: 15 [15360/70488 (22%)]\tLoss: 0.919114\n",
      "2102\n",
      "Train Epoch: 15 [15872/70488 (22%)]\tLoss: 0.916450\n",
      "2103\n",
      "Train Epoch: 15 [16384/70488 (23%)]\tLoss: 0.915797\n",
      "2104\n",
      "Train Epoch: 15 [16896/70488 (24%)]\tLoss: 0.915816\n",
      "2105\n",
      "Train Epoch: 15 [17408/70488 (25%)]\tLoss: 0.917035\n",
      "2106\n",
      "Train Epoch: 15 [17920/70488 (25%)]\tLoss: 0.916216\n",
      "2107\n",
      "Train Epoch: 15 [18432/70488 (26%)]\tLoss: 0.915901\n",
      "2108\n",
      "Train Epoch: 15 [18944/70488 (27%)]\tLoss: 0.917122\n",
      "2109\n",
      "Train Epoch: 15 [19456/70488 (28%)]\tLoss: 0.918957\n",
      "2110\n",
      "Train Epoch: 15 [19968/70488 (28%)]\tLoss: 0.916582\n",
      "2111\n",
      "Train Epoch: 15 [20480/70488 (29%)]\tLoss: 0.918014\n",
      "2112\n",
      "Train Epoch: 15 [20992/70488 (30%)]\tLoss: 0.917569\n",
      "2113\n",
      "Train Epoch: 15 [21504/70488 (30%)]\tLoss: 0.917020\n",
      "2114\n",
      "Train Epoch: 15 [22016/70488 (31%)]\tLoss: 0.917579\n",
      "2115\n",
      "Train Epoch: 15 [22528/70488 (32%)]\tLoss: 0.915779\n",
      "2116\n",
      "Train Epoch: 15 [23040/70488 (33%)]\tLoss: 0.916630\n",
      "2117\n",
      "Train Epoch: 15 [23552/70488 (33%)]\tLoss: 0.916606\n",
      "2118\n",
      "Train Epoch: 15 [24064/70488 (34%)]\tLoss: 0.916422\n",
      "2119\n",
      "Train Epoch: 15 [24576/70488 (35%)]\tLoss: 0.916121\n",
      "2120\n",
      "Train Epoch: 15 [25088/70488 (36%)]\tLoss: 0.918182\n",
      "2121\n",
      "Train Epoch: 15 [25600/70488 (36%)]\tLoss: 0.918378\n",
      "2122\n",
      "Train Epoch: 15 [26112/70488 (37%)]\tLoss: 0.917551\n",
      "2123\n",
      "Train Epoch: 15 [26624/70488 (38%)]\tLoss: 0.920926\n",
      "2124\n",
      "Train Epoch: 15 [27136/70488 (38%)]\tLoss: 0.918483\n",
      "2125\n",
      "Train Epoch: 15 [27648/70488 (39%)]\tLoss: 0.917198\n",
      "2126\n",
      "Train Epoch: 15 [28160/70488 (40%)]\tLoss: 0.916822\n",
      "2127\n",
      "Train Epoch: 15 [28672/70488 (41%)]\tLoss: 0.917307\n",
      "2128\n",
      "Train Epoch: 15 [29184/70488 (41%)]\tLoss: 0.916015\n",
      "2129\n",
      "Train Epoch: 15 [29696/70488 (42%)]\tLoss: 0.918437\n",
      "2130\n",
      "Train Epoch: 15 [30208/70488 (43%)]\tLoss: 0.916938\n",
      "2131\n",
      "Train Epoch: 15 [30720/70488 (43%)]\tLoss: 0.917769\n",
      "2132\n",
      "Train Epoch: 15 [31232/70488 (44%)]\tLoss: 0.919429\n",
      "2133\n",
      "Train Epoch: 15 [31744/70488 (45%)]\tLoss: 0.916634\n",
      "2134\n",
      "Train Epoch: 15 [32256/70488 (46%)]\tLoss: 0.919766\n",
      "2135\n",
      "Train Epoch: 15 [32768/70488 (46%)]\tLoss: 0.918402\n",
      "2136\n",
      "Train Epoch: 15 [33280/70488 (47%)]\tLoss: 0.917500\n",
      "2137\n",
      "Train Epoch: 15 [33792/70488 (48%)]\tLoss: 0.917059\n",
      "2138\n",
      "Train Epoch: 15 [34304/70488 (49%)]\tLoss: 0.919231\n",
      "2139\n",
      "Train Epoch: 15 [34816/70488 (49%)]\tLoss: 0.918669\n",
      "2140\n",
      "Train Epoch: 15 [35328/70488 (50%)]\tLoss: 0.918228\n",
      "2141\n",
      "Train Epoch: 15 [35840/70488 (51%)]\tLoss: 0.917623\n",
      "2142\n",
      "Train Epoch: 15 [36352/70488 (51%)]\tLoss: 0.916926\n",
      "2143\n",
      "Train Epoch: 15 [36864/70488 (52%)]\tLoss: 0.917880\n",
      "2144\n",
      "Train Epoch: 15 [37376/70488 (53%)]\tLoss: 0.916071\n",
      "2145\n",
      "Train Epoch: 15 [37888/70488 (54%)]\tLoss: 0.915913\n",
      "2146\n",
      "Train Epoch: 15 [38400/70488 (54%)]\tLoss: 0.918648\n",
      "2147\n",
      "Train Epoch: 15 [38912/70488 (55%)]\tLoss: 0.918397\n",
      "2148\n",
      "Train Epoch: 15 [39424/70488 (56%)]\tLoss: 0.919124\n",
      "2149\n",
      "Train Epoch: 15 [39936/70488 (57%)]\tLoss: 0.917306\n",
      "2150\n",
      "Train Epoch: 15 [40448/70488 (57%)]\tLoss: 0.918419\n",
      "2151\n",
      "Train Epoch: 15 [40960/70488 (58%)]\tLoss: 0.916499\n",
      "2152\n",
      "Train Epoch: 15 [41472/70488 (59%)]\tLoss: 0.920516\n",
      "2153\n",
      "Train Epoch: 15 [41984/70488 (59%)]\tLoss: 0.918586\n",
      "2154\n",
      "Train Epoch: 15 [42496/70488 (60%)]\tLoss: 0.915872\n",
      "2155\n",
      "Train Epoch: 15 [43008/70488 (61%)]\tLoss: 0.918278\n",
      "2156\n",
      "Train Epoch: 15 [43520/70488 (62%)]\tLoss: 0.915934\n",
      "2157\n",
      "Train Epoch: 15 [44032/70488 (62%)]\tLoss: 0.917369\n",
      "2158\n",
      "Train Epoch: 15 [44544/70488 (63%)]\tLoss: 0.916434\n",
      "2159\n",
      "Train Epoch: 15 [45056/70488 (64%)]\tLoss: 0.919630\n",
      "2160\n",
      "Train Epoch: 15 [45568/70488 (64%)]\tLoss: 0.918986\n",
      "2161\n",
      "Train Epoch: 15 [46080/70488 (65%)]\tLoss: 0.916820\n",
      "2162\n",
      "Train Epoch: 15 [46592/70488 (66%)]\tLoss: 0.916384\n",
      "2163\n",
      "Train Epoch: 15 [47104/70488 (67%)]\tLoss: 0.917564\n",
      "2164\n",
      "Train Epoch: 15 [47616/70488 (67%)]\tLoss: 0.916303\n",
      "2165\n",
      "Train Epoch: 15 [48128/70488 (68%)]\tLoss: 0.917770\n",
      "2166\n",
      "Train Epoch: 15 [48640/70488 (69%)]\tLoss: 0.915288\n",
      "2167\n",
      "Train Epoch: 15 [49152/70488 (70%)]\tLoss: 0.917549\n",
      "2168\n",
      "Train Epoch: 15 [49664/70488 (70%)]\tLoss: 0.917665\n",
      "2169\n",
      "Train Epoch: 15 [50176/70488 (71%)]\tLoss: 0.916130\n",
      "2170\n",
      "Train Epoch: 15 [50688/70488 (72%)]\tLoss: 0.919972\n",
      "2171\n",
      "Train Epoch: 15 [51200/70488 (72%)]\tLoss: 0.919004\n",
      "2172\n",
      "Train Epoch: 15 [51712/70488 (73%)]\tLoss: 0.915930\n",
      "2173\n",
      "Train Epoch: 15 [52224/70488 (74%)]\tLoss: 0.917065\n",
      "2174\n",
      "Train Epoch: 15 [52736/70488 (75%)]\tLoss: 0.914756\n",
      "2175\n",
      "Train Epoch: 15 [53248/70488 (75%)]\tLoss: 0.917386\n",
      "2176\n",
      "Train Epoch: 15 [53760/70488 (76%)]\tLoss: 0.916643\n",
      "2177\n",
      "Train Epoch: 15 [54272/70488 (77%)]\tLoss: 0.916386\n",
      "2178\n",
      "Train Epoch: 15 [54784/70488 (78%)]\tLoss: 0.916428\n",
      "2179\n",
      "Train Epoch: 15 [55296/70488 (78%)]\tLoss: 0.918116\n",
      "2180\n",
      "Train Epoch: 15 [55808/70488 (79%)]\tLoss: 0.917815\n",
      "2181\n",
      "Train Epoch: 15 [56320/70488 (80%)]\tLoss: 0.918336\n",
      "2182\n",
      "Train Epoch: 15 [56832/70488 (80%)]\tLoss: 0.916747\n",
      "2183\n",
      "Train Epoch: 15 [57344/70488 (81%)]\tLoss: 0.920142\n",
      "2184\n",
      "Train Epoch: 15 [57856/70488 (82%)]\tLoss: 0.917552\n",
      "2185\n",
      "Train Epoch: 15 [58368/70488 (83%)]\tLoss: 0.920594\n",
      "2186\n",
      "Train Epoch: 15 [58880/70488 (83%)]\tLoss: 0.920219\n",
      "2187\n",
      "Train Epoch: 15 [59392/70488 (84%)]\tLoss: 0.918926\n",
      "2188\n",
      "Train Epoch: 15 [59904/70488 (85%)]\tLoss: 0.920084\n",
      "2189\n",
      "Train Epoch: 15 [60416/70488 (86%)]\tLoss: 0.917827\n",
      "2190\n",
      "Train Epoch: 15 [60928/70488 (86%)]\tLoss: 0.915397\n",
      "2191\n",
      "Train Epoch: 15 [61440/70488 (87%)]\tLoss: 0.918818\n",
      "2192\n",
      "Train Epoch: 15 [61952/70488 (88%)]\tLoss: 0.918661\n",
      "2193\n",
      "Train Epoch: 15 [62464/70488 (88%)]\tLoss: 0.918309\n",
      "2194\n",
      "Train Epoch: 15 [62976/70488 (89%)]\tLoss: 0.918001\n",
      "2195\n",
      "Train Epoch: 15 [63488/70488 (90%)]\tLoss: 0.919154\n",
      "2196\n",
      "Train Epoch: 15 [64000/70488 (91%)]\tLoss: 0.917690\n",
      "2197\n",
      "Train Epoch: 15 [64512/70488 (91%)]\tLoss: 0.919759\n",
      "2198\n",
      "Train Epoch: 15 [65024/70488 (92%)]\tLoss: 0.916307\n",
      "2199\n",
      "Train Epoch: 15 [65536/70488 (93%)]\tLoss: 0.920299\n",
      "2200\n",
      "Train Epoch: 15 [66048/70488 (93%)]\tLoss: 0.919676\n",
      "2201\n",
      "Train Epoch: 15 [66560/70488 (94%)]\tLoss: 0.917430\n",
      "2202\n",
      "Train Epoch: 15 [67072/70488 (95%)]\tLoss: 0.918796\n",
      "2203\n",
      "Train Epoch: 15 [67584/70488 (96%)]\tLoss: 0.920235\n",
      "2204\n",
      "Train Epoch: 15 [68096/70488 (96%)]\tLoss: 0.920815\n",
      "2205\n",
      "Train Epoch: 15 [68608/70488 (97%)]\tLoss: 0.920376\n",
      "2206\n",
      "Train Epoch: 15 [69120/70488 (98%)]\tLoss: 0.918320\n",
      "2207\n",
      "Train Epoch: 15 [69632/70488 (99%)]\tLoss: 0.918184\n",
      "2208\n",
      "Train Epoch: 15 [47128/70488 (99%)]\tLoss: 0.917956\n",
      "2209\n",
      "====> Epoch: 15 Average loss: 0.00359367\n",
      "====> Test set loss: 30.42435646\n",
      "Train Epoch: 16 [0/70488 (0%)]\tLoss: 0.917295\n",
      "2220\n",
      "Train Epoch: 16 [512/70488 (1%)]\tLoss: 0.916631\n",
      "2221\n",
      "Train Epoch: 16 [1024/70488 (1%)]\tLoss: 0.916229\n",
      "2222\n",
      "Train Epoch: 16 [1536/70488 (2%)]\tLoss: 0.917254\n",
      "2223\n",
      "Train Epoch: 16 [2048/70488 (3%)]\tLoss: 0.916281\n",
      "2224\n",
      "Train Epoch: 16 [2560/70488 (4%)]\tLoss: 0.914933\n",
      "2225\n",
      "Train Epoch: 16 [3072/70488 (4%)]\tLoss: 0.916692\n",
      "2226\n",
      "Train Epoch: 16 [3584/70488 (5%)]\tLoss: 0.917556\n",
      "2227\n",
      "Train Epoch: 16 [4096/70488 (6%)]\tLoss: 0.916517\n",
      "2228\n",
      "Train Epoch: 16 [4608/70488 (7%)]\tLoss: 0.914205\n",
      "2229\n",
      "Train Epoch: 16 [5120/70488 (7%)]\tLoss: 0.913975\n",
      "2230\n",
      "Train Epoch: 16 [5632/70488 (8%)]\tLoss: 0.918991\n",
      "2231\n",
      "Train Epoch: 16 [6144/70488 (9%)]\tLoss: 0.917594\n",
      "2232\n",
      "Train Epoch: 16 [6656/70488 (9%)]\tLoss: 0.915658\n",
      "2233\n",
      "Train Epoch: 16 [7168/70488 (10%)]\tLoss: 0.916824\n",
      "2234\n",
      "Train Epoch: 16 [7680/70488 (11%)]\tLoss: 0.915037\n",
      "2235\n",
      "Train Epoch: 16 [8192/70488 (12%)]\tLoss: 0.917827\n",
      "2236\n",
      "Train Epoch: 16 [8704/70488 (12%)]\tLoss: 0.917427\n",
      "2237\n",
      "Train Epoch: 16 [9216/70488 (13%)]\tLoss: 0.917992\n",
      "2238\n",
      "Train Epoch: 16 [9728/70488 (14%)]\tLoss: 0.917537\n",
      "2239\n",
      "Train Epoch: 16 [10240/70488 (14%)]\tLoss: 0.917121\n",
      "2240\n",
      "Train Epoch: 16 [10752/70488 (15%)]\tLoss: 0.913955\n",
      "2241\n",
      "Train Epoch: 16 [11264/70488 (16%)]\tLoss: 0.916103\n",
      "2242\n",
      "Train Epoch: 16 [11776/70488 (17%)]\tLoss: 0.917008\n",
      "2243\n",
      "Train Epoch: 16 [12288/70488 (17%)]\tLoss: 0.918163\n",
      "2244\n",
      "Train Epoch: 16 [12800/70488 (18%)]\tLoss: 0.918788\n",
      "2245\n",
      "Train Epoch: 16 [13312/70488 (19%)]\tLoss: 0.917219\n",
      "2246\n",
      "Train Epoch: 16 [13824/70488 (20%)]\tLoss: 0.917742\n",
      "2247\n",
      "Train Epoch: 16 [14336/70488 (20%)]\tLoss: 0.918224\n",
      "2248\n",
      "Train Epoch: 16 [14848/70488 (21%)]\tLoss: 0.917541\n",
      "2249\n",
      "Train Epoch: 16 [15360/70488 (22%)]\tLoss: 0.915506\n",
      "2250\n",
      "Train Epoch: 16 [15872/70488 (22%)]\tLoss: 0.916189\n",
      "2251\n",
      "Train Epoch: 16 [16384/70488 (23%)]\tLoss: 0.915392\n",
      "2252\n",
      "Train Epoch: 16 [16896/70488 (24%)]\tLoss: 0.915420\n",
      "2253\n",
      "Train Epoch: 16 [17408/70488 (25%)]\tLoss: 0.915137\n",
      "2254\n",
      "Train Epoch: 16 [17920/70488 (25%)]\tLoss: 0.919273\n",
      "2255\n",
      "Train Epoch: 16 [18432/70488 (26%)]\tLoss: 0.916124\n",
      "2256\n",
      "Train Epoch: 16 [18944/70488 (27%)]\tLoss: 0.915614\n",
      "2257\n",
      "Train Epoch: 16 [19456/70488 (28%)]\tLoss: 0.916729\n",
      "2258\n",
      "Train Epoch: 16 [19968/70488 (28%)]\tLoss: 0.916071\n",
      "2259\n",
      "Train Epoch: 16 [20480/70488 (29%)]\tLoss: 0.914365\n",
      "2260\n",
      "Train Epoch: 16 [20992/70488 (30%)]\tLoss: 0.917721\n",
      "2261\n",
      "Train Epoch: 16 [21504/70488 (30%)]\tLoss: 0.915686\n",
      "2262\n",
      "Train Epoch: 16 [22016/70488 (31%)]\tLoss: 0.917798\n",
      "2263\n",
      "Train Epoch: 16 [22528/70488 (32%)]\tLoss: 0.916813\n",
      "2264\n",
      "Train Epoch: 16 [23040/70488 (33%)]\tLoss: 0.917860\n",
      "2265\n",
      "Train Epoch: 16 [23552/70488 (33%)]\tLoss: 0.915689\n",
      "2266\n",
      "Train Epoch: 16 [24064/70488 (34%)]\tLoss: 0.912941\n",
      "2267\n",
      "Train Epoch: 16 [24576/70488 (35%)]\tLoss: 0.915512\n",
      "2268\n",
      "Train Epoch: 16 [25088/70488 (36%)]\tLoss: 0.915224\n",
      "2269\n",
      "Train Epoch: 16 [25600/70488 (36%)]\tLoss: 0.915532\n",
      "2270\n",
      "Train Epoch: 16 [26112/70488 (37%)]\tLoss: 0.914743\n",
      "2271\n",
      "Train Epoch: 16 [26624/70488 (38%)]\tLoss: 0.915091\n",
      "2272\n",
      "Train Epoch: 16 [27136/70488 (38%)]\tLoss: 0.915226\n",
      "2273\n",
      "Train Epoch: 16 [27648/70488 (39%)]\tLoss: 0.915242\n",
      "2274\n",
      "Train Epoch: 16 [28160/70488 (40%)]\tLoss: 0.914839\n",
      "2275\n",
      "Train Epoch: 16 [28672/70488 (41%)]\tLoss: 0.916200\n",
      "2276\n",
      "Train Epoch: 16 [29184/70488 (41%)]\tLoss: 0.915842\n",
      "2277\n",
      "Train Epoch: 16 [29696/70488 (42%)]\tLoss: 0.915754\n",
      "2278\n",
      "Train Epoch: 16 [30208/70488 (43%)]\tLoss: 0.917600\n",
      "2279\n",
      "Train Epoch: 16 [30720/70488 (43%)]\tLoss: 0.915899\n",
      "2280\n",
      "Train Epoch: 16 [31232/70488 (44%)]\tLoss: 0.918783\n",
      "2281\n",
      "Train Epoch: 16 [31744/70488 (45%)]\tLoss: 0.914698\n",
      "2282\n",
      "Train Epoch: 16 [32256/70488 (46%)]\tLoss: 0.916196\n",
      "2283\n",
      "Train Epoch: 16 [32768/70488 (46%)]\tLoss: 0.915189\n",
      "2284\n",
      "Train Epoch: 16 [33280/70488 (47%)]\tLoss: 0.914916\n",
      "2285\n",
      "Train Epoch: 16 [33792/70488 (48%)]\tLoss: 0.919558\n",
      "2286\n",
      "Train Epoch: 16 [34304/70488 (49%)]\tLoss: 0.916386\n",
      "2287\n",
      "Train Epoch: 16 [34816/70488 (49%)]\tLoss: 0.916558\n",
      "2288\n",
      "Train Epoch: 16 [35328/70488 (50%)]\tLoss: 0.917424\n",
      "2289\n",
      "Train Epoch: 16 [35840/70488 (51%)]\tLoss: 0.915685\n",
      "2290\n",
      "Train Epoch: 16 [36352/70488 (51%)]\tLoss: 0.916630\n",
      "2291\n",
      "Train Epoch: 16 [36864/70488 (52%)]\tLoss: 0.918377\n",
      "2292\n",
      "Train Epoch: 16 [37376/70488 (53%)]\tLoss: 0.915456\n",
      "2293\n",
      "Train Epoch: 16 [37888/70488 (54%)]\tLoss: 0.916375\n",
      "2294\n",
      "Train Epoch: 16 [38400/70488 (54%)]\tLoss: 0.918907\n",
      "2295\n",
      "Train Epoch: 16 [38912/70488 (55%)]\tLoss: 0.917130\n",
      "2296\n",
      "Train Epoch: 16 [39424/70488 (56%)]\tLoss: 0.916012\n",
      "2297\n",
      "Train Epoch: 16 [39936/70488 (57%)]\tLoss: 0.917223\n",
      "2298\n",
      "Train Epoch: 16 [40448/70488 (57%)]\tLoss: 0.918812\n",
      "2299\n",
      "Train Epoch: 16 [40960/70488 (58%)]\tLoss: 0.915220\n",
      "2300\n",
      "Train Epoch: 16 [41472/70488 (59%)]\tLoss: 0.915342\n",
      "2301\n",
      "Train Epoch: 16 [41984/70488 (59%)]\tLoss: 0.916807\n",
      "2302\n",
      "Train Epoch: 16 [42496/70488 (60%)]\tLoss: 0.916303\n",
      "2303\n",
      "Train Epoch: 16 [43008/70488 (61%)]\tLoss: 0.917336\n",
      "2304\n",
      "Train Epoch: 16 [43520/70488 (62%)]\tLoss: 0.916386\n",
      "2305\n",
      "Train Epoch: 16 [44032/70488 (62%)]\tLoss: 0.917046\n",
      "2306\n",
      "Train Epoch: 16 [44544/70488 (63%)]\tLoss: 0.916693\n",
      "2307\n",
      "Train Epoch: 16 [45056/70488 (64%)]\tLoss: 0.915711\n",
      "2308\n",
      "Train Epoch: 16 [45568/70488 (64%)]\tLoss: 0.917732\n",
      "2309\n",
      "Train Epoch: 16 [46080/70488 (65%)]\tLoss: 0.916054\n",
      "2310\n",
      "Train Epoch: 16 [46592/70488 (66%)]\tLoss: 0.918274\n",
      "2311\n",
      "Train Epoch: 16 [47104/70488 (67%)]\tLoss: 0.918941\n",
      "2312\n",
      "Train Epoch: 16 [47616/70488 (67%)]\tLoss: 0.916183\n",
      "2313\n",
      "Train Epoch: 16 [48128/70488 (68%)]\tLoss: 0.915165\n",
      "2314\n",
      "Train Epoch: 16 [48640/70488 (69%)]\tLoss: 0.915210\n",
      "2315\n",
      "Train Epoch: 16 [49152/70488 (70%)]\tLoss: 0.915636\n",
      "2316\n",
      "Train Epoch: 16 [49664/70488 (70%)]\tLoss: 0.916807\n",
      "2317\n",
      "Train Epoch: 16 [50176/70488 (71%)]\tLoss: 0.915427\n",
      "2318\n",
      "Train Epoch: 16 [50688/70488 (72%)]\tLoss: 0.916823\n",
      "2319\n",
      "Train Epoch: 16 [51200/70488 (72%)]\tLoss: 0.916114\n",
      "2320\n",
      "Train Epoch: 16 [51712/70488 (73%)]\tLoss: 0.915413\n",
      "2321\n",
      "Train Epoch: 16 [52224/70488 (74%)]\tLoss: 0.914694\n",
      "2322\n",
      "Train Epoch: 16 [52736/70488 (75%)]\tLoss: 0.916141\n",
      "2323\n",
      "Train Epoch: 16 [53248/70488 (75%)]\tLoss: 0.915237\n",
      "2324\n",
      "Train Epoch: 16 [53760/70488 (76%)]\tLoss: 0.919440\n",
      "2325\n",
      "Train Epoch: 16 [54272/70488 (77%)]\tLoss: 0.916789\n",
      "2326\n",
      "Train Epoch: 16 [54784/70488 (78%)]\tLoss: 0.914687\n",
      "2327\n",
      "Train Epoch: 16 [55296/70488 (78%)]\tLoss: 0.917577\n",
      "2328\n",
      "Train Epoch: 16 [55808/70488 (79%)]\tLoss: 0.918287\n",
      "2329\n",
      "Train Epoch: 16 [56320/70488 (80%)]\tLoss: 0.917647\n",
      "2330\n",
      "Train Epoch: 16 [56832/70488 (80%)]\tLoss: 0.915980\n",
      "2331\n",
      "Train Epoch: 16 [57344/70488 (81%)]\tLoss: 0.916865\n",
      "2332\n",
      "Train Epoch: 16 [57856/70488 (82%)]\tLoss: 0.916244\n",
      "2333\n",
      "Train Epoch: 16 [58368/70488 (83%)]\tLoss: 0.916340\n",
      "2334\n",
      "Train Epoch: 16 [58880/70488 (83%)]\tLoss: 0.916112\n",
      "2335\n",
      "Train Epoch: 16 [59392/70488 (84%)]\tLoss: 0.914883\n",
      "2336\n",
      "Train Epoch: 16 [59904/70488 (85%)]\tLoss: 0.918537\n",
      "2337\n",
      "Train Epoch: 16 [60416/70488 (86%)]\tLoss: 0.917809\n",
      "2338\n",
      "Train Epoch: 16 [60928/70488 (86%)]\tLoss: 0.917066\n",
      "2339\n",
      "Train Epoch: 16 [61440/70488 (87%)]\tLoss: 0.916565\n",
      "2340\n",
      "Train Epoch: 16 [61952/70488 (88%)]\tLoss: 0.917998\n",
      "2341\n",
      "Train Epoch: 16 [62464/70488 (88%)]\tLoss: 0.916359\n",
      "2342\n",
      "Train Epoch: 16 [62976/70488 (89%)]\tLoss: 0.916416\n",
      "2343\n",
      "Train Epoch: 16 [63488/70488 (90%)]\tLoss: 0.916762\n",
      "2344\n",
      "Train Epoch: 16 [64000/70488 (91%)]\tLoss: 0.920097\n",
      "2345\n",
      "Train Epoch: 16 [64512/70488 (91%)]\tLoss: 0.915266\n",
      "2346\n",
      "Train Epoch: 16 [65024/70488 (92%)]\tLoss: 0.915262\n",
      "2347\n",
      "Train Epoch: 16 [65536/70488 (93%)]\tLoss: 0.917599\n",
      "2348\n",
      "Train Epoch: 16 [66048/70488 (93%)]\tLoss: 0.916167\n",
      "2349\n",
      "Train Epoch: 16 [66560/70488 (94%)]\tLoss: 0.917233\n",
      "2350\n",
      "Train Epoch: 16 [67072/70488 (95%)]\tLoss: 0.918175\n",
      "2351\n",
      "Train Epoch: 16 [67584/70488 (96%)]\tLoss: 0.916565\n",
      "2352\n",
      "Train Epoch: 16 [68096/70488 (96%)]\tLoss: 0.919164\n",
      "2353\n",
      "Train Epoch: 16 [68608/70488 (97%)]\tLoss: 0.915698\n",
      "2354\n",
      "Train Epoch: 16 [69120/70488 (98%)]\tLoss: 0.917339\n",
      "2355\n",
      "Train Epoch: 16 [69632/70488 (99%)]\tLoss: 0.915296\n",
      "2356\n",
      "Train Epoch: 16 [47128/70488 (99%)]\tLoss: 0.913579\n",
      "2357\n",
      "====> Epoch: 16 Average loss: 0.00358870\n",
      "====> Test set loss: 30.43104506\n",
      "Train Epoch: 17 [0/70488 (0%)]\tLoss: 0.914458\n",
      "2368\n",
      "Train Epoch: 17 [512/70488 (1%)]\tLoss: 0.916841\n",
      "2369\n",
      "Train Epoch: 17 [1024/70488 (1%)]\tLoss: 0.915642\n",
      "2370\n",
      "Train Epoch: 17 [1536/70488 (2%)]\tLoss: 0.913582\n",
      "2371\n",
      "Train Epoch: 17 [2048/70488 (3%)]\tLoss: 0.914445\n",
      "2372\n",
      "Train Epoch: 17 [2560/70488 (4%)]\tLoss: 0.916889\n",
      "2373\n",
      "Train Epoch: 17 [3072/70488 (4%)]\tLoss: 0.914988\n",
      "2374\n",
      "Train Epoch: 17 [3584/70488 (5%)]\tLoss: 0.915078\n",
      "2375\n",
      "Train Epoch: 17 [4096/70488 (6%)]\tLoss: 0.914847\n",
      "2376\n",
      "Train Epoch: 17 [4608/70488 (7%)]\tLoss: 0.915817\n",
      "2377\n",
      "Train Epoch: 17 [5120/70488 (7%)]\tLoss: 0.915122\n",
      "2378\n",
      "Train Epoch: 17 [5632/70488 (8%)]\tLoss: 0.915812\n",
      "2379\n",
      "Train Epoch: 17 [6144/70488 (9%)]\tLoss: 0.913534\n",
      "2380\n",
      "Train Epoch: 17 [6656/70488 (9%)]\tLoss: 0.915661\n",
      "2381\n",
      "Train Epoch: 17 [7168/70488 (10%)]\tLoss: 0.916430\n",
      "2382\n",
      "Train Epoch: 17 [7680/70488 (11%)]\tLoss: 0.917838\n",
      "2383\n",
      "Train Epoch: 17 [8192/70488 (12%)]\tLoss: 0.916669\n",
      "2384\n",
      "Train Epoch: 17 [8704/70488 (12%)]\tLoss: 0.914177\n",
      "2385\n",
      "Train Epoch: 17 [9216/70488 (13%)]\tLoss: 0.915359\n",
      "2386\n",
      "Train Epoch: 17 [9728/70488 (14%)]\tLoss: 0.915641\n",
      "2387\n",
      "Train Epoch: 17 [10240/70488 (14%)]\tLoss: 0.916167\n",
      "2388\n",
      "Train Epoch: 17 [10752/70488 (15%)]\tLoss: 0.914529\n",
      "2389\n",
      "Train Epoch: 17 [11264/70488 (16%)]\tLoss: 0.912893\n",
      "2390\n",
      "Train Epoch: 17 [11776/70488 (17%)]\tLoss: 0.916251\n",
      "2391\n",
      "Train Epoch: 17 [12288/70488 (17%)]\tLoss: 0.916006\n",
      "2392\n",
      "Train Epoch: 17 [12800/70488 (18%)]\tLoss: 0.916688\n",
      "2393\n",
      "Train Epoch: 17 [13312/70488 (19%)]\tLoss: 0.913366\n",
      "2394\n",
      "Train Epoch: 17 [13824/70488 (20%)]\tLoss: 0.915228\n",
      "2395\n",
      "Train Epoch: 17 [14336/70488 (20%)]\tLoss: 0.916951\n",
      "2396\n",
      "Train Epoch: 17 [14848/70488 (21%)]\tLoss: 0.915700\n",
      "2397\n",
      "Train Epoch: 17 [15360/70488 (22%)]\tLoss: 0.915383\n",
      "2398\n",
      "Train Epoch: 17 [15872/70488 (22%)]\tLoss: 0.914051\n",
      "2399\n",
      "Train Epoch: 17 [16384/70488 (23%)]\tLoss: 0.915153\n",
      "2400\n",
      "Train Epoch: 17 [16896/70488 (24%)]\tLoss: 0.914735\n",
      "2401\n",
      "Train Epoch: 17 [17408/70488 (25%)]\tLoss: 0.914009\n",
      "2402\n",
      "Train Epoch: 17 [17920/70488 (25%)]\tLoss: 0.915455\n",
      "2403\n",
      "Train Epoch: 17 [18432/70488 (26%)]\tLoss: 0.916021\n",
      "2404\n",
      "Train Epoch: 17 [18944/70488 (27%)]\tLoss: 0.913097\n",
      "2405\n",
      "Train Epoch: 17 [19456/70488 (28%)]\tLoss: 0.914688\n",
      "2406\n",
      "Train Epoch: 17 [19968/70488 (28%)]\tLoss: 0.914321\n",
      "2407\n",
      "Train Epoch: 17 [20480/70488 (29%)]\tLoss: 0.914741\n",
      "2408\n",
      "Train Epoch: 17 [20992/70488 (30%)]\tLoss: 0.916782\n",
      "2409\n",
      "Train Epoch: 17 [21504/70488 (30%)]\tLoss: 0.918459\n",
      "2410\n",
      "Train Epoch: 17 [22016/70488 (31%)]\tLoss: 0.915959\n",
      "2411\n",
      "Train Epoch: 17 [22528/70488 (32%)]\tLoss: 0.916069\n",
      "2412\n",
      "Train Epoch: 17 [23040/70488 (33%)]\tLoss: 0.915594\n",
      "2413\n",
      "Train Epoch: 17 [23552/70488 (33%)]\tLoss: 0.914959\n",
      "2414\n",
      "Train Epoch: 17 [24064/70488 (34%)]\tLoss: 0.914852\n",
      "2415\n",
      "Train Epoch: 17 [24576/70488 (35%)]\tLoss: 0.916860\n",
      "2416\n",
      "Train Epoch: 17 [25088/70488 (36%)]\tLoss: 0.912342\n",
      "2417\n",
      "Train Epoch: 17 [25600/70488 (36%)]\tLoss: 0.915541\n",
      "2418\n",
      "Train Epoch: 17 [26112/70488 (37%)]\tLoss: 0.914632\n",
      "2419\n",
      "Train Epoch: 17 [26624/70488 (38%)]\tLoss: 0.915192\n",
      "2420\n",
      "Train Epoch: 17 [27136/70488 (38%)]\tLoss: 0.914039\n",
      "2421\n",
      "Train Epoch: 17 [27648/70488 (39%)]\tLoss: 0.914447\n",
      "2422\n",
      "Train Epoch: 17 [28160/70488 (40%)]\tLoss: 0.915505\n",
      "2423\n",
      "Train Epoch: 17 [28672/70488 (41%)]\tLoss: 0.915757\n",
      "2424\n",
      "Train Epoch: 17 [29184/70488 (41%)]\tLoss: 0.912788\n",
      "2425\n",
      "Train Epoch: 17 [29696/70488 (42%)]\tLoss: 0.915045\n",
      "2426\n",
      "Train Epoch: 17 [30208/70488 (43%)]\tLoss: 0.914239\n",
      "2427\n",
      "Train Epoch: 17 [30720/70488 (43%)]\tLoss: 0.917826\n",
      "2428\n",
      "Train Epoch: 17 [31232/70488 (44%)]\tLoss: 0.916336\n",
      "2429\n",
      "Train Epoch: 17 [31744/70488 (45%)]\tLoss: 0.913061\n",
      "2430\n",
      "Train Epoch: 17 [32256/70488 (46%)]\tLoss: 0.913917\n",
      "2431\n",
      "Train Epoch: 17 [32768/70488 (46%)]\tLoss: 0.913951\n",
      "2432\n",
      "Train Epoch: 17 [33280/70488 (47%)]\tLoss: 0.914815\n",
      "2433\n",
      "Train Epoch: 17 [33792/70488 (48%)]\tLoss: 0.913905\n",
      "2434\n",
      "Train Epoch: 17 [34304/70488 (49%)]\tLoss: 0.914802\n",
      "2435\n",
      "Train Epoch: 17 [34816/70488 (49%)]\tLoss: 0.913796\n",
      "2436\n",
      "Train Epoch: 17 [35328/70488 (50%)]\tLoss: 0.916851\n",
      "2437\n",
      "Train Epoch: 17 [35840/70488 (51%)]\tLoss: 0.913257\n",
      "2438\n",
      "Train Epoch: 17 [36352/70488 (51%)]\tLoss: 0.914668\n",
      "2439\n",
      "Train Epoch: 17 [36864/70488 (52%)]\tLoss: 0.916673\n",
      "2440\n",
      "Train Epoch: 17 [37376/70488 (53%)]\tLoss: 0.916944\n",
      "2441\n",
      "Train Epoch: 17 [37888/70488 (54%)]\tLoss: 0.915321\n",
      "2442\n",
      "Train Epoch: 17 [38400/70488 (54%)]\tLoss: 0.915071\n",
      "2443\n",
      "Train Epoch: 17 [38912/70488 (55%)]\tLoss: 0.913462\n",
      "2444\n",
      "Train Epoch: 17 [39424/70488 (56%)]\tLoss: 0.916934\n",
      "2445\n",
      "Train Epoch: 17 [39936/70488 (57%)]\tLoss: 0.916592\n",
      "2446\n",
      "Train Epoch: 17 [40448/70488 (57%)]\tLoss: 0.917737\n",
      "2447\n",
      "Train Epoch: 17 [40960/70488 (58%)]\tLoss: 0.915378\n",
      "2448\n",
      "Train Epoch: 17 [41472/70488 (59%)]\tLoss: 0.915947\n",
      "2449\n",
      "Train Epoch: 17 [41984/70488 (59%)]\tLoss: 0.914521\n",
      "2450\n",
      "Train Epoch: 17 [42496/70488 (60%)]\tLoss: 0.914643\n",
      "2451\n",
      "Train Epoch: 17 [43008/70488 (61%)]\tLoss: 0.914630\n",
      "2452\n",
      "Train Epoch: 17 [43520/70488 (62%)]\tLoss: 0.913823\n",
      "2453\n",
      "Train Epoch: 17 [44032/70488 (62%)]\tLoss: 0.918687\n",
      "2454\n",
      "Train Epoch: 17 [44544/70488 (63%)]\tLoss: 0.915050\n",
      "2455\n",
      "Train Epoch: 17 [45056/70488 (64%)]\tLoss: 0.918787\n",
      "2456\n",
      "Train Epoch: 17 [45568/70488 (64%)]\tLoss: 0.914506\n",
      "2457\n",
      "Train Epoch: 17 [46080/70488 (65%)]\tLoss: 0.916861\n",
      "2458\n",
      "Train Epoch: 17 [46592/70488 (66%)]\tLoss: 0.914071\n",
      "2459\n",
      "Train Epoch: 17 [47104/70488 (67%)]\tLoss: 0.917672\n",
      "2460\n",
      "Train Epoch: 17 [47616/70488 (67%)]\tLoss: 0.915617\n",
      "2461\n",
      "Train Epoch: 17 [48128/70488 (68%)]\tLoss: 0.915708\n",
      "2462\n",
      "Train Epoch: 17 [48640/70488 (69%)]\tLoss: 0.916123\n",
      "2463\n",
      "Train Epoch: 17 [49152/70488 (70%)]\tLoss: 0.915549\n",
      "2464\n",
      "Train Epoch: 17 [49664/70488 (70%)]\tLoss: 0.913324\n",
      "2465\n",
      "Train Epoch: 17 [50176/70488 (71%)]\tLoss: 0.913985\n",
      "2466\n",
      "Train Epoch: 17 [50688/70488 (72%)]\tLoss: 0.914164\n",
      "2467\n",
      "Train Epoch: 17 [51200/70488 (72%)]\tLoss: 0.915490\n",
      "2468\n",
      "Train Epoch: 17 [51712/70488 (73%)]\tLoss: 0.915030\n",
      "2469\n",
      "Train Epoch: 17 [52224/70488 (74%)]\tLoss: 0.915681\n",
      "2470\n",
      "Train Epoch: 17 [52736/70488 (75%)]\tLoss: 0.912344\n",
      "2471\n",
      "Train Epoch: 17 [53248/70488 (75%)]\tLoss: 0.915292\n",
      "2472\n",
      "Train Epoch: 17 [53760/70488 (76%)]\tLoss: 0.916048\n",
      "2473\n",
      "Train Epoch: 17 [54272/70488 (77%)]\tLoss: 0.914961\n",
      "2474\n",
      "Train Epoch: 17 [54784/70488 (78%)]\tLoss: 0.916166\n",
      "2475\n",
      "Train Epoch: 17 [55296/70488 (78%)]\tLoss: 0.914887\n",
      "2476\n",
      "Train Epoch: 17 [55808/70488 (79%)]\tLoss: 0.914770\n",
      "2477\n",
      "Train Epoch: 17 [56320/70488 (80%)]\tLoss: 0.915206\n",
      "2478\n",
      "Train Epoch: 17 [56832/70488 (80%)]\tLoss: 0.913475\n",
      "2479\n",
      "Train Epoch: 17 [57344/70488 (81%)]\tLoss: 0.915526\n",
      "2480\n",
      "Train Epoch: 17 [57856/70488 (82%)]\tLoss: 0.917302\n",
      "2481\n",
      "Train Epoch: 17 [58368/70488 (83%)]\tLoss: 0.915638\n",
      "2482\n",
      "Train Epoch: 17 [58880/70488 (83%)]\tLoss: 0.916684\n",
      "2483\n",
      "Train Epoch: 17 [59392/70488 (84%)]\tLoss: 0.914588\n",
      "2484\n",
      "Train Epoch: 17 [59904/70488 (85%)]\tLoss: 0.914370\n",
      "2485\n",
      "Train Epoch: 17 [60416/70488 (86%)]\tLoss: 0.915011\n",
      "2486\n",
      "Train Epoch: 17 [60928/70488 (86%)]\tLoss: 0.918539\n",
      "2487\n",
      "Train Epoch: 17 [61440/70488 (87%)]\tLoss: 0.916792\n",
      "2488\n",
      "Train Epoch: 17 [61952/70488 (88%)]\tLoss: 0.914709\n",
      "2489\n",
      "Train Epoch: 17 [62464/70488 (88%)]\tLoss: 0.916540\n",
      "2490\n",
      "Train Epoch: 17 [62976/70488 (89%)]\tLoss: 0.916038\n",
      "2491\n",
      "Train Epoch: 17 [63488/70488 (90%)]\tLoss: 0.915273\n",
      "2492\n",
      "Train Epoch: 17 [64000/70488 (91%)]\tLoss: 0.916784\n",
      "2493\n",
      "Train Epoch: 17 [64512/70488 (91%)]\tLoss: 0.913392\n",
      "2494\n",
      "Train Epoch: 17 [65024/70488 (92%)]\tLoss: 0.914087\n",
      "2495\n",
      "Train Epoch: 17 [65536/70488 (93%)]\tLoss: 0.916389\n",
      "2496\n",
      "Train Epoch: 17 [66048/70488 (93%)]\tLoss: 0.915699\n",
      "2497\n",
      "Train Epoch: 17 [66560/70488 (94%)]\tLoss: 0.916431\n",
      "2498\n",
      "Train Epoch: 17 [67072/70488 (95%)]\tLoss: 0.916147\n",
      "2499\n",
      "Train Epoch: 17 [67584/70488 (96%)]\tLoss: 0.916089\n",
      "2500\n",
      "Train Epoch: 17 [68096/70488 (96%)]\tLoss: 0.912039\n",
      "2501\n",
      "Train Epoch: 17 [68608/70488 (97%)]\tLoss: 0.914898\n",
      "2502\n",
      "Train Epoch: 17 [69120/70488 (98%)]\tLoss: 0.914987\n",
      "2503\n",
      "Train Epoch: 17 [69632/70488 (99%)]\tLoss: 0.916363\n",
      "2504\n",
      "Train Epoch: 17 [47128/70488 (99%)]\tLoss: 0.915997\n",
      "2505\n",
      "====> Epoch: 17 Average loss: 0.00358398\n",
      "====> Test set loss: 30.46095347\n",
      "Train Epoch: 18 [0/70488 (0%)]\tLoss: 0.916754\n",
      "2516\n",
      "Train Epoch: 18 [512/70488 (1%)]\tLoss: 0.914111\n",
      "2517\n",
      "Train Epoch: 18 [1024/70488 (1%)]\tLoss: 0.914373\n",
      "2518\n",
      "Train Epoch: 18 [1536/70488 (2%)]\tLoss: 0.913198\n",
      "2519\n",
      "Train Epoch: 18 [2048/70488 (3%)]\tLoss: 0.912821\n",
      "2520\n",
      "Train Epoch: 18 [2560/70488 (4%)]\tLoss: 0.913668\n",
      "2521\n",
      "Train Epoch: 18 [3072/70488 (4%)]\tLoss: 0.914690\n",
      "2522\n",
      "Train Epoch: 18 [3584/70488 (5%)]\tLoss: 0.915766\n",
      "2523\n",
      "Train Epoch: 18 [4096/70488 (6%)]\tLoss: 0.912959\n",
      "2524\n",
      "Train Epoch: 18 [4608/70488 (7%)]\tLoss: 0.912234\n",
      "2525\n",
      "Train Epoch: 18 [5120/70488 (7%)]\tLoss: 0.913888\n",
      "2526\n",
      "Train Epoch: 18 [5632/70488 (8%)]\tLoss: 0.912772\n",
      "2527\n",
      "Train Epoch: 18 [6144/70488 (9%)]\tLoss: 0.914150\n",
      "2528\n",
      "Train Epoch: 18 [6656/70488 (9%)]\tLoss: 0.917090\n",
      "2529\n",
      "Train Epoch: 18 [7168/70488 (10%)]\tLoss: 0.913475\n",
      "2530\n",
      "Train Epoch: 18 [7680/70488 (11%)]\tLoss: 0.914382\n",
      "2531\n",
      "Train Epoch: 18 [8192/70488 (12%)]\tLoss: 0.913384\n",
      "2532\n",
      "Train Epoch: 18 [8704/70488 (12%)]\tLoss: 0.915975\n",
      "2533\n",
      "Train Epoch: 18 [9216/70488 (13%)]\tLoss: 0.913615\n",
      "2534\n",
      "Train Epoch: 18 [9728/70488 (14%)]\tLoss: 0.914371\n",
      "2535\n",
      "Train Epoch: 18 [10240/70488 (14%)]\tLoss: 0.918166\n",
      "2536\n",
      "Train Epoch: 18 [10752/70488 (15%)]\tLoss: 0.916730\n",
      "2537\n",
      "Train Epoch: 18 [11264/70488 (16%)]\tLoss: 0.914196\n",
      "2538\n",
      "Train Epoch: 18 [11776/70488 (17%)]\tLoss: 0.912982\n",
      "2539\n",
      "Train Epoch: 18 [12288/70488 (17%)]\tLoss: 0.913607\n",
      "2540\n",
      "Train Epoch: 18 [12800/70488 (18%)]\tLoss: 0.916647\n",
      "2541\n",
      "Train Epoch: 18 [13312/70488 (19%)]\tLoss: 0.914772\n",
      "2542\n",
      "Train Epoch: 18 [13824/70488 (20%)]\tLoss: 0.914004\n",
      "2543\n",
      "Train Epoch: 18 [14336/70488 (20%)]\tLoss: 0.913951\n",
      "2544\n",
      "Train Epoch: 18 [14848/70488 (21%)]\tLoss: 0.913638\n",
      "2545\n",
      "Train Epoch: 18 [15360/70488 (22%)]\tLoss: 0.913906\n",
      "2546\n",
      "Train Epoch: 18 [15872/70488 (22%)]\tLoss: 0.915309\n",
      "2547\n",
      "Train Epoch: 18 [16384/70488 (23%)]\tLoss: 0.913395\n",
      "2548\n",
      "Train Epoch: 18 [16896/70488 (24%)]\tLoss: 0.913881\n",
      "2549\n",
      "Train Epoch: 18 [17408/70488 (25%)]\tLoss: 0.912612\n",
      "2550\n",
      "Train Epoch: 18 [17920/70488 (25%)]\tLoss: 0.913121\n",
      "2551\n",
      "Train Epoch: 18 [18432/70488 (26%)]\tLoss: 0.912997\n",
      "2552\n",
      "Train Epoch: 18 [18944/70488 (27%)]\tLoss: 0.912047\n",
      "2553\n",
      "Train Epoch: 18 [19456/70488 (28%)]\tLoss: 0.915128\n",
      "2554\n",
      "Train Epoch: 18 [19968/70488 (28%)]\tLoss: 0.911803\n",
      "2555\n",
      "Train Epoch: 18 [20480/70488 (29%)]\tLoss: 0.915840\n",
      "2556\n",
      "Train Epoch: 18 [20992/70488 (30%)]\tLoss: 0.913714\n",
      "2557\n",
      "Train Epoch: 18 [21504/70488 (30%)]\tLoss: 0.914065\n",
      "2558\n",
      "Train Epoch: 18 [22016/70488 (31%)]\tLoss: 0.914214\n",
      "2559\n",
      "Train Epoch: 18 [22528/70488 (32%)]\tLoss: 0.913815\n",
      "2560\n",
      "Train Epoch: 18 [23040/70488 (33%)]\tLoss: 0.914456\n",
      "2561\n",
      "Train Epoch: 18 [23552/70488 (33%)]\tLoss: 0.913777\n",
      "2562\n",
      "Train Epoch: 18 [24064/70488 (34%)]\tLoss: 0.913206\n",
      "2563\n",
      "Train Epoch: 18 [24576/70488 (35%)]\tLoss: 0.911873\n",
      "2564\n",
      "Train Epoch: 18 [25088/70488 (36%)]\tLoss: 0.913581\n",
      "2565\n",
      "Train Epoch: 18 [25600/70488 (36%)]\tLoss: 0.915474\n",
      "2566\n",
      "Train Epoch: 18 [26112/70488 (37%)]\tLoss: 0.915057\n",
      "2567\n",
      "Train Epoch: 18 [26624/70488 (38%)]\tLoss: 0.915058\n",
      "2568\n",
      "Train Epoch: 18 [27136/70488 (38%)]\tLoss: 0.913481\n",
      "2569\n",
      "Train Epoch: 18 [27648/70488 (39%)]\tLoss: 0.914324\n",
      "2570\n",
      "Train Epoch: 18 [28160/70488 (40%)]\tLoss: 0.913554\n",
      "2571\n",
      "Train Epoch: 18 [28672/70488 (41%)]\tLoss: 0.916310\n",
      "2572\n",
      "Train Epoch: 18 [29184/70488 (41%)]\tLoss: 0.913979\n",
      "2573\n",
      "Train Epoch: 18 [29696/70488 (42%)]\tLoss: 0.915337\n",
      "2574\n",
      "Train Epoch: 18 [30208/70488 (43%)]\tLoss: 0.913856\n",
      "2575\n",
      "Train Epoch: 18 [30720/70488 (43%)]\tLoss: 0.914455\n",
      "2576\n",
      "Train Epoch: 18 [31232/70488 (44%)]\tLoss: 0.918557\n",
      "2577\n",
      "Train Epoch: 18 [31744/70488 (45%)]\tLoss: 0.913862\n",
      "2578\n",
      "Train Epoch: 18 [32256/70488 (46%)]\tLoss: 0.916592\n",
      "2579\n",
      "Train Epoch: 18 [32768/70488 (46%)]\tLoss: 0.913545\n",
      "2580\n",
      "Train Epoch: 18 [33280/70488 (47%)]\tLoss: 0.911689\n",
      "2581\n",
      "Train Epoch: 18 [33792/70488 (48%)]\tLoss: 0.915435\n",
      "2582\n",
      "Train Epoch: 18 [34304/70488 (49%)]\tLoss: 0.914390\n",
      "2583\n",
      "Train Epoch: 18 [34816/70488 (49%)]\tLoss: 0.912337\n",
      "2584\n",
      "Train Epoch: 18 [35328/70488 (50%)]\tLoss: 0.913199\n",
      "2585\n",
      "Train Epoch: 18 [35840/70488 (51%)]\tLoss: 0.915985\n",
      "2586\n",
      "Train Epoch: 18 [36352/70488 (51%)]\tLoss: 0.913098\n",
      "2587\n",
      "Train Epoch: 18 [36864/70488 (52%)]\tLoss: 0.915070\n",
      "2588\n",
      "Train Epoch: 18 [37376/70488 (53%)]\tLoss: 0.915066\n",
      "2589\n",
      "Train Epoch: 18 [37888/70488 (54%)]\tLoss: 0.915663\n",
      "2590\n",
      "Train Epoch: 18 [38400/70488 (54%)]\tLoss: 0.912006\n",
      "2591\n",
      "Train Epoch: 18 [38912/70488 (55%)]\tLoss: 0.913917\n",
      "2592\n",
      "Train Epoch: 18 [39424/70488 (56%)]\tLoss: 0.913642\n",
      "2593\n",
      "Train Epoch: 18 [39936/70488 (57%)]\tLoss: 0.914270\n",
      "2594\n",
      "Train Epoch: 18 [40448/70488 (57%)]\tLoss: 0.913715\n",
      "2595\n",
      "Train Epoch: 18 [40960/70488 (58%)]\tLoss: 0.912987\n",
      "2596\n",
      "Train Epoch: 18 [41472/70488 (59%)]\tLoss: 0.914954\n",
      "2597\n",
      "Train Epoch: 18 [41984/70488 (59%)]\tLoss: 0.916593\n",
      "2598\n",
      "Train Epoch: 18 [42496/70488 (60%)]\tLoss: 0.914541\n",
      "2599\n",
      "Train Epoch: 18 [43008/70488 (61%)]\tLoss: 0.911945\n",
      "2600\n",
      "Train Epoch: 18 [43520/70488 (62%)]\tLoss: 0.912865\n",
      "2601\n",
      "Train Epoch: 18 [44032/70488 (62%)]\tLoss: 0.914690\n",
      "2602\n",
      "Train Epoch: 18 [44544/70488 (63%)]\tLoss: 0.917452\n",
      "2603\n",
      "Train Epoch: 18 [45056/70488 (64%)]\tLoss: 0.916504\n",
      "2604\n",
      "Train Epoch: 18 [45568/70488 (64%)]\tLoss: 0.914436\n",
      "2605\n",
      "Train Epoch: 18 [46080/70488 (65%)]\tLoss: 0.915150\n",
      "2606\n",
      "Train Epoch: 18 [46592/70488 (66%)]\tLoss: 0.911396\n",
      "2607\n",
      "Train Epoch: 18 [47104/70488 (67%)]\tLoss: 0.912991\n",
      "2608\n",
      "Train Epoch: 18 [47616/70488 (67%)]\tLoss: 0.914461\n",
      "2609\n",
      "Train Epoch: 18 [48128/70488 (68%)]\tLoss: 0.914873\n",
      "2610\n",
      "Train Epoch: 18 [48640/70488 (69%)]\tLoss: 0.915611\n",
      "2611\n",
      "Train Epoch: 18 [49152/70488 (70%)]\tLoss: 0.917458\n",
      "2612\n",
      "Train Epoch: 18 [49664/70488 (70%)]\tLoss: 0.913991\n",
      "2613\n",
      "Train Epoch: 18 [50176/70488 (71%)]\tLoss: 0.912534\n",
      "2614\n",
      "Train Epoch: 18 [50688/70488 (72%)]\tLoss: 0.914549\n",
      "2615\n",
      "Train Epoch: 18 [51200/70488 (72%)]\tLoss: 0.914644\n",
      "2616\n",
      "Train Epoch: 18 [51712/70488 (73%)]\tLoss: 0.913767\n",
      "2617\n",
      "Train Epoch: 18 [52224/70488 (74%)]\tLoss: 0.913156\n",
      "2618\n",
      "Train Epoch: 18 [52736/70488 (75%)]\tLoss: 0.912268\n",
      "2619\n",
      "Train Epoch: 18 [53248/70488 (75%)]\tLoss: 0.913732\n",
      "2620\n",
      "Train Epoch: 18 [53760/70488 (76%)]\tLoss: 0.912763\n",
      "2621\n",
      "Train Epoch: 18 [54272/70488 (77%)]\tLoss: 0.916255\n",
      "2622\n",
      "Train Epoch: 18 [54784/70488 (78%)]\tLoss: 0.914638\n",
      "2623\n",
      "Train Epoch: 18 [55296/70488 (78%)]\tLoss: 0.914231\n",
      "2624\n",
      "Train Epoch: 18 [55808/70488 (79%)]\tLoss: 0.914246\n",
      "2625\n",
      "Train Epoch: 18 [56320/70488 (80%)]\tLoss: 0.914132\n",
      "2626\n",
      "Train Epoch: 18 [56832/70488 (80%)]\tLoss: 0.915557\n",
      "2627\n",
      "Train Epoch: 18 [57344/70488 (81%)]\tLoss: 0.913540\n",
      "2628\n",
      "Train Epoch: 18 [57856/70488 (82%)]\tLoss: 0.915887\n",
      "2629\n",
      "Train Epoch: 18 [58368/70488 (83%)]\tLoss: 0.914958\n",
      "2630\n",
      "Train Epoch: 18 [58880/70488 (83%)]\tLoss: 0.915376\n",
      "2631\n",
      "Train Epoch: 18 [59392/70488 (84%)]\tLoss: 0.915485\n",
      "2632\n",
      "Train Epoch: 18 [59904/70488 (85%)]\tLoss: 0.913100\n",
      "2633\n",
      "Train Epoch: 18 [60416/70488 (86%)]\tLoss: 0.914375\n",
      "2634\n",
      "Train Epoch: 18 [60928/70488 (86%)]\tLoss: 0.916451\n",
      "2635\n",
      "Train Epoch: 18 [61440/70488 (87%)]\tLoss: 0.916582\n",
      "2636\n",
      "Train Epoch: 18 [61952/70488 (88%)]\tLoss: 0.913321\n",
      "2637\n",
      "Train Epoch: 18 [62464/70488 (88%)]\tLoss: 0.916230\n",
      "2638\n",
      "Train Epoch: 18 [62976/70488 (89%)]\tLoss: 0.915582\n",
      "2639\n",
      "Train Epoch: 18 [63488/70488 (90%)]\tLoss: 0.913880\n",
      "2640\n",
      "Train Epoch: 18 [64000/70488 (91%)]\tLoss: 0.915722\n",
      "2641\n",
      "Train Epoch: 18 [64512/70488 (91%)]\tLoss: 0.913527\n",
      "2642\n",
      "Train Epoch: 18 [65024/70488 (92%)]\tLoss: 0.914322\n",
      "2643\n",
      "Train Epoch: 18 [65536/70488 (93%)]\tLoss: 0.912535\n",
      "2644\n",
      "Train Epoch: 18 [66048/70488 (93%)]\tLoss: 0.913177\n",
      "2645\n",
      "Train Epoch: 18 [66560/70488 (94%)]\tLoss: 0.913419\n",
      "2646\n",
      "Train Epoch: 18 [67072/70488 (95%)]\tLoss: 0.914178\n",
      "2647\n",
      "Train Epoch: 18 [67584/70488 (96%)]\tLoss: 0.915803\n",
      "2648\n",
      "Train Epoch: 18 [68096/70488 (96%)]\tLoss: 0.914564\n",
      "2649\n",
      "Train Epoch: 18 [68608/70488 (97%)]\tLoss: 0.915211\n",
      "2650\n",
      "Train Epoch: 18 [69120/70488 (98%)]\tLoss: 0.914518\n",
      "2651\n",
      "Train Epoch: 18 [69632/70488 (99%)]\tLoss: 0.914220\n",
      "2652\n",
      "Train Epoch: 18 [47128/70488 (99%)]\tLoss: 0.916049\n",
      "2653\n",
      "====> Epoch: 18 Average loss: 0.00358017\n",
      "====> Test set loss: 30.44815469\n",
      "Train Epoch: 19 [0/70488 (0%)]\tLoss: 0.914452\n",
      "2664\n",
      "Train Epoch: 19 [512/70488 (1%)]\tLoss: 0.912504\n",
      "2665\n",
      "Train Epoch: 19 [1024/70488 (1%)]\tLoss: 0.911898\n",
      "2666\n",
      "Train Epoch: 19 [1536/70488 (2%)]\tLoss: 0.913295\n",
      "2667\n",
      "Train Epoch: 19 [2048/70488 (3%)]\tLoss: 0.913945\n",
      "2668\n",
      "Train Epoch: 19 [2560/70488 (4%)]\tLoss: 0.911109\n",
      "2669\n",
      "Train Epoch: 19 [3072/70488 (4%)]\tLoss: 0.913561\n",
      "2670\n",
      "Train Epoch: 19 [3584/70488 (5%)]\tLoss: 0.913103\n",
      "2671\n",
      "Train Epoch: 19 [4096/70488 (6%)]\tLoss: 0.914855\n",
      "2672\n",
      "Train Epoch: 19 [4608/70488 (7%)]\tLoss: 0.911710\n",
      "2673\n",
      "Train Epoch: 19 [5120/70488 (7%)]\tLoss: 0.914383\n",
      "2674\n",
      "Train Epoch: 19 [5632/70488 (8%)]\tLoss: 0.911190\n",
      "2675\n",
      "Train Epoch: 19 [6144/70488 (9%)]\tLoss: 0.911997\n",
      "2676\n",
      "Train Epoch: 19 [6656/70488 (9%)]\tLoss: 0.912925\n",
      "2677\n",
      "Train Epoch: 19 [7168/70488 (10%)]\tLoss: 0.911601\n",
      "2678\n",
      "Train Epoch: 19 [7680/70488 (11%)]\tLoss: 0.911889\n",
      "2679\n",
      "Train Epoch: 19 [8192/70488 (12%)]\tLoss: 0.912819\n",
      "2680\n",
      "Train Epoch: 19 [8704/70488 (12%)]\tLoss: 0.914051\n",
      "2681\n",
      "Train Epoch: 19 [9216/70488 (13%)]\tLoss: 0.913783\n",
      "2682\n",
      "Train Epoch: 19 [9728/70488 (14%)]\tLoss: 0.914341\n",
      "2683\n",
      "Train Epoch: 19 [10240/70488 (14%)]\tLoss: 0.913620\n",
      "2684\n",
      "Train Epoch: 19 [10752/70488 (15%)]\tLoss: 0.914639\n",
      "2685\n",
      "Train Epoch: 19 [11264/70488 (16%)]\tLoss: 0.914378\n",
      "2686\n",
      "Train Epoch: 19 [11776/70488 (17%)]\tLoss: 0.914969\n",
      "2687\n",
      "Train Epoch: 19 [12288/70488 (17%)]\tLoss: 0.913800\n",
      "2688\n",
      "Train Epoch: 19 [12800/70488 (18%)]\tLoss: 0.913604\n",
      "2689\n",
      "Train Epoch: 19 [13312/70488 (19%)]\tLoss: 0.913153\n",
      "2690\n",
      "Train Epoch: 19 [13824/70488 (20%)]\tLoss: 0.914437\n",
      "2691\n",
      "Train Epoch: 19 [14336/70488 (20%)]\tLoss: 0.912013\n",
      "2692\n",
      "Train Epoch: 19 [14848/70488 (21%)]\tLoss: 0.913345\n",
      "2693\n",
      "Train Epoch: 19 [15360/70488 (22%)]\tLoss: 0.911135\n",
      "2694\n",
      "Train Epoch: 19 [15872/70488 (22%)]\tLoss: 0.910874\n",
      "2695\n",
      "Train Epoch: 19 [16384/70488 (23%)]\tLoss: 0.912526\n",
      "2696\n",
      "Train Epoch: 19 [16896/70488 (24%)]\tLoss: 0.914277\n",
      "2697\n",
      "Train Epoch: 19 [17408/70488 (25%)]\tLoss: 0.914469\n",
      "2698\n",
      "Train Epoch: 19 [17920/70488 (25%)]\tLoss: 0.912628\n",
      "2699\n",
      "Train Epoch: 19 [18432/70488 (26%)]\tLoss: 0.909621\n",
      "2700\n",
      "Train Epoch: 19 [18944/70488 (27%)]\tLoss: 0.912717\n",
      "2701\n",
      "Train Epoch: 19 [19456/70488 (28%)]\tLoss: 0.916697\n",
      "2702\n",
      "Train Epoch: 19 [19968/70488 (28%)]\tLoss: 0.912960\n",
      "2703\n",
      "Train Epoch: 19 [20480/70488 (29%)]\tLoss: 0.914262\n",
      "2704\n",
      "Train Epoch: 19 [20992/70488 (30%)]\tLoss: 0.914440\n",
      "2705\n",
      "Train Epoch: 19 [21504/70488 (30%)]\tLoss: 0.913777\n",
      "2706\n",
      "Train Epoch: 19 [22016/70488 (31%)]\tLoss: 0.911921\n",
      "2707\n",
      "Train Epoch: 19 [22528/70488 (32%)]\tLoss: 0.914612\n",
      "2708\n",
      "Train Epoch: 19 [23040/70488 (33%)]\tLoss: 0.915579\n",
      "2709\n",
      "Train Epoch: 19 [23552/70488 (33%)]\tLoss: 0.914127\n",
      "2710\n",
      "Train Epoch: 19 [24064/70488 (34%)]\tLoss: 0.911207\n",
      "2711\n",
      "Train Epoch: 19 [24576/70488 (35%)]\tLoss: 0.913614\n",
      "2712\n",
      "Train Epoch: 19 [25088/70488 (36%)]\tLoss: 0.914447\n",
      "2713\n",
      "Train Epoch: 19 [25600/70488 (36%)]\tLoss: 0.911152\n",
      "2714\n",
      "Train Epoch: 19 [26112/70488 (37%)]\tLoss: 0.913222\n",
      "2715\n",
      "Train Epoch: 19 [26624/70488 (38%)]\tLoss: 0.913077\n",
      "2716\n",
      "Train Epoch: 19 [27136/70488 (38%)]\tLoss: 0.916610\n",
      "2717\n",
      "Train Epoch: 19 [27648/70488 (39%)]\tLoss: 0.912157\n",
      "2718\n",
      "Train Epoch: 19 [28160/70488 (40%)]\tLoss: 0.915187\n",
      "2719\n",
      "Train Epoch: 19 [28672/70488 (41%)]\tLoss: 0.912943\n",
      "2720\n",
      "Train Epoch: 19 [29184/70488 (41%)]\tLoss: 0.912760\n",
      "2721\n",
      "Train Epoch: 19 [29696/70488 (42%)]\tLoss: 0.912560\n",
      "2722\n",
      "Train Epoch: 19 [30208/70488 (43%)]\tLoss: 0.913958\n",
      "2723\n",
      "Train Epoch: 19 [30720/70488 (43%)]\tLoss: 0.915493\n",
      "2724\n",
      "Train Epoch: 19 [31232/70488 (44%)]\tLoss: 0.911737\n",
      "2725\n",
      "Train Epoch: 19 [31744/70488 (45%)]\tLoss: 0.912778\n",
      "2726\n",
      "Train Epoch: 19 [32256/70488 (46%)]\tLoss: 0.914841\n",
      "2727\n",
      "Train Epoch: 19 [32768/70488 (46%)]\tLoss: 0.913173\n",
      "2728\n",
      "Train Epoch: 19 [33280/70488 (47%)]\tLoss: 0.914456\n",
      "2729\n",
      "Train Epoch: 19 [33792/70488 (48%)]\tLoss: 0.911563\n",
      "2730\n",
      "Train Epoch: 19 [34304/70488 (49%)]\tLoss: 0.914797\n",
      "2731\n",
      "Train Epoch: 19 [34816/70488 (49%)]\tLoss: 0.918465\n",
      "2732\n",
      "Train Epoch: 19 [35328/70488 (50%)]\tLoss: 0.911635\n",
      "2733\n",
      "Train Epoch: 19 [35840/70488 (51%)]\tLoss: 0.914544\n",
      "2734\n",
      "Train Epoch: 19 [36352/70488 (51%)]\tLoss: 0.913255\n",
      "2735\n",
      "Train Epoch: 19 [36864/70488 (52%)]\tLoss: 0.913749\n",
      "2736\n",
      "Train Epoch: 19 [37376/70488 (53%)]\tLoss: 0.914739\n",
      "2737\n",
      "Train Epoch: 19 [37888/70488 (54%)]\tLoss: 0.912414\n",
      "2738\n",
      "Train Epoch: 19 [38400/70488 (54%)]\tLoss: 0.913630\n",
      "2739\n",
      "Train Epoch: 19 [38912/70488 (55%)]\tLoss: 0.913692\n",
      "2740\n",
      "Train Epoch: 19 [39424/70488 (56%)]\tLoss: 0.912488\n",
      "2741\n",
      "Train Epoch: 19 [39936/70488 (57%)]\tLoss: 0.913077\n",
      "2742\n",
      "Train Epoch: 19 [40448/70488 (57%)]\tLoss: 0.913053\n",
      "2743\n",
      "Train Epoch: 19 [40960/70488 (58%)]\tLoss: 0.913625\n",
      "2744\n",
      "Train Epoch: 19 [41472/70488 (59%)]\tLoss: 0.913846\n",
      "2745\n",
      "Train Epoch: 19 [41984/70488 (59%)]\tLoss: 0.913730\n",
      "2746\n",
      "Train Epoch: 19 [42496/70488 (60%)]\tLoss: 0.915103\n",
      "2747\n",
      "Train Epoch: 19 [43008/70488 (61%)]\tLoss: 0.911035\n",
      "2748\n",
      "Train Epoch: 19 [43520/70488 (62%)]\tLoss: 0.913950\n",
      "2749\n",
      "Train Epoch: 19 [44032/70488 (62%)]\tLoss: 0.912305\n",
      "2750\n",
      "Train Epoch: 19 [44544/70488 (63%)]\tLoss: 0.911604\n",
      "2751\n",
      "Train Epoch: 19 [45056/70488 (64%)]\tLoss: 0.915091\n",
      "2752\n",
      "Train Epoch: 19 [45568/70488 (64%)]\tLoss: 0.914891\n",
      "2753\n",
      "Train Epoch: 19 [46080/70488 (65%)]\tLoss: 0.912889\n",
      "2754\n",
      "Train Epoch: 19 [46592/70488 (66%)]\tLoss: 0.914900\n",
      "2755\n",
      "Train Epoch: 19 [47104/70488 (67%)]\tLoss: 0.913119\n",
      "2756\n",
      "Train Epoch: 19 [47616/70488 (67%)]\tLoss: 0.912309\n",
      "2757\n",
      "Train Epoch: 19 [48128/70488 (68%)]\tLoss: 0.912412\n",
      "2758\n",
      "Train Epoch: 19 [48640/70488 (69%)]\tLoss: 0.911616\n",
      "2759\n",
      "Train Epoch: 19 [49152/70488 (70%)]\tLoss: 0.913007\n",
      "2760\n",
      "Train Epoch: 19 [49664/70488 (70%)]\tLoss: 0.912136\n",
      "2761\n",
      "Train Epoch: 19 [50176/70488 (71%)]\tLoss: 0.912229\n",
      "2762\n",
      "Train Epoch: 19 [50688/70488 (72%)]\tLoss: 0.911316\n",
      "2763\n",
      "Train Epoch: 19 [51200/70488 (72%)]\tLoss: 0.912702\n",
      "2764\n",
      "Train Epoch: 19 [51712/70488 (73%)]\tLoss: 0.914545\n",
      "2765\n",
      "Train Epoch: 19 [52224/70488 (74%)]\tLoss: 0.912747\n",
      "2766\n",
      "Train Epoch: 19 [52736/70488 (75%)]\tLoss: 0.913434\n",
      "2767\n",
      "Train Epoch: 19 [53248/70488 (75%)]\tLoss: 0.910628\n",
      "2768\n",
      "Train Epoch: 19 [53760/70488 (76%)]\tLoss: 0.914917\n",
      "2769\n",
      "Train Epoch: 19 [54272/70488 (77%)]\tLoss: 0.915486\n",
      "2770\n",
      "Train Epoch: 19 [54784/70488 (78%)]\tLoss: 0.914464\n",
      "2771\n",
      "Train Epoch: 19 [55296/70488 (78%)]\tLoss: 0.911224\n",
      "2772\n",
      "Train Epoch: 19 [55808/70488 (79%)]\tLoss: 0.912844\n",
      "2773\n",
      "Train Epoch: 19 [56320/70488 (80%)]\tLoss: 0.915962\n",
      "2774\n",
      "Train Epoch: 19 [56832/70488 (80%)]\tLoss: 0.914362\n",
      "2775\n",
      "Train Epoch: 19 [57344/70488 (81%)]\tLoss: 0.914459\n",
      "2776\n",
      "Train Epoch: 19 [57856/70488 (82%)]\tLoss: 0.914223\n",
      "2777\n",
      "Train Epoch: 19 [58368/70488 (83%)]\tLoss: 0.913148\n",
      "2778\n",
      "Train Epoch: 19 [58880/70488 (83%)]\tLoss: 0.914663\n",
      "2779\n",
      "Train Epoch: 19 [59392/70488 (84%)]\tLoss: 0.915943\n",
      "2780\n",
      "Train Epoch: 19 [59904/70488 (85%)]\tLoss: 0.915371\n",
      "2781\n",
      "Train Epoch: 19 [60416/70488 (86%)]\tLoss: 0.910782\n",
      "2782\n",
      "Train Epoch: 19 [60928/70488 (86%)]\tLoss: 0.914081\n",
      "2783\n",
      "Train Epoch: 19 [61440/70488 (87%)]\tLoss: 0.914921\n",
      "2784\n",
      "Train Epoch: 19 [61952/70488 (88%)]\tLoss: 0.914677\n",
      "2785\n",
      "Train Epoch: 19 [62464/70488 (88%)]\tLoss: 0.914606\n",
      "2786\n",
      "Train Epoch: 19 [62976/70488 (89%)]\tLoss: 0.914080\n",
      "2787\n",
      "Train Epoch: 19 [63488/70488 (90%)]\tLoss: 0.913513\n",
      "2788\n",
      "Train Epoch: 19 [64000/70488 (91%)]\tLoss: 0.912559\n",
      "2789\n",
      "Train Epoch: 19 [64512/70488 (91%)]\tLoss: 0.913545\n",
      "2790\n",
      "Train Epoch: 19 [65024/70488 (92%)]\tLoss: 0.913773\n",
      "2791\n",
      "Train Epoch: 19 [65536/70488 (93%)]\tLoss: 0.911573\n",
      "2792\n",
      "Train Epoch: 19 [66048/70488 (93%)]\tLoss: 0.912440\n",
      "2793\n",
      "Train Epoch: 19 [66560/70488 (94%)]\tLoss: 0.913155\n",
      "2794\n",
      "Train Epoch: 19 [67072/70488 (95%)]\tLoss: 0.913228\n",
      "2795\n",
      "Train Epoch: 19 [67584/70488 (96%)]\tLoss: 0.913955\n",
      "2796\n",
      "Train Epoch: 19 [68096/70488 (96%)]\tLoss: 0.912764\n",
      "2797\n",
      "Train Epoch: 19 [68608/70488 (97%)]\tLoss: 0.913136\n",
      "2798\n",
      "Train Epoch: 19 [69120/70488 (98%)]\tLoss: 0.911054\n",
      "2799\n",
      "Train Epoch: 19 [69632/70488 (99%)]\tLoss: 0.913993\n",
      "2800\n",
      "Train Epoch: 19 [47128/70488 (99%)]\tLoss: 0.914540\n",
      "2801\n",
      "====> Epoch: 19 Average loss: 0.00357647\n",
      "====> Test set loss: 30.42486477\n",
      "Train Epoch: 20 [0/70488 (0%)]\tLoss: 0.912910\n",
      "2812\n",
      "Train Epoch: 20 [512/70488 (1%)]\tLoss: 0.911440\n",
      "2813\n",
      "Train Epoch: 20 [1024/70488 (1%)]\tLoss: 0.912331\n",
      "2814\n",
      "Train Epoch: 20 [1536/70488 (2%)]\tLoss: 0.913861\n",
      "2815\n",
      "Train Epoch: 20 [2048/70488 (3%)]\tLoss: 0.910797\n",
      "2816\n",
      "Train Epoch: 20 [2560/70488 (4%)]\tLoss: 0.913174\n",
      "2817\n",
      "Train Epoch: 20 [3072/70488 (4%)]\tLoss: 0.915352\n",
      "2818\n",
      "Train Epoch: 20 [3584/70488 (5%)]\tLoss: 0.912504\n",
      "2819\n",
      "Train Epoch: 20 [4096/70488 (6%)]\tLoss: 0.913666\n",
      "2820\n",
      "Train Epoch: 20 [4608/70488 (7%)]\tLoss: 0.913137\n",
      "2821\n",
      "Train Epoch: 20 [5120/70488 (7%)]\tLoss: 0.914627\n",
      "2822\n",
      "Train Epoch: 20 [5632/70488 (8%)]\tLoss: 0.912562\n",
      "2823\n",
      "Train Epoch: 20 [6144/70488 (9%)]\tLoss: 0.910319\n",
      "2824\n",
      "Train Epoch: 20 [6656/70488 (9%)]\tLoss: 0.912589\n",
      "2825\n",
      "Train Epoch: 20 [7168/70488 (10%)]\tLoss: 0.912791\n",
      "2826\n",
      "Train Epoch: 20 [7680/70488 (11%)]\tLoss: 0.912389\n",
      "2827\n",
      "Train Epoch: 20 [8192/70488 (12%)]\tLoss: 0.913253\n",
      "2828\n",
      "Train Epoch: 20 [8704/70488 (12%)]\tLoss: 0.913161\n",
      "2829\n",
      "Train Epoch: 20 [9216/70488 (13%)]\tLoss: 0.912055\n",
      "2830\n",
      "Train Epoch: 20 [9728/70488 (14%)]\tLoss: 0.912171\n",
      "2831\n",
      "Train Epoch: 20 [10240/70488 (14%)]\tLoss: 0.911164\n",
      "2832\n",
      "Train Epoch: 20 [10752/70488 (15%)]\tLoss: 0.913644\n",
      "2833\n",
      "Train Epoch: 20 [11264/70488 (16%)]\tLoss: 0.913376\n",
      "2834\n",
      "Train Epoch: 20 [11776/70488 (17%)]\tLoss: 0.911757\n",
      "2835\n",
      "Train Epoch: 20 [12288/70488 (17%)]\tLoss: 0.911404\n",
      "2836\n",
      "Train Epoch: 20 [12800/70488 (18%)]\tLoss: 0.915058\n",
      "2837\n",
      "Train Epoch: 20 [13312/70488 (19%)]\tLoss: 0.912924\n",
      "2838\n",
      "Train Epoch: 20 [13824/70488 (20%)]\tLoss: 0.911120\n",
      "2839\n",
      "Train Epoch: 20 [14336/70488 (20%)]\tLoss: 0.913143\n",
      "2840\n",
      "Train Epoch: 20 [14848/70488 (21%)]\tLoss: 0.913577\n",
      "2841\n",
      "Train Epoch: 20 [15360/70488 (22%)]\tLoss: 0.912548\n",
      "2842\n",
      "Train Epoch: 20 [15872/70488 (22%)]\tLoss: 0.913064\n",
      "2843\n",
      "Train Epoch: 20 [16384/70488 (23%)]\tLoss: 0.910866\n",
      "2844\n",
      "Train Epoch: 20 [16896/70488 (24%)]\tLoss: 0.911246\n",
      "2845\n",
      "Train Epoch: 20 [17408/70488 (25%)]\tLoss: 0.913690\n",
      "2846\n",
      "Train Epoch: 20 [17920/70488 (25%)]\tLoss: 0.911982\n",
      "2847\n",
      "Train Epoch: 20 [18432/70488 (26%)]\tLoss: 0.911683\n",
      "2848\n",
      "Train Epoch: 20 [18944/70488 (27%)]\tLoss: 0.909914\n",
      "2849\n",
      "Train Epoch: 20 [19456/70488 (28%)]\tLoss: 0.911226\n",
      "2850\n",
      "Train Epoch: 20 [19968/70488 (28%)]\tLoss: 0.913040\n",
      "2851\n",
      "Train Epoch: 20 [20480/70488 (29%)]\tLoss: 0.913619\n",
      "2852\n",
      "Train Epoch: 20 [20992/70488 (30%)]\tLoss: 0.913781\n",
      "2853\n",
      "Train Epoch: 20 [21504/70488 (30%)]\tLoss: 0.911821\n",
      "2854\n",
      "Train Epoch: 20 [22016/70488 (31%)]\tLoss: 0.913724\n",
      "2855\n",
      "Train Epoch: 20 [22528/70488 (32%)]\tLoss: 0.911894\n",
      "2856\n",
      "Train Epoch: 20 [23040/70488 (33%)]\tLoss: 0.913038\n",
      "2857\n",
      "Train Epoch: 20 [23552/70488 (33%)]\tLoss: 0.912442\n",
      "2858\n",
      "Train Epoch: 20 [24064/70488 (34%)]\tLoss: 0.914030\n",
      "2859\n",
      "Train Epoch: 20 [24576/70488 (35%)]\tLoss: 0.910447\n",
      "2860\n",
      "Train Epoch: 20 [25088/70488 (36%)]\tLoss: 0.912856\n",
      "2861\n",
      "Train Epoch: 20 [25600/70488 (36%)]\tLoss: 0.913430\n",
      "2862\n",
      "Train Epoch: 20 [26112/70488 (37%)]\tLoss: 0.912961\n",
      "2863\n",
      "Train Epoch: 20 [26624/70488 (38%)]\tLoss: 0.913842\n",
      "2864\n",
      "Train Epoch: 20 [27136/70488 (38%)]\tLoss: 0.912714\n",
      "2865\n",
      "Train Epoch: 20 [27648/70488 (39%)]\tLoss: 0.912639\n",
      "2866\n",
      "Train Epoch: 20 [28160/70488 (40%)]\tLoss: 0.912375\n",
      "2867\n",
      "Train Epoch: 20 [28672/70488 (41%)]\tLoss: 0.912785\n",
      "2868\n",
      "Train Epoch: 20 [29184/70488 (41%)]\tLoss: 0.909927\n",
      "2869\n",
      "Train Epoch: 20 [29696/70488 (42%)]\tLoss: 0.913636\n",
      "2870\n",
      "Train Epoch: 20 [30208/70488 (43%)]\tLoss: 0.911004\n",
      "2871\n",
      "Train Epoch: 20 [30720/70488 (43%)]\tLoss: 0.912949\n",
      "2872\n",
      "Train Epoch: 20 [31232/70488 (44%)]\tLoss: 0.914273\n",
      "2873\n",
      "Train Epoch: 20 [31744/70488 (45%)]\tLoss: 0.913032\n",
      "2874\n",
      "Train Epoch: 20 [32256/70488 (46%)]\tLoss: 0.913618\n",
      "2875\n",
      "Train Epoch: 20 [32768/70488 (46%)]\tLoss: 0.913370\n",
      "2876\n",
      "Train Epoch: 20 [33280/70488 (47%)]\tLoss: 0.914106\n",
      "2877\n",
      "Train Epoch: 20 [33792/70488 (48%)]\tLoss: 0.912476\n",
      "2878\n",
      "Train Epoch: 20 [34304/70488 (49%)]\tLoss: 0.913109\n",
      "2879\n",
      "Train Epoch: 20 [34816/70488 (49%)]\tLoss: 0.911859\n",
      "2880\n",
      "Train Epoch: 20 [35328/70488 (50%)]\tLoss: 0.910984\n",
      "2881\n",
      "Train Epoch: 20 [35840/70488 (51%)]\tLoss: 0.916185\n",
      "2882\n",
      "Train Epoch: 20 [36352/70488 (51%)]\tLoss: 0.913316\n",
      "2883\n",
      "Train Epoch: 20 [36864/70488 (52%)]\tLoss: 0.911449\n",
      "2884\n",
      "Train Epoch: 20 [37376/70488 (53%)]\tLoss: 0.912092\n",
      "2885\n",
      "Train Epoch: 20 [37888/70488 (54%)]\tLoss: 0.913323\n",
      "2886\n",
      "Train Epoch: 20 [38400/70488 (54%)]\tLoss: 0.914289\n",
      "2887\n",
      "Train Epoch: 20 [38912/70488 (55%)]\tLoss: 0.911900\n",
      "2888\n",
      "Train Epoch: 20 [39424/70488 (56%)]\tLoss: 0.913236\n",
      "2889\n",
      "Train Epoch: 20 [39936/70488 (57%)]\tLoss: 0.913969\n",
      "2890\n",
      "Train Epoch: 20 [40448/70488 (57%)]\tLoss: 0.912422\n",
      "2891\n",
      "Train Epoch: 20 [40960/70488 (58%)]\tLoss: 0.913949\n",
      "2892\n",
      "Train Epoch: 20 [41472/70488 (59%)]\tLoss: 0.911970\n",
      "2893\n",
      "Train Epoch: 20 [41984/70488 (59%)]\tLoss: 0.912968\n",
      "2894\n",
      "Train Epoch: 20 [42496/70488 (60%)]\tLoss: 0.912880\n",
      "2895\n",
      "Train Epoch: 20 [43008/70488 (61%)]\tLoss: 0.911031\n",
      "2896\n",
      "Train Epoch: 20 [43520/70488 (62%)]\tLoss: 0.914101\n",
      "2897\n",
      "Train Epoch: 20 [44032/70488 (62%)]\tLoss: 0.913570\n",
      "2898\n",
      "Train Epoch: 20 [44544/70488 (63%)]\tLoss: 0.913399\n",
      "2899\n",
      "Train Epoch: 20 [45056/70488 (64%)]\tLoss: 0.910903\n",
      "2900\n",
      "Train Epoch: 20 [45568/70488 (64%)]\tLoss: 0.910870\n",
      "2901\n",
      "Train Epoch: 20 [46080/70488 (65%)]\tLoss: 0.914224\n",
      "2902\n",
      "Train Epoch: 20 [46592/70488 (66%)]\tLoss: 0.911015\n",
      "2903\n",
      "Train Epoch: 20 [47104/70488 (67%)]\tLoss: 0.911190\n",
      "2904\n",
      "Train Epoch: 20 [47616/70488 (67%)]\tLoss: 0.911105\n",
      "2905\n",
      "Train Epoch: 20 [48128/70488 (68%)]\tLoss: 0.911789\n",
      "2906\n",
      "Train Epoch: 20 [48640/70488 (69%)]\tLoss: 0.912383\n",
      "2907\n",
      "Train Epoch: 20 [49152/70488 (70%)]\tLoss: 0.912116\n",
      "2908\n",
      "Train Epoch: 20 [49664/70488 (70%)]\tLoss: 0.910669\n",
      "2909\n",
      "Train Epoch: 20 [50176/70488 (71%)]\tLoss: 0.912792\n",
      "2910\n",
      "Train Epoch: 20 [50688/70488 (72%)]\tLoss: 0.913637\n",
      "2911\n",
      "Train Epoch: 20 [51200/70488 (72%)]\tLoss: 0.913008\n",
      "2912\n",
      "Train Epoch: 20 [51712/70488 (73%)]\tLoss: 0.912701\n",
      "2913\n",
      "Train Epoch: 20 [52224/70488 (74%)]\tLoss: 0.911621\n",
      "2914\n",
      "Train Epoch: 20 [52736/70488 (75%)]\tLoss: 0.913966\n",
      "2915\n",
      "Train Epoch: 20 [53248/70488 (75%)]\tLoss: 0.911560\n",
      "2916\n",
      "Train Epoch: 20 [53760/70488 (76%)]\tLoss: 0.912955\n",
      "2917\n",
      "Train Epoch: 20 [54272/70488 (77%)]\tLoss: 0.912438\n",
      "2918\n",
      "Train Epoch: 20 [54784/70488 (78%)]\tLoss: 0.910420\n",
      "2919\n",
      "Train Epoch: 20 [55296/70488 (78%)]\tLoss: 0.911593\n",
      "2920\n",
      "Train Epoch: 20 [55808/70488 (79%)]\tLoss: 0.912074\n",
      "2921\n",
      "Train Epoch: 20 [56320/70488 (80%)]\tLoss: 0.913092\n",
      "2922\n",
      "Train Epoch: 20 [56832/70488 (80%)]\tLoss: 0.912713\n",
      "2923\n",
      "Train Epoch: 20 [57344/70488 (81%)]\tLoss: 0.914574\n",
      "2924\n",
      "Train Epoch: 20 [57856/70488 (82%)]\tLoss: 0.911010\n",
      "2925\n",
      "Train Epoch: 20 [58368/70488 (83%)]\tLoss: 0.913580\n",
      "2926\n",
      "Train Epoch: 20 [58880/70488 (83%)]\tLoss: 0.912580\n",
      "2927\n",
      "Train Epoch: 20 [59392/70488 (84%)]\tLoss: 0.912487\n",
      "2928\n",
      "Train Epoch: 20 [59904/70488 (85%)]\tLoss: 0.913447\n",
      "2929\n",
      "Train Epoch: 20 [60416/70488 (86%)]\tLoss: 0.908129\n",
      "2930\n",
      "Train Epoch: 20 [60928/70488 (86%)]\tLoss: 0.911351\n",
      "2931\n",
      "Train Epoch: 20 [61440/70488 (87%)]\tLoss: 0.913797\n",
      "2932\n",
      "Train Epoch: 20 [61952/70488 (88%)]\tLoss: 0.912075\n",
      "2933\n",
      "Train Epoch: 20 [62464/70488 (88%)]\tLoss: 0.914303\n",
      "2934\n",
      "Train Epoch: 20 [62976/70488 (89%)]\tLoss: 0.913765\n",
      "2935\n",
      "Train Epoch: 20 [63488/70488 (90%)]\tLoss: 0.914893\n",
      "2936\n",
      "Train Epoch: 20 [64000/70488 (91%)]\tLoss: 0.912886\n",
      "2937\n",
      "Train Epoch: 20 [64512/70488 (91%)]\tLoss: 0.914152\n",
      "2938\n",
      "Train Epoch: 20 [65024/70488 (92%)]\tLoss: 0.913611\n",
      "2939\n",
      "Train Epoch: 20 [65536/70488 (93%)]\tLoss: 0.912457\n",
      "2940\n",
      "Train Epoch: 20 [66048/70488 (93%)]\tLoss: 0.913460\n",
      "2941\n",
      "Train Epoch: 20 [66560/70488 (94%)]\tLoss: 0.911812\n",
      "2942\n",
      "Train Epoch: 20 [67072/70488 (95%)]\tLoss: 0.914234\n",
      "2943\n",
      "Train Epoch: 20 [67584/70488 (96%)]\tLoss: 0.910489\n",
      "2944\n",
      "Train Epoch: 20 [68096/70488 (96%)]\tLoss: 0.913657\n",
      "2945\n",
      "Train Epoch: 20 [68608/70488 (97%)]\tLoss: 0.911226\n",
      "2946\n",
      "Train Epoch: 20 [69120/70488 (98%)]\tLoss: 0.913940\n",
      "2947\n",
      "Train Epoch: 20 [69632/70488 (99%)]\tLoss: 0.913243\n",
      "2948\n",
      "Train Epoch: 20 [47128/70488 (99%)]\tLoss: 0.911299\n",
      "2949\n",
      "====> Epoch: 20 Average loss: 0.00357348\n",
      "====> Test set loss: 30.46351337\n"
     ]
    }
   ],
   "source": [
    "def train(epoch,recode_dict,writer):\n",
    "    global step\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb = data[0]\n",
    "        target = data[1]\n",
    "        target = recode_tags(target,recode_dict)\n",
    "        batch_size = target.shape[0]\n",
    "        #preds = F.one_hot(preds.to(torch.int64))\n",
    "        target = target.reshape(batch_size,128,128)\n",
    "        target = target.to(device,dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        #dont need latent space output while training\n",
    "        y_batch,_  = model(rgb)\n",
    "        loss = loss_fn(y_batch,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_size * batch_idx, len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))\n",
    "\n",
    "        print(((epoch-1)*10)+step)\n",
    "        writer.add_scalar(\"AE Loss\", loss.item(), step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "          epoch, avg_loss))\n",
    "    writer.flush()\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def test(epoch,recode_dict,writer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            rgb = data[0]\n",
    "            target = data[1]\n",
    "            batch_size = target.shape[0]\n",
    "            target = recode_tags(target,recode_dict)\n",
    "            #preds = F.one_hot(preds.to(torch.int64))\n",
    "            target = target.permute(0,3,1,2)\n",
    "            target= target.reshape(batch_size,128,128)\n",
    "            target = target.to(device,dtype=torch.long)\n",
    "            y_batch,_   = model(rgb)\n",
    "            test_loss += loss_fn(y_batch,target).item()\n",
    "\n",
    "    print('====> Test set loss: {:.8f}'.format(test_loss))\n",
    "    writer.add_scalar(\"AE test Loss\", test_loss,epoch)\n",
    "    writer.flush()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "for epoch in range(1, 20 + 1):\n",
    "    \n",
    "    smallest_loss = 1000\n",
    "    \n",
    "    avg_loss = train(epoch,recode_dict,writer)\n",
    "    if avg_loss < smallest_loss:\n",
    "        torch.save(model.state_dict(), './AE_params/model_54.best')\n",
    "    test(epoch,recode_dict,writer)\n",
    "    torch.save(model.state_dict(), './AE_params/model_54.final')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88e7369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cf8f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b638e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_dict = {\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:2,\n",
    "    3:3,\n",
    "    4:4,\n",
    "    5:5,\n",
    "    6:6,\n",
    "    7:7,\n",
    "    8:8,\n",
    "    9:9,\n",
    "    10:10,\n",
    "    11:11,\n",
    "    12:12,\n",
    "    13:0,\n",
    "    14:3,\n",
    "    15:1,\n",
    "    16:3,\n",
    "    17:2,\n",
    "    18:5,\n",
    "    19:3,\n",
    "    20:4,\n",
    "    21:3,\n",
    "    22:9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07047bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_tags(sem_image,recode_dict):\n",
    "    for value in recode_dict.keys():\n",
    "        sem_image[sem_image==value] = recode_dict[value]\n",
    "    \n",
    "    return sem_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5052998d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./AE_params/model_53.best'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "37e7663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17371"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data = town03_data = CustomImageDataset('.','Town03',test=False)\n",
    "old_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fff9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_iou_acc(pred,true):\n",
    "    default_evaluator = create_supervised_evaluator(model)\n",
    "    cm = ConfusionMatrix(num_classes=13)\n",
    "    metric = IoU(cm)\n",
    "    metric.attach(default_evaluator, 'iou')\n",
    "    state = default_evaluator.run([[pred, true]])\n",
    "    print(state.metrics['iou'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae97504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = random.randint(1,70000)\n",
    "print(num)\n",
    "data = train_data.__getitem__(num)\n",
    "imgs = []\n",
    "org = data[0]\n",
    "sem = replace(data[1].numpy().transpose(1,2,0))\n",
    "imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "imgs.append(Image.fromarray(sem))\n",
    "imgs.append(generate_semantic_im(data[0],model))\n",
    "fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "for i, img in enumerate(imgs):\n",
    "    axs[0, i].imshow(np.asarray(img))\n",
    "    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    #img.save('./AE_results/AE/'+str(num)+'_'+str(i)+'.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1d8c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: 'tuple' object has no attribute 'detach'\n",
      "Engine run is terminating due to exception: 'tuple' object has no attribute 'detach'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13489/1039701256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_iou_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13489/3536069146.py\u001b[0m in \u001b[0;36mget_iou_acc\u001b[0;34m(pred, true)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIoU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_evaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iou'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iou'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate_single_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/metrics/metric.py\u001b[0m in \u001b[0;36miteration_completed\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_o1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_o2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/metrics/metric.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/metrics/confusion_matrix.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mreinit__is_reduced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/ignite/metrics/confusion_matrix.py\u001b[0m in \u001b[0;36m_check_shape\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "from ignite.engine import create_supervised_evaluator\n",
    "from ignite.metrics import IoU, ConfusionMatrix\n",
    "\n",
    "new_obs = data[0].reshape(1,3,128,128)\n",
    "\n",
    "\n",
    "get_iou_acc(new_obs,data[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a1ec869",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13489/1320554701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_iou_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_semantic_im\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13489/1326988315.py\u001b[0m in \u001b[0;36mget_iou_acc\u001b[0;34m(pred, true)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIoU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_evaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iou'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iou'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default_evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "get_iou_acc(generate_semantic_im(data[0],model),data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "691a80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[70,130,180],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f6cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = RGB_image.reshape(1,3,128,128)\n",
    "    out,_ ,_,_ = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    print(sample.shape)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be49b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = T.ToPILImage()(img.to('cpu'))\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    num = random.randint(0,6000)\n",
    "    data = town03_data.__getitem__(num)\n",
    "    imgs = []\n",
    "    org = data[0]\n",
    "    org = TF.resized_crop(org,64,14,50,100,(128,128))\n",
    "    sem = TF.resized_crop(torch.tensor(data[1]).permute(2,0,1),64,14,50,100,(128,128))\n",
    "    \n",
    "    sem = replace(data[1])\n",
    "\n",
    "\n",
    "    imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "    imgs.append(Image.fromarray(sem.reshape(128,128,3)))\n",
    "    imgs.append(generate_semantic_im(org))\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453cf6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
