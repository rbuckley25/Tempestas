{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ec9984",
   "metadata": {},
   "source": [
    "# Learning to Drive in Adverse Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6519255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07881483",
   "metadata": {},
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "This project is designed to explore using Reinforcement learning to teach an autonomous agent to drive in adverse weather conditions.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- The project will be performed using an autonomous drving simulator called CARLA.\n",
    "- Python 3.8\n",
    "- Anconda\n",
    "\n",
    "[Project Github](https://github.com/rbuckley25/Tempestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdd1a5",
   "metadata": {},
   "source": [
    "### Image Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc38f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import carla\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58bddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, weather, town, test=False , transform=None, target_transform=None):\n",
    "        dirt = './Datasets/'+weather+'/'+town\n",
    "        if test:\n",
    "            dirt = dirt+'/test'\n",
    "        \n",
    "        self.sem_dir = dirt+'/Semantic'\n",
    "        self.rgb_dir = dirt+'/RGB'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.names = os.listdir(self.rgb_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.sem_dir))\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = os.path.join(self.rgb_dir, self.names[idx])\n",
    "        image = read_image(img_path)\n",
    "        label_name = self.names[idx].split('.')[0]+'.npy'\n",
    "        label = np.load(os.path.join(self.sem_dir, label_name))\n",
    "        label = torch.tensor(label).permute(2,0,1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4de061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropResizeTransform:\n",
    "    def __init__(self, top, left, width, height, size):\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return TF.resized_crop(x,self.top,self.left,self.width,self.height,self.size)\n",
    "\n",
    "class Hflip:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return TF.hflip(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378cfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return torch.sigmoid(self.conv12(x))\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        latent = self.encode(x)\n",
    "        #latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fda9eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerceptionNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e4d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Conv2d) or isinstance(layer,torch.nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "050d50fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initalize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46cb67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=lable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_batch,pred,mu,logvar):\n",
    "    cross = nn.BCELoss(reduction='sum')\n",
    "    rec_loss = cross(y_batch,pred)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rec_loss + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337d41d",
   "metadata": {},
   "source": [
    "Reference: https://github.com/pytorch/examples/blob/master/vae/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd868ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_transform = CropResizeTransform(64,14,50,100,(128,128))\n",
    "town03_data = CustomImageDataset('ClearNoon','Town03',test=False)\n",
    "town03_test_data = CustomImageDataset('ClearNoon','Town03',test=True)\n",
    "town04_data = CustomImageDataset('ClearNoon','Town04',test=False)\n",
    "town04_test_data = CustomImageDataset('ClearNoon','Town04',test=True)\n",
    "town07_data = CustomImageDataset('ClearNoon','Town07',test=False)\n",
    "#flipped data\n",
    "town03_data_hf = CustomImageDataset('ClearNoon','Town03',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town03_test_data_hf = CustomImageDataset('ClearNoon','Town03',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town04_data_hf = CustomImageDataset('ClearNoon','Town04',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town04_test_data_hf = CustomImageDataset('ClearNoon','Town04',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town07_data_hf = CustomImageDataset('ClearNoon','Town07',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "\n",
    "train_data = ConcatDataset([town03_data,town04_data,town07_data,town03_data_hf,town04_data_hf,town07_data_hf])\n",
    "test_data = ConcatDataset([town03_test_data,town04_test_data,town03_test_data_hf,town04_test_data_hf])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate label frequency for loss weigthing\n",
    "label_freq_weights = np.zeros(13)\n",
    "count = 0\n",
    "for index, image in enumerate(train_data):\n",
    "    if index % 100 == 0:\n",
    "        im = recode_tags(image[1],recode_dict)\n",
    "        values, counts = np.unique(im,return_counts=True)\n",
    "        label_freq_weights[values] += counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_4 = np.array([1121034.,  530218.,  116395.,  193273.,   23201.,   83135.,\n",
    "        258966., 6886841.,  121457.,  777388.,   23087.,  230427.,\n",
    "         71186.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd54795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_3 = np.array([ 426548.,  103471.,   59734.,   61954.,    3507.,   17818.,\n",
    "         40000., 2355300.,   28709.,  355806.,    4624.,   49085.,\n",
    "         21033.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "67739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b77f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_2 = np.array([2198123.,  321519.,   74114.,  139571.,    9590.,   49337.,\n",
    "         94720., 1339440.,   78098.,  407971., 1242280.,  132659.,\n",
    "         40194.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a385913",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights_1 = np.array([  44733.,  115794.,  302399.,  278934.,   55560.,   64991.,\n",
    "        435785., 6489878.,  461536.,  765703., 4474860.,  176931.,\n",
    "         46304.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdb8dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lables = np.where(label_freq_weights_3 < 10000, 15000, label_freq_weights_3)\n",
    "\n",
    "inverse_lables = 1/clean_lables\n",
    "normalized_lables = inverse_lables/sum(inverse_lables)\n",
    "lable_weights = torch.tensor(normalized_lables,dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0c9fdde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2106d01210>]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(normalized_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to caculate an individual weight for each sample for BCELoss\n",
    "def calculate_weights(im_batch):\n",
    "    weights = torch.empty(13)\n",
    "    counts = torch.bincount(im_batch[i].reshape(128*128),minlength=13).numpy()\n",
    "    counts = np.where(counts < 1, 1, counts)\n",
    "    weights = torch.tensor((1/counts)/sum(1/counts))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d760b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/70488 (0%)]\tLoss: 1.284768\n",
      "Train Epoch: 1 [512/70488 (1%)]\tLoss: 1.284189\n",
      "Train Epoch: 1 [1024/70488 (1%)]\tLoss: 1.283840\n",
      "Train Epoch: 1 [1536/70488 (2%)]\tLoss: 1.284064\n",
      "Train Epoch: 1 [2048/70488 (3%)]\tLoss: 1.284375\n",
      "Train Epoch: 1 [2560/70488 (4%)]\tLoss: 1.284043\n",
      "Train Epoch: 1 [3072/70488 (4%)]\tLoss: 1.284042\n",
      "Train Epoch: 1 [3584/70488 (5%)]\tLoss: 1.284260\n",
      "Train Epoch: 1 [4096/70488 (6%)]\tLoss: 1.284174\n",
      "Train Epoch: 1 [4608/70488 (7%)]\tLoss: 1.284216\n",
      "Train Epoch: 1 [5120/70488 (7%)]\tLoss: 1.283955\n",
      "Train Epoch: 1 [5632/70488 (8%)]\tLoss: 1.282320\n",
      "Train Epoch: 1 [6144/70488 (9%)]\tLoss: 1.283786\n",
      "Train Epoch: 1 [6656/70488 (9%)]\tLoss: 1.282474\n",
      "Train Epoch: 1 [7168/70488 (10%)]\tLoss: 1.283804\n",
      "Train Epoch: 1 [7680/70488 (11%)]\tLoss: 1.283552\n",
      "Train Epoch: 1 [8192/70488 (12%)]\tLoss: 1.283366\n",
      "Train Epoch: 1 [8704/70488 (12%)]\tLoss: 1.284470\n",
      "Train Epoch: 1 [9216/70488 (13%)]\tLoss: 1.283520\n",
      "Train Epoch: 1 [9728/70488 (14%)]\tLoss: 1.284803\n",
      "Train Epoch: 1 [10240/70488 (14%)]\tLoss: 1.284088\n",
      "Train Epoch: 1 [10752/70488 (15%)]\tLoss: 1.284277\n",
      "Train Epoch: 1 [11264/70488 (16%)]\tLoss: 1.284042\n",
      "Train Epoch: 1 [11776/70488 (17%)]\tLoss: 1.283195\n",
      "Train Epoch: 1 [12288/70488 (17%)]\tLoss: 1.283293\n",
      "Train Epoch: 1 [12800/70488 (18%)]\tLoss: 1.283389\n",
      "Train Epoch: 1 [13312/70488 (19%)]\tLoss: 1.284428\n",
      "Train Epoch: 1 [13824/70488 (20%)]\tLoss: 1.283434\n",
      "Train Epoch: 1 [14336/70488 (20%)]\tLoss: 1.284829\n",
      "Train Epoch: 1 [14848/70488 (21%)]\tLoss: 1.284837\n",
      "Train Epoch: 1 [15360/70488 (22%)]\tLoss: 1.283721\n",
      "Train Epoch: 1 [15872/70488 (22%)]\tLoss: 1.284071\n",
      "Train Epoch: 1 [16384/70488 (23%)]\tLoss: 1.283510\n",
      "Train Epoch: 1 [16896/70488 (24%)]\tLoss: 1.284655\n",
      "Train Epoch: 1 [17408/70488 (25%)]\tLoss: 1.283081\n",
      "Train Epoch: 1 [17920/70488 (25%)]\tLoss: 1.284074\n",
      "Train Epoch: 1 [18432/70488 (26%)]\tLoss: 1.284252\n",
      "Train Epoch: 1 [18944/70488 (27%)]\tLoss: 1.284061\n",
      "Train Epoch: 1 [19456/70488 (28%)]\tLoss: 1.283608\n",
      "Train Epoch: 1 [19968/70488 (28%)]\tLoss: 1.283893\n",
      "Train Epoch: 1 [20480/70488 (29%)]\tLoss: 1.283585\n",
      "Train Epoch: 1 [20992/70488 (30%)]\tLoss: 1.284002\n",
      "Train Epoch: 1 [21504/70488 (30%)]\tLoss: 1.283614\n",
      "Train Epoch: 1 [22016/70488 (31%)]\tLoss: 1.283593\n",
      "Train Epoch: 1 [22528/70488 (32%)]\tLoss: 1.284174\n",
      "Train Epoch: 1 [23040/70488 (33%)]\tLoss: 1.284547\n",
      "Train Epoch: 1 [23552/70488 (33%)]\tLoss: 1.283592\n",
      "Train Epoch: 1 [24064/70488 (34%)]\tLoss: 1.285297\n",
      "Train Epoch: 1 [24576/70488 (35%)]\tLoss: 1.284732\n",
      "Train Epoch: 1 [25088/70488 (36%)]\tLoss: 1.282740\n",
      "Train Epoch: 1 [25600/70488 (36%)]\tLoss: 1.284103\n",
      "Train Epoch: 1 [26112/70488 (37%)]\tLoss: 1.284120\n",
      "Train Epoch: 1 [26624/70488 (38%)]\tLoss: 1.283607\n",
      "Train Epoch: 1 [27136/70488 (38%)]\tLoss: 1.284300\n",
      "Train Epoch: 1 [27648/70488 (39%)]\tLoss: 1.285202\n",
      "Train Epoch: 1 [28160/70488 (40%)]\tLoss: 1.283276\n",
      "Train Epoch: 1 [28672/70488 (41%)]\tLoss: 1.282755\n",
      "Train Epoch: 1 [29184/70488 (41%)]\tLoss: 1.283075\n",
      "Train Epoch: 1 [29696/70488 (42%)]\tLoss: 1.283366\n",
      "Train Epoch: 1 [30208/70488 (43%)]\tLoss: 1.283696\n",
      "Train Epoch: 1 [30720/70488 (43%)]\tLoss: 1.284072\n",
      "Train Epoch: 1 [31232/70488 (44%)]\tLoss: 1.284663\n",
      "Train Epoch: 1 [31744/70488 (45%)]\tLoss: 1.284895\n",
      "Train Epoch: 1 [32256/70488 (46%)]\tLoss: 1.284022\n",
      "Train Epoch: 1 [32768/70488 (46%)]\tLoss: 1.284433\n",
      "Train Epoch: 1 [33280/70488 (47%)]\tLoss: 1.284249\n",
      "Train Epoch: 1 [33792/70488 (48%)]\tLoss: 1.284174\n",
      "Train Epoch: 1 [34304/70488 (49%)]\tLoss: 1.284332\n",
      "Train Epoch: 1 [34816/70488 (49%)]\tLoss: 1.283951\n",
      "Train Epoch: 1 [35328/70488 (50%)]\tLoss: 1.284381\n",
      "Train Epoch: 1 [35840/70488 (51%)]\tLoss: 1.284341\n",
      "Train Epoch: 1 [36352/70488 (51%)]\tLoss: 1.284597\n",
      "Train Epoch: 1 [36864/70488 (52%)]\tLoss: 1.283704\n",
      "Train Epoch: 1 [37376/70488 (53%)]\tLoss: 1.284248\n",
      "Train Epoch: 1 [37888/70488 (54%)]\tLoss: 1.284019\n",
      "Train Epoch: 1 [38400/70488 (54%)]\tLoss: 1.284211\n",
      "Train Epoch: 1 [38912/70488 (55%)]\tLoss: 1.282070\n",
      "Train Epoch: 1 [39424/70488 (56%)]\tLoss: 1.284119\n",
      "Train Epoch: 1 [39936/70488 (57%)]\tLoss: 1.283418\n",
      "Train Epoch: 1 [40448/70488 (57%)]\tLoss: 1.283279\n",
      "Train Epoch: 1 [40960/70488 (58%)]\tLoss: 1.285304\n",
      "Train Epoch: 1 [41472/70488 (59%)]\tLoss: 1.283565\n",
      "Train Epoch: 1 [41984/70488 (59%)]\tLoss: 1.283283\n",
      "Train Epoch: 1 [42496/70488 (60%)]\tLoss: 1.283898\n",
      "Train Epoch: 1 [43008/70488 (61%)]\tLoss: 1.284583\n",
      "Train Epoch: 1 [43520/70488 (62%)]\tLoss: 1.283850\n",
      "Train Epoch: 1 [44032/70488 (62%)]\tLoss: 1.283804\n",
      "Train Epoch: 1 [44544/70488 (63%)]\tLoss: 1.283844\n",
      "Train Epoch: 1 [45056/70488 (64%)]\tLoss: 1.284019\n",
      "Train Epoch: 1 [45568/70488 (64%)]\tLoss: 1.283273\n",
      "Train Epoch: 1 [46080/70488 (65%)]\tLoss: 1.283883\n",
      "Train Epoch: 1 [46592/70488 (66%)]\tLoss: 1.284494\n",
      "Train Epoch: 1 [47104/70488 (67%)]\tLoss: 1.284681\n",
      "Train Epoch: 1 [47616/70488 (67%)]\tLoss: 1.283707\n",
      "Train Epoch: 1 [48128/70488 (68%)]\tLoss: 1.283996\n",
      "Train Epoch: 1 [48640/70488 (69%)]\tLoss: 1.283276\n",
      "Train Epoch: 1 [49152/70488 (70%)]\tLoss: 1.283509\n",
      "Train Epoch: 1 [49664/70488 (70%)]\tLoss: 1.283738\n",
      "Train Epoch: 1 [50176/70488 (71%)]\tLoss: 1.284067\n",
      "Train Epoch: 1 [50688/70488 (72%)]\tLoss: 1.283552\n",
      "Train Epoch: 1 [51200/70488 (72%)]\tLoss: 1.284660\n",
      "Train Epoch: 1 [51712/70488 (73%)]\tLoss: 1.284958\n",
      "Train Epoch: 1 [52224/70488 (74%)]\tLoss: 1.284665\n",
      "Train Epoch: 1 [52736/70488 (75%)]\tLoss: 1.284995\n",
      "Train Epoch: 1 [53248/70488 (75%)]\tLoss: 1.285159\n",
      "Train Epoch: 1 [53760/70488 (76%)]\tLoss: 1.283951\n",
      "Train Epoch: 1 [54272/70488 (77%)]\tLoss: 1.284184\n",
      "Train Epoch: 1 [54784/70488 (78%)]\tLoss: 1.283942\n",
      "Train Epoch: 1 [55296/70488 (78%)]\tLoss: 1.282642\n",
      "Train Epoch: 1 [55808/70488 (79%)]\tLoss: 1.283971\n",
      "Train Epoch: 1 [56320/70488 (80%)]\tLoss: 1.284056\n",
      "Train Epoch: 1 [56832/70488 (80%)]\tLoss: 1.284300\n",
      "Train Epoch: 1 [57344/70488 (81%)]\tLoss: 1.283730\n",
      "Train Epoch: 1 [57856/70488 (82%)]\tLoss: 1.283813\n",
      "Train Epoch: 1 [58368/70488 (83%)]\tLoss: 1.284001\n",
      "Train Epoch: 1 [58880/70488 (83%)]\tLoss: 1.283837\n",
      "Train Epoch: 1 [59392/70488 (84%)]\tLoss: 1.283924\n",
      "Train Epoch: 1 [59904/70488 (85%)]\tLoss: 1.284514\n",
      "Train Epoch: 1 [60416/70488 (86%)]\tLoss: 1.284687\n",
      "Train Epoch: 1 [60928/70488 (86%)]\tLoss: 1.284087\n",
      "Train Epoch: 1 [61440/70488 (87%)]\tLoss: 1.283487\n",
      "Train Epoch: 1 [61952/70488 (88%)]\tLoss: 1.283985\n",
      "Train Epoch: 1 [62464/70488 (88%)]\tLoss: 1.283622\n",
      "Train Epoch: 1 [62976/70488 (89%)]\tLoss: 1.284314\n",
      "Train Epoch: 1 [63488/70488 (90%)]\tLoss: 1.283699\n",
      "Train Epoch: 1 [64000/70488 (91%)]\tLoss: 1.284829\n",
      "Train Epoch: 1 [64512/70488 (91%)]\tLoss: 1.283637\n",
      "Train Epoch: 1 [65024/70488 (92%)]\tLoss: 1.284142\n",
      "Train Epoch: 1 [65536/70488 (93%)]\tLoss: 1.283494\n",
      "Train Epoch: 1 [66048/70488 (93%)]\tLoss: 1.283388\n",
      "Train Epoch: 1 [66560/70488 (94%)]\tLoss: 1.283431\n",
      "Train Epoch: 1 [67072/70488 (95%)]\tLoss: 1.283832\n",
      "Train Epoch: 1 [67584/70488 (96%)]\tLoss: 1.284120\n",
      "Train Epoch: 1 [68096/70488 (96%)]\tLoss: 1.282481\n",
      "Train Epoch: 1 [68608/70488 (97%)]\tLoss: 1.283450\n",
      "Train Epoch: 1 [69120/70488 (98%)]\tLoss: 1.284769\n",
      "Train Epoch: 1 [69632/70488 (99%)]\tLoss: 1.283774\n",
      "Train Epoch: 1 [47128/70488 (99%)]\tLoss: 1.284538\n",
      "====> Epoch: 1 Average loss: 0.00502743\n",
      "====> Test set loss: 0.00515289\n",
      "Train Epoch: 2 [0/70488 (0%)]\tLoss: 1.284606\n",
      "Train Epoch: 2 [512/70488 (1%)]\tLoss: 1.283819\n",
      "Train Epoch: 2 [1024/70488 (1%)]\tLoss: 1.283846\n",
      "Train Epoch: 2 [1536/70488 (2%)]\tLoss: 1.283432\n",
      "Train Epoch: 2 [2048/70488 (3%)]\tLoss: 1.284551\n",
      "Train Epoch: 2 [2560/70488 (4%)]\tLoss: 1.283754\n",
      "Train Epoch: 2 [3072/70488 (4%)]\tLoss: 1.283415\n",
      "Train Epoch: 2 [3584/70488 (5%)]\tLoss: 1.283211\n",
      "Train Epoch: 2 [4096/70488 (6%)]\tLoss: 1.284088\n",
      "Train Epoch: 2 [4608/70488 (7%)]\tLoss: 1.284567\n",
      "Train Epoch: 2 [5120/70488 (7%)]\tLoss: 1.284203\n",
      "Train Epoch: 2 [5632/70488 (8%)]\tLoss: 1.284002\n",
      "Train Epoch: 2 [6144/70488 (9%)]\tLoss: 1.283696\n",
      "Train Epoch: 2 [6656/70488 (9%)]\tLoss: 1.284150\n",
      "Train Epoch: 2 [7168/70488 (10%)]\tLoss: 1.284679\n",
      "Train Epoch: 2 [7680/70488 (11%)]\tLoss: 1.284178\n",
      "Train Epoch: 2 [8192/70488 (12%)]\tLoss: 1.284330\n",
      "Train Epoch: 2 [8704/70488 (12%)]\tLoss: 1.284797\n",
      "Train Epoch: 2 [9216/70488 (13%)]\tLoss: 1.284237\n",
      "Train Epoch: 2 [9728/70488 (14%)]\tLoss: 1.283651\n",
      "Train Epoch: 2 [10240/70488 (14%)]\tLoss: 1.283987\n",
      "Train Epoch: 2 [10752/70488 (15%)]\tLoss: 1.284479\n",
      "Train Epoch: 2 [11264/70488 (16%)]\tLoss: 1.284394\n",
      "Train Epoch: 2 [11776/70488 (17%)]\tLoss: 1.284105\n",
      "Train Epoch: 2 [12288/70488 (17%)]\tLoss: 1.284015\n",
      "Train Epoch: 2 [12800/70488 (18%)]\tLoss: 1.283711\n",
      "Train Epoch: 2 [13312/70488 (19%)]\tLoss: 1.283224\n",
      "Train Epoch: 2 [13824/70488 (20%)]\tLoss: 1.284730\n",
      "Train Epoch: 2 [14336/70488 (20%)]\tLoss: 1.285086\n",
      "Train Epoch: 2 [14848/70488 (21%)]\tLoss: 1.283339\n",
      "Train Epoch: 2 [15360/70488 (22%)]\tLoss: 1.284291\n",
      "Train Epoch: 2 [15872/70488 (22%)]\tLoss: 1.284082\n",
      "Train Epoch: 2 [16384/70488 (23%)]\tLoss: 1.284420\n",
      "Train Epoch: 2 [16896/70488 (24%)]\tLoss: 1.283925\n",
      "Train Epoch: 2 [17408/70488 (25%)]\tLoss: 1.284091\n",
      "Train Epoch: 2 [17920/70488 (25%)]\tLoss: 1.283366\n",
      "Train Epoch: 2 [18432/70488 (26%)]\tLoss: 1.284033\n",
      "Train Epoch: 2 [18944/70488 (27%)]\tLoss: 1.282624\n",
      "Train Epoch: 2 [19456/70488 (28%)]\tLoss: 1.283343\n",
      "Train Epoch: 2 [19968/70488 (28%)]\tLoss: 1.283821\n",
      "Train Epoch: 2 [20480/70488 (29%)]\tLoss: 1.284228\n",
      "Train Epoch: 2 [20992/70488 (30%)]\tLoss: 1.284564\n",
      "Train Epoch: 2 [21504/70488 (30%)]\tLoss: 1.283496\n",
      "Train Epoch: 2 [22016/70488 (31%)]\tLoss: 1.283149\n",
      "Train Epoch: 2 [22528/70488 (32%)]\tLoss: 1.284055\n",
      "Train Epoch: 2 [23040/70488 (33%)]\tLoss: 1.284088\n",
      "Train Epoch: 2 [23552/70488 (33%)]\tLoss: 1.283734\n",
      "Train Epoch: 2 [24064/70488 (34%)]\tLoss: 1.283860\n",
      "Train Epoch: 2 [24576/70488 (35%)]\tLoss: 1.283459\n",
      "Train Epoch: 2 [25088/70488 (36%)]\tLoss: 1.283963\n",
      "Train Epoch: 2 [25600/70488 (36%)]\tLoss: 1.283936\n",
      "Train Epoch: 2 [26112/70488 (37%)]\tLoss: 1.283815\n",
      "Train Epoch: 2 [26624/70488 (38%)]\tLoss: 1.284776\n",
      "Train Epoch: 2 [27136/70488 (38%)]\tLoss: 1.283783\n",
      "Train Epoch: 2 [27648/70488 (39%)]\tLoss: 1.284651\n",
      "Train Epoch: 2 [28160/70488 (40%)]\tLoss: 1.283483\n",
      "Train Epoch: 2 [28672/70488 (41%)]\tLoss: 1.284592\n",
      "Train Epoch: 2 [29184/70488 (41%)]\tLoss: 1.284573\n",
      "Train Epoch: 2 [29696/70488 (42%)]\tLoss: 1.283691\n",
      "Train Epoch: 2 [30208/70488 (43%)]\tLoss: 1.283940\n",
      "Train Epoch: 2 [30720/70488 (43%)]\tLoss: 1.284805\n",
      "Train Epoch: 2 [31232/70488 (44%)]\tLoss: 1.283502\n",
      "Train Epoch: 2 [31744/70488 (45%)]\tLoss: 1.283109\n",
      "Train Epoch: 2 [32256/70488 (46%)]\tLoss: 1.283589\n",
      "Train Epoch: 2 [32768/70488 (46%)]\tLoss: 1.284019\n",
      "Train Epoch: 2 [33280/70488 (47%)]\tLoss: 1.283730\n",
      "Train Epoch: 2 [33792/70488 (48%)]\tLoss: 1.284710\n",
      "Train Epoch: 2 [34304/70488 (49%)]\tLoss: 1.284238\n",
      "Train Epoch: 2 [34816/70488 (49%)]\tLoss: 1.285046\n",
      "Train Epoch: 2 [35328/70488 (50%)]\tLoss: 1.284445\n",
      "Train Epoch: 2 [35840/70488 (51%)]\tLoss: 1.282676\n",
      "Train Epoch: 2 [36352/70488 (51%)]\tLoss: 1.283355\n",
      "Train Epoch: 2 [36864/70488 (52%)]\tLoss: 1.284581\n",
      "Train Epoch: 2 [37376/70488 (53%)]\tLoss: 1.284324\n",
      "Train Epoch: 2 [37888/70488 (54%)]\tLoss: 1.284008\n",
      "Train Epoch: 2 [38400/70488 (54%)]\tLoss: 1.282468\n",
      "Train Epoch: 2 [38912/70488 (55%)]\tLoss: 1.284148\n",
      "Train Epoch: 2 [39424/70488 (56%)]\tLoss: 1.284359\n",
      "Train Epoch: 2 [39936/70488 (57%)]\tLoss: 1.283275\n",
      "Train Epoch: 2 [40448/70488 (57%)]\tLoss: 1.284376\n",
      "Train Epoch: 2 [40960/70488 (58%)]\tLoss: 1.285116\n",
      "Train Epoch: 2 [41472/70488 (59%)]\tLoss: 1.283896\n",
      "Train Epoch: 2 [41984/70488 (59%)]\tLoss: 1.284150\n",
      "Train Epoch: 2 [42496/70488 (60%)]\tLoss: 1.283623\n",
      "Train Epoch: 2 [43008/70488 (61%)]\tLoss: 1.283938\n",
      "Train Epoch: 2 [43520/70488 (62%)]\tLoss: 1.283675\n",
      "Train Epoch: 2 [44032/70488 (62%)]\tLoss: 1.283553\n",
      "Train Epoch: 2 [44544/70488 (63%)]\tLoss: 1.284234\n",
      "Train Epoch: 2 [45056/70488 (64%)]\tLoss: 1.283730\n",
      "Train Epoch: 2 [45568/70488 (64%)]\tLoss: 1.284324\n",
      "Train Epoch: 2 [46080/70488 (65%)]\tLoss: 1.283577\n",
      "Train Epoch: 2 [46592/70488 (66%)]\tLoss: 1.283864\n",
      "Train Epoch: 2 [47104/70488 (67%)]\tLoss: 1.284458\n",
      "Train Epoch: 2 [47616/70488 (67%)]\tLoss: 1.282838\n",
      "Train Epoch: 2 [48128/70488 (68%)]\tLoss: 1.282629\n",
      "Train Epoch: 2 [48640/70488 (69%)]\tLoss: 1.283921\n",
      "Train Epoch: 2 [49152/70488 (70%)]\tLoss: 1.284671\n",
      "Train Epoch: 2 [49664/70488 (70%)]\tLoss: 1.283713\n",
      "Train Epoch: 2 [50176/70488 (71%)]\tLoss: 1.282384\n",
      "Train Epoch: 2 [50688/70488 (72%)]\tLoss: 1.283980\n",
      "Train Epoch: 2 [51200/70488 (72%)]\tLoss: 1.282789\n",
      "Train Epoch: 2 [51712/70488 (73%)]\tLoss: 1.283334\n",
      "Train Epoch: 2 [52224/70488 (74%)]\tLoss: 1.283724\n",
      "Train Epoch: 2 [52736/70488 (75%)]\tLoss: 1.284083\n",
      "Train Epoch: 2 [53248/70488 (75%)]\tLoss: 1.283960\n",
      "Train Epoch: 2 [53760/70488 (76%)]\tLoss: 1.283519\n",
      "Train Epoch: 2 [54272/70488 (77%)]\tLoss: 1.283139\n",
      "Train Epoch: 2 [54784/70488 (78%)]\tLoss: 1.283467\n",
      "Train Epoch: 2 [55296/70488 (78%)]\tLoss: 1.283924\n",
      "Train Epoch: 2 [55808/70488 (79%)]\tLoss: 1.283837\n",
      "Train Epoch: 2 [56320/70488 (80%)]\tLoss: 1.284737\n",
      "Train Epoch: 2 [56832/70488 (80%)]\tLoss: 1.282966\n",
      "Train Epoch: 2 [57344/70488 (81%)]\tLoss: 1.284636\n",
      "Train Epoch: 2 [57856/70488 (82%)]\tLoss: 1.284190\n",
      "Train Epoch: 2 [58368/70488 (83%)]\tLoss: 1.283252\n",
      "Train Epoch: 2 [58880/70488 (83%)]\tLoss: 1.283556\n",
      "Train Epoch: 2 [59392/70488 (84%)]\tLoss: 1.283822\n",
      "Train Epoch: 2 [59904/70488 (85%)]\tLoss: 1.284750\n",
      "Train Epoch: 2 [60416/70488 (86%)]\tLoss: 1.284349\n",
      "Train Epoch: 2 [60928/70488 (86%)]\tLoss: 1.285029\n",
      "Train Epoch: 2 [61440/70488 (87%)]\tLoss: 1.284484\n",
      "Train Epoch: 2 [61952/70488 (88%)]\tLoss: 1.282666\n",
      "Train Epoch: 2 [62464/70488 (88%)]\tLoss: 1.284043\n",
      "Train Epoch: 2 [62976/70488 (89%)]\tLoss: 1.284666\n",
      "Train Epoch: 2 [63488/70488 (90%)]\tLoss: 1.284372\n",
      "Train Epoch: 2 [64000/70488 (91%)]\tLoss: 1.283857\n",
      "Train Epoch: 2 [64512/70488 (91%)]\tLoss: 1.284516\n",
      "Train Epoch: 2 [65024/70488 (92%)]\tLoss: 1.283669\n",
      "Train Epoch: 2 [65536/70488 (93%)]\tLoss: 1.285062\n",
      "Train Epoch: 2 [66048/70488 (93%)]\tLoss: 1.284047\n",
      "Train Epoch: 2 [66560/70488 (94%)]\tLoss: 1.284293\n",
      "Train Epoch: 2 [67072/70488 (95%)]\tLoss: 1.284233\n",
      "Train Epoch: 2 [67584/70488 (96%)]\tLoss: 1.283443\n",
      "Train Epoch: 2 [68096/70488 (96%)]\tLoss: 1.283464\n",
      "Train Epoch: 2 [68608/70488 (97%)]\tLoss: 1.283784\n",
      "Train Epoch: 2 [69120/70488 (98%)]\tLoss: 1.284235\n",
      "Train Epoch: 2 [69632/70488 (99%)]\tLoss: 1.284583\n",
      "Train Epoch: 2 [47128/70488 (99%)]\tLoss: 1.284724\n",
      "====> Epoch: 2 Average loss: 0.00502742\n",
      "====> Test set loss: 0.00515300\n",
      "Train Epoch: 3 [0/70488 (0%)]\tLoss: 1.284863\n",
      "Train Epoch: 3 [512/70488 (1%)]\tLoss: 1.285395\n",
      "Train Epoch: 3 [1024/70488 (1%)]\tLoss: 1.283764\n",
      "Train Epoch: 3 [1536/70488 (2%)]\tLoss: 1.284385\n",
      "Train Epoch: 3 [2048/70488 (3%)]\tLoss: 1.284171\n",
      "Train Epoch: 3 [2560/70488 (4%)]\tLoss: 1.284603\n",
      "Train Epoch: 3 [3072/70488 (4%)]\tLoss: 1.284174\n",
      "Train Epoch: 3 [3584/70488 (5%)]\tLoss: 1.284119\n",
      "Train Epoch: 3 [4096/70488 (6%)]\tLoss: 1.282991\n",
      "Train Epoch: 3 [4608/70488 (7%)]\tLoss: 1.284425\n",
      "Train Epoch: 3 [5120/70488 (7%)]\tLoss: 1.283741\n",
      "Train Epoch: 3 [5632/70488 (8%)]\tLoss: 1.284239\n",
      "Train Epoch: 3 [6144/70488 (9%)]\tLoss: 1.283869\n",
      "Train Epoch: 3 [6656/70488 (9%)]\tLoss: 1.283075\n",
      "Train Epoch: 3 [7168/70488 (10%)]\tLoss: 1.284252\n",
      "Train Epoch: 3 [7680/70488 (11%)]\tLoss: 1.283266\n",
      "Train Epoch: 3 [8192/70488 (12%)]\tLoss: 1.284282\n",
      "Train Epoch: 3 [8704/70488 (12%)]\tLoss: 1.282996\n",
      "Train Epoch: 3 [9216/70488 (13%)]\tLoss: 1.283930\n",
      "Train Epoch: 3 [9728/70488 (14%)]\tLoss: 1.284651\n",
      "Train Epoch: 3 [10240/70488 (14%)]\tLoss: 1.284247\n",
      "Train Epoch: 3 [10752/70488 (15%)]\tLoss: 1.283104\n",
      "Train Epoch: 3 [11264/70488 (16%)]\tLoss: 1.284249\n",
      "Train Epoch: 3 [11776/70488 (17%)]\tLoss: 1.284467\n",
      "Train Epoch: 3 [12288/70488 (17%)]\tLoss: 1.283889\n",
      "Train Epoch: 3 [12800/70488 (18%)]\tLoss: 1.283326\n",
      "Train Epoch: 3 [13312/70488 (19%)]\tLoss: 1.284525\n",
      "Train Epoch: 3 [13824/70488 (20%)]\tLoss: 1.283927\n",
      "Train Epoch: 3 [14336/70488 (20%)]\tLoss: 1.283989\n",
      "Train Epoch: 3 [14848/70488 (21%)]\tLoss: 1.284209\n",
      "Train Epoch: 3 [15360/70488 (22%)]\tLoss: 1.284744\n",
      "Train Epoch: 3 [15872/70488 (22%)]\tLoss: 1.283540\n",
      "Train Epoch: 3 [16384/70488 (23%)]\tLoss: 1.283634\n",
      "Train Epoch: 3 [16896/70488 (24%)]\tLoss: 1.284762\n",
      "Train Epoch: 3 [17408/70488 (25%)]\tLoss: 1.283690\n",
      "Train Epoch: 3 [17920/70488 (25%)]\tLoss: 1.284090\n",
      "Train Epoch: 3 [18432/70488 (26%)]\tLoss: 1.284266\n",
      "Train Epoch: 3 [18944/70488 (27%)]\tLoss: 1.284555\n",
      "Train Epoch: 3 [19456/70488 (28%)]\tLoss: 1.284413\n",
      "Train Epoch: 3 [19968/70488 (28%)]\tLoss: 1.284988\n",
      "Train Epoch: 3 [20480/70488 (29%)]\tLoss: 1.283557\n",
      "Train Epoch: 3 [20992/70488 (30%)]\tLoss: 1.284398\n",
      "Train Epoch: 3 [21504/70488 (30%)]\tLoss: 1.284612\n",
      "Train Epoch: 3 [22016/70488 (31%)]\tLoss: 1.283397\n",
      "Train Epoch: 3 [22528/70488 (32%)]\tLoss: 1.284655\n",
      "Train Epoch: 3 [23040/70488 (33%)]\tLoss: 1.284361\n",
      "Train Epoch: 3 [23552/70488 (33%)]\tLoss: 1.283488\n",
      "Train Epoch: 3 [24064/70488 (34%)]\tLoss: 1.282547\n",
      "Train Epoch: 3 [24576/70488 (35%)]\tLoss: 1.282439\n",
      "Train Epoch: 3 [25088/70488 (36%)]\tLoss: 1.283154\n",
      "Train Epoch: 3 [25600/70488 (36%)]\tLoss: 1.283136\n",
      "Train Epoch: 3 [26112/70488 (37%)]\tLoss: 1.283171\n",
      "Train Epoch: 3 [26624/70488 (38%)]\tLoss: 1.283948\n",
      "Train Epoch: 3 [27136/70488 (38%)]\tLoss: 1.283778\n",
      "Train Epoch: 3 [27648/70488 (39%)]\tLoss: 1.284432\n",
      "Train Epoch: 3 [28160/70488 (40%)]\tLoss: 1.284796\n",
      "Train Epoch: 3 [28672/70488 (41%)]\tLoss: 1.284122\n",
      "Train Epoch: 3 [29184/70488 (41%)]\tLoss: 1.283991\n",
      "Train Epoch: 3 [29696/70488 (42%)]\tLoss: 1.284907\n",
      "Train Epoch: 3 [30208/70488 (43%)]\tLoss: 1.283556\n",
      "Train Epoch: 3 [30720/70488 (43%)]\tLoss: 1.283483\n",
      "Train Epoch: 3 [31232/70488 (44%)]\tLoss: 1.282138\n",
      "Train Epoch: 3 [31744/70488 (45%)]\tLoss: 1.283555\n",
      "Train Epoch: 3 [32256/70488 (46%)]\tLoss: 1.283814\n",
      "Train Epoch: 3 [32768/70488 (46%)]\tLoss: 1.284062\n",
      "Train Epoch: 3 [33280/70488 (47%)]\tLoss: 1.283649\n",
      "Train Epoch: 3 [33792/70488 (48%)]\tLoss: 1.283920\n",
      "Train Epoch: 3 [34304/70488 (49%)]\tLoss: 1.283524\n",
      "Train Epoch: 3 [34816/70488 (49%)]\tLoss: 1.283641\n",
      "Train Epoch: 3 [35328/70488 (50%)]\tLoss: 1.283394\n",
      "Train Epoch: 3 [35840/70488 (51%)]\tLoss: 1.283471\n",
      "Train Epoch: 3 [36352/70488 (51%)]\tLoss: 1.283902\n",
      "Train Epoch: 3 [36864/70488 (52%)]\tLoss: 1.284603\n",
      "Train Epoch: 3 [37376/70488 (53%)]\tLoss: 1.284350\n",
      "Train Epoch: 3 [37888/70488 (54%)]\tLoss: 1.284499\n",
      "Train Epoch: 3 [38400/70488 (54%)]\tLoss: 1.284299\n",
      "Train Epoch: 3 [38912/70488 (55%)]\tLoss: 1.283806\n",
      "Train Epoch: 3 [39424/70488 (56%)]\tLoss: 1.283902\n",
      "Train Epoch: 3 [39936/70488 (57%)]\tLoss: 1.283976\n",
      "Train Epoch: 3 [40448/70488 (57%)]\tLoss: 1.284052\n",
      "Train Epoch: 3 [40960/70488 (58%)]\tLoss: 1.283921\n",
      "Train Epoch: 3 [41472/70488 (59%)]\tLoss: 1.283622\n",
      "Train Epoch: 3 [41984/70488 (59%)]\tLoss: 1.283481\n",
      "Train Epoch: 3 [42496/70488 (60%)]\tLoss: 1.282727\n",
      "Train Epoch: 3 [43008/70488 (61%)]\tLoss: 1.283296\n",
      "Train Epoch: 3 [43520/70488 (62%)]\tLoss: 1.283149\n",
      "Train Epoch: 3 [44032/70488 (62%)]\tLoss: 1.284252\n",
      "Train Epoch: 3 [44544/70488 (63%)]\tLoss: 1.283458\n",
      "Train Epoch: 3 [45056/70488 (64%)]\tLoss: 1.283885\n",
      "Train Epoch: 3 [45568/70488 (64%)]\tLoss: 1.284482\n",
      "Train Epoch: 3 [46080/70488 (65%)]\tLoss: 1.283434\n",
      "Train Epoch: 3 [46592/70488 (66%)]\tLoss: 1.284569\n",
      "Train Epoch: 3 [47104/70488 (67%)]\tLoss: 1.283621\n",
      "Train Epoch: 3 [47616/70488 (67%)]\tLoss: 1.284018\n",
      "Train Epoch: 3 [48128/70488 (68%)]\tLoss: 1.283148\n",
      "Train Epoch: 3 [48640/70488 (69%)]\tLoss: 1.284314\n",
      "Train Epoch: 3 [49152/70488 (70%)]\tLoss: 1.284460\n",
      "Train Epoch: 3 [49664/70488 (70%)]\tLoss: 1.282866\n",
      "Train Epoch: 3 [50176/70488 (71%)]\tLoss: 1.284467\n",
      "Train Epoch: 3 [50688/70488 (72%)]\tLoss: 1.284820\n",
      "Train Epoch: 3 [51200/70488 (72%)]\tLoss: 1.284204\n",
      "Train Epoch: 3 [51712/70488 (73%)]\tLoss: 1.283846\n",
      "Train Epoch: 3 [52224/70488 (74%)]\tLoss: 1.283271\n",
      "Train Epoch: 3 [52736/70488 (75%)]\tLoss: 1.284121\n",
      "Train Epoch: 3 [53248/70488 (75%)]\tLoss: 1.284404\n",
      "Train Epoch: 3 [53760/70488 (76%)]\tLoss: 1.283668\n",
      "Train Epoch: 3 [54272/70488 (77%)]\tLoss: 1.284268\n",
      "Train Epoch: 3 [54784/70488 (78%)]\tLoss: 1.283536\n",
      "Train Epoch: 3 [55296/70488 (78%)]\tLoss: 1.284651\n",
      "Train Epoch: 3 [55808/70488 (79%)]\tLoss: 1.283879\n",
      "Train Epoch: 3 [56320/70488 (80%)]\tLoss: 1.283910\n",
      "Train Epoch: 3 [56832/70488 (80%)]\tLoss: 1.283619\n",
      "Train Epoch: 3 [57344/70488 (81%)]\tLoss: 1.284514\n",
      "Train Epoch: 3 [57856/70488 (82%)]\tLoss: 1.283188\n",
      "Train Epoch: 3 [58368/70488 (83%)]\tLoss: 1.283868\n",
      "Train Epoch: 3 [58880/70488 (83%)]\tLoss: 1.284090\n",
      "Train Epoch: 3 [59392/70488 (84%)]\tLoss: 1.284405\n",
      "Train Epoch: 3 [59904/70488 (85%)]\tLoss: 1.284326\n",
      "Train Epoch: 3 [60416/70488 (86%)]\tLoss: 1.283724\n",
      "Train Epoch: 3 [60928/70488 (86%)]\tLoss: 1.284288\n",
      "Train Epoch: 3 [61440/70488 (87%)]\tLoss: 1.284262\n",
      "Train Epoch: 3 [61952/70488 (88%)]\tLoss: 1.284301\n",
      "Train Epoch: 3 [62464/70488 (88%)]\tLoss: 1.282235\n",
      "Train Epoch: 3 [62976/70488 (89%)]\tLoss: 1.283619\n",
      "Train Epoch: 3 [63488/70488 (90%)]\tLoss: 1.283334\n",
      "Train Epoch: 3 [64000/70488 (91%)]\tLoss: 1.285219\n",
      "Train Epoch: 3 [64512/70488 (91%)]\tLoss: 1.284534\n",
      "Train Epoch: 3 [65024/70488 (92%)]\tLoss: 1.284357\n",
      "Train Epoch: 3 [65536/70488 (93%)]\tLoss: 1.284190\n",
      "Train Epoch: 3 [66048/70488 (93%)]\tLoss: 1.283912\n",
      "Train Epoch: 3 [66560/70488 (94%)]\tLoss: 1.284431\n",
      "Train Epoch: 3 [67072/70488 (95%)]\tLoss: 1.284493\n",
      "Train Epoch: 3 [67584/70488 (96%)]\tLoss: 1.284989\n",
      "Train Epoch: 3 [68096/70488 (96%)]\tLoss: 1.284400\n",
      "Train Epoch: 3 [68608/70488 (97%)]\tLoss: 1.284026\n",
      "Train Epoch: 3 [69120/70488 (98%)]\tLoss: 1.283677\n",
      "Train Epoch: 3 [69632/70488 (99%)]\tLoss: 1.283970\n",
      "Train Epoch: 3 [47128/70488 (99%)]\tLoss: 1.284727\n",
      "====> Epoch: 3 Average loss: 0.00502743\n",
      "====> Test set loss: 0.00515264\n",
      "Train Epoch: 4 [0/70488 (0%)]\tLoss: 1.283097\n",
      "Train Epoch: 4 [512/70488 (1%)]\tLoss: 1.283742\n",
      "Train Epoch: 4 [1024/70488 (1%)]\tLoss: 1.283680\n",
      "Train Epoch: 4 [1536/70488 (2%)]\tLoss: 1.284114\n",
      "Train Epoch: 4 [2048/70488 (3%)]\tLoss: 1.284255\n",
      "Train Epoch: 4 [2560/70488 (4%)]\tLoss: 1.284076\n",
      "Train Epoch: 4 [3072/70488 (4%)]\tLoss: 1.283934\n",
      "Train Epoch: 4 [3584/70488 (5%)]\tLoss: 1.283174\n",
      "Train Epoch: 4 [4096/70488 (6%)]\tLoss: 1.284119\n",
      "Train Epoch: 4 [4608/70488 (7%)]\tLoss: 1.284367\n",
      "Train Epoch: 4 [5120/70488 (7%)]\tLoss: 1.284091\n",
      "Train Epoch: 4 [5632/70488 (8%)]\tLoss: 1.282631\n",
      "Train Epoch: 4 [6144/70488 (9%)]\tLoss: 1.283573\n",
      "Train Epoch: 4 [6656/70488 (9%)]\tLoss: 1.284719\n",
      "Train Epoch: 4 [7168/70488 (10%)]\tLoss: 1.284862\n",
      "Train Epoch: 4 [7680/70488 (11%)]\tLoss: 1.284329\n",
      "Train Epoch: 4 [8192/70488 (12%)]\tLoss: 1.284209\n",
      "Train Epoch: 4 [8704/70488 (12%)]\tLoss: 1.283340\n",
      "Train Epoch: 4 [9216/70488 (13%)]\tLoss: 1.283314\n",
      "Train Epoch: 4 [9728/70488 (14%)]\tLoss: 1.284044\n",
      "Train Epoch: 4 [10240/70488 (14%)]\tLoss: 1.283248\n",
      "Train Epoch: 4 [10752/70488 (15%)]\tLoss: 1.284844\n",
      "Train Epoch: 4 [11264/70488 (16%)]\tLoss: 1.284613\n",
      "Train Epoch: 4 [11776/70488 (17%)]\tLoss: 1.283763\n",
      "Train Epoch: 4 [12288/70488 (17%)]\tLoss: 1.283696\n",
      "Train Epoch: 4 [12800/70488 (18%)]\tLoss: 1.283500\n",
      "Train Epoch: 4 [13312/70488 (19%)]\tLoss: 1.284804\n",
      "Train Epoch: 4 [13824/70488 (20%)]\tLoss: 1.283957\n",
      "Train Epoch: 4 [14336/70488 (20%)]\tLoss: 1.283586\n",
      "Train Epoch: 4 [14848/70488 (21%)]\tLoss: 1.284046\n",
      "Train Epoch: 4 [15360/70488 (22%)]\tLoss: 1.283813\n",
      "Train Epoch: 4 [15872/70488 (22%)]\tLoss: 1.283776\n",
      "Train Epoch: 4 [16384/70488 (23%)]\tLoss: 1.284389\n",
      "Train Epoch: 4 [16896/70488 (24%)]\tLoss: 1.283705\n",
      "Train Epoch: 4 [17408/70488 (25%)]\tLoss: 1.284436\n",
      "Train Epoch: 4 [17920/70488 (25%)]\tLoss: 1.283570\n",
      "Train Epoch: 4 [18432/70488 (26%)]\tLoss: 1.283569\n",
      "Train Epoch: 4 [18944/70488 (27%)]\tLoss: 1.284189\n",
      "Train Epoch: 4 [19456/70488 (28%)]\tLoss: 1.283891\n",
      "Train Epoch: 4 [19968/70488 (28%)]\tLoss: 1.284038\n",
      "Train Epoch: 4 [20480/70488 (29%)]\tLoss: 1.282690\n",
      "Train Epoch: 4 [20992/70488 (30%)]\tLoss: 1.284997\n",
      "Train Epoch: 4 [21504/70488 (30%)]\tLoss: 1.284118\n",
      "Train Epoch: 4 [22016/70488 (31%)]\tLoss: 1.282868\n",
      "Train Epoch: 4 [22528/70488 (32%)]\tLoss: 1.284585\n",
      "Train Epoch: 4 [23040/70488 (33%)]\tLoss: 1.283736\n",
      "Train Epoch: 4 [23552/70488 (33%)]\tLoss: 1.284870\n",
      "Train Epoch: 4 [24064/70488 (34%)]\tLoss: 1.283789\n",
      "Train Epoch: 4 [24576/70488 (35%)]\tLoss: 1.284553\n",
      "Train Epoch: 4 [25088/70488 (36%)]\tLoss: 1.284651\n",
      "Train Epoch: 4 [25600/70488 (36%)]\tLoss: 1.283711\n",
      "Train Epoch: 4 [26112/70488 (37%)]\tLoss: 1.283231\n",
      "Train Epoch: 4 [26624/70488 (38%)]\tLoss: 1.284264\n",
      "Train Epoch: 4 [27136/70488 (38%)]\tLoss: 1.284309\n",
      "Train Epoch: 4 [27648/70488 (39%)]\tLoss: 1.282997\n",
      "Train Epoch: 4 [28160/70488 (40%)]\tLoss: 1.282605\n",
      "Train Epoch: 4 [28672/70488 (41%)]\tLoss: 1.283908\n",
      "Train Epoch: 4 [29184/70488 (41%)]\tLoss: 1.283658\n",
      "Train Epoch: 4 [29696/70488 (42%)]\tLoss: 1.284292\n",
      "Train Epoch: 4 [30208/70488 (43%)]\tLoss: 1.284397\n",
      "Train Epoch: 4 [30720/70488 (43%)]\tLoss: 1.283453\n",
      "Train Epoch: 4 [31232/70488 (44%)]\tLoss: 1.283481\n",
      "Train Epoch: 4 [31744/70488 (45%)]\tLoss: 1.284400\n",
      "Train Epoch: 4 [32256/70488 (46%)]\tLoss: 1.283760\n",
      "Train Epoch: 4 [32768/70488 (46%)]\tLoss: 1.283416\n",
      "Train Epoch: 4 [33280/70488 (47%)]\tLoss: 1.284161\n",
      "Train Epoch: 4 [33792/70488 (48%)]\tLoss: 1.284092\n",
      "Train Epoch: 4 [34304/70488 (49%)]\tLoss: 1.283846\n",
      "Train Epoch: 4 [34816/70488 (49%)]\tLoss: 1.283273\n",
      "Train Epoch: 4 [35328/70488 (50%)]\tLoss: 1.284257\n",
      "Train Epoch: 4 [35840/70488 (51%)]\tLoss: 1.284135\n",
      "Train Epoch: 4 [36352/70488 (51%)]\tLoss: 1.282868\n",
      "Train Epoch: 4 [36864/70488 (52%)]\tLoss: 1.284650\n",
      "Train Epoch: 4 [37376/70488 (53%)]\tLoss: 1.282371\n",
      "Train Epoch: 4 [37888/70488 (54%)]\tLoss: 1.284180\n",
      "Train Epoch: 4 [38400/70488 (54%)]\tLoss: 1.283904\n",
      "Train Epoch: 4 [38912/70488 (55%)]\tLoss: 1.284491\n",
      "Train Epoch: 4 [39424/70488 (56%)]\tLoss: 1.283616\n",
      "Train Epoch: 4 [39936/70488 (57%)]\tLoss: 1.283376\n",
      "Train Epoch: 4 [40448/70488 (57%)]\tLoss: 1.283206\n",
      "Train Epoch: 4 [40960/70488 (58%)]\tLoss: 1.284043\n",
      "Train Epoch: 4 [41472/70488 (59%)]\tLoss: 1.284212\n",
      "Train Epoch: 4 [41984/70488 (59%)]\tLoss: 1.284487\n",
      "Train Epoch: 4 [42496/70488 (60%)]\tLoss: 1.283978\n",
      "Train Epoch: 4 [43008/70488 (61%)]\tLoss: 1.283673\n",
      "Train Epoch: 4 [43520/70488 (62%)]\tLoss: 1.284121\n",
      "Train Epoch: 4 [44032/70488 (62%)]\tLoss: 1.284525\n",
      "Train Epoch: 4 [44544/70488 (63%)]\tLoss: 1.284877\n",
      "Train Epoch: 4 [45056/70488 (64%)]\tLoss: 1.284092\n",
      "Train Epoch: 4 [45568/70488 (64%)]\tLoss: 1.284574\n",
      "Train Epoch: 4 [46080/70488 (65%)]\tLoss: 1.284866\n",
      "Train Epoch: 4 [46592/70488 (66%)]\tLoss: 1.282648\n",
      "Train Epoch: 4 [47104/70488 (67%)]\tLoss: 1.284203\n",
      "Train Epoch: 4 [47616/70488 (67%)]\tLoss: 1.284739\n",
      "Train Epoch: 4 [48128/70488 (68%)]\tLoss: 1.283814\n",
      "Train Epoch: 4 [48640/70488 (69%)]\tLoss: 1.283942\n",
      "Train Epoch: 4 [49152/70488 (70%)]\tLoss: 1.284041\n",
      "Train Epoch: 4 [49664/70488 (70%)]\tLoss: 1.284221\n",
      "Train Epoch: 4 [50176/70488 (71%)]\tLoss: 1.284047\n",
      "Train Epoch: 4 [50688/70488 (72%)]\tLoss: 1.283967\n",
      "Train Epoch: 4 [51200/70488 (72%)]\tLoss: 1.282447\n",
      "Train Epoch: 4 [51712/70488 (73%)]\tLoss: 1.283119\n",
      "Train Epoch: 4 [52224/70488 (74%)]\tLoss: 1.283254\n",
      "Train Epoch: 4 [52736/70488 (75%)]\tLoss: 1.284733\n",
      "Train Epoch: 4 [53248/70488 (75%)]\tLoss: 1.283924\n",
      "Train Epoch: 4 [53760/70488 (76%)]\tLoss: 1.284413\n",
      "Train Epoch: 4 [54272/70488 (77%)]\tLoss: 1.284022\n",
      "Train Epoch: 4 [54784/70488 (78%)]\tLoss: 1.285348\n",
      "Train Epoch: 4 [55296/70488 (78%)]\tLoss: 1.283885\n",
      "Train Epoch: 4 [55808/70488 (79%)]\tLoss: 1.283853\n",
      "Train Epoch: 4 [56320/70488 (80%)]\tLoss: 1.285327\n",
      "Train Epoch: 4 [56832/70488 (80%)]\tLoss: 1.284302\n",
      "Train Epoch: 4 [57344/70488 (81%)]\tLoss: 1.283627\n",
      "Train Epoch: 4 [57856/70488 (82%)]\tLoss: 1.283705\n",
      "Train Epoch: 4 [58368/70488 (83%)]\tLoss: 1.283921\n",
      "Train Epoch: 4 [58880/70488 (83%)]\tLoss: 1.283543\n",
      "Train Epoch: 4 [59392/70488 (84%)]\tLoss: 1.282892\n",
      "Train Epoch: 4 [59904/70488 (85%)]\tLoss: 1.283825\n",
      "Train Epoch: 4 [60416/70488 (86%)]\tLoss: 1.283774\n",
      "Train Epoch: 4 [60928/70488 (86%)]\tLoss: 1.283844\n",
      "Train Epoch: 4 [61440/70488 (87%)]\tLoss: 1.284294\n",
      "Train Epoch: 4 [61952/70488 (88%)]\tLoss: 1.284195\n",
      "Train Epoch: 4 [62464/70488 (88%)]\tLoss: 1.284318\n",
      "Train Epoch: 4 [62976/70488 (89%)]\tLoss: 1.284260\n",
      "Train Epoch: 4 [63488/70488 (90%)]\tLoss: 1.284272\n",
      "Train Epoch: 4 [64000/70488 (91%)]\tLoss: 1.283480\n",
      "Train Epoch: 4 [64512/70488 (91%)]\tLoss: 1.284857\n",
      "Train Epoch: 4 [65024/70488 (92%)]\tLoss: 1.284363\n",
      "Train Epoch: 4 [65536/70488 (93%)]\tLoss: 1.283963\n",
      "Train Epoch: 4 [66048/70488 (93%)]\tLoss: 1.284328\n",
      "Train Epoch: 4 [66560/70488 (94%)]\tLoss: 1.283982\n",
      "Train Epoch: 4 [67072/70488 (95%)]\tLoss: 1.283740\n",
      "Train Epoch: 4 [67584/70488 (96%)]\tLoss: 1.284499\n",
      "Train Epoch: 4 [68096/70488 (96%)]\tLoss: 1.284368\n",
      "Train Epoch: 4 [68608/70488 (97%)]\tLoss: 1.283315\n",
      "Train Epoch: 4 [69120/70488 (98%)]\tLoss: 1.284672\n",
      "Train Epoch: 4 [69632/70488 (99%)]\tLoss: 1.284680\n",
      "Train Epoch: 4 [47128/70488 (99%)]\tLoss: 1.283866\n",
      "====> Epoch: 4 Average loss: 0.00502742\n",
      "====> Test set loss: 0.00515284\n",
      "Train Epoch: 5 [0/70488 (0%)]\tLoss: 1.284247\n",
      "Train Epoch: 5 [512/70488 (1%)]\tLoss: 1.283262\n",
      "Train Epoch: 5 [1024/70488 (1%)]\tLoss: 1.283039\n",
      "Train Epoch: 5 [1536/70488 (2%)]\tLoss: 1.284454\n",
      "Train Epoch: 5 [2048/70488 (3%)]\tLoss: 1.283783\n",
      "Train Epoch: 5 [2560/70488 (4%)]\tLoss: 1.283727\n",
      "Train Epoch: 5 [3072/70488 (4%)]\tLoss: 1.284224\n",
      "Train Epoch: 5 [3584/70488 (5%)]\tLoss: 1.283274\n",
      "Train Epoch: 5 [4096/70488 (6%)]\tLoss: 1.284163\n",
      "Train Epoch: 5 [4608/70488 (7%)]\tLoss: 1.284375\n",
      "Train Epoch: 5 [5120/70488 (7%)]\tLoss: 1.283350\n",
      "Train Epoch: 5 [5632/70488 (8%)]\tLoss: 1.283902\n",
      "Train Epoch: 5 [6144/70488 (9%)]\tLoss: 1.283516\n",
      "Train Epoch: 5 [6656/70488 (9%)]\tLoss: 1.284636\n",
      "Train Epoch: 5 [7168/70488 (10%)]\tLoss: 1.283978\n",
      "Train Epoch: 5 [7680/70488 (11%)]\tLoss: 1.284372\n",
      "Train Epoch: 5 [8192/70488 (12%)]\tLoss: 1.284039\n",
      "Train Epoch: 5 [8704/70488 (12%)]\tLoss: 1.284286\n",
      "Train Epoch: 5 [9216/70488 (13%)]\tLoss: 1.285151\n",
      "Train Epoch: 5 [9728/70488 (14%)]\tLoss: 1.284800\n",
      "Train Epoch: 5 [10240/70488 (14%)]\tLoss: 1.284305\n",
      "Train Epoch: 5 [10752/70488 (15%)]\tLoss: 1.284329\n",
      "Train Epoch: 5 [11264/70488 (16%)]\tLoss: 1.284354\n",
      "Train Epoch: 5 [11776/70488 (17%)]\tLoss: 1.284475\n",
      "Train Epoch: 5 [12288/70488 (17%)]\tLoss: 1.285010\n",
      "Train Epoch: 5 [12800/70488 (18%)]\tLoss: 1.283924\n",
      "Train Epoch: 5 [13312/70488 (19%)]\tLoss: 1.284641\n",
      "Train Epoch: 5 [13824/70488 (20%)]\tLoss: 1.283632\n",
      "Train Epoch: 5 [14336/70488 (20%)]\tLoss: 1.284722\n",
      "Train Epoch: 5 [14848/70488 (21%)]\tLoss: 1.284661\n",
      "Train Epoch: 5 [15360/70488 (22%)]\tLoss: 1.283807\n",
      "Train Epoch: 5 [15872/70488 (22%)]\tLoss: 1.283690\n",
      "Train Epoch: 5 [16384/70488 (23%)]\tLoss: 1.283984\n",
      "Train Epoch: 5 [16896/70488 (24%)]\tLoss: 1.283401\n",
      "Train Epoch: 5 [17408/70488 (25%)]\tLoss: 1.283609\n",
      "Train Epoch: 5 [17920/70488 (25%)]\tLoss: 1.284073\n",
      "Train Epoch: 5 [18432/70488 (26%)]\tLoss: 1.283650\n",
      "Train Epoch: 5 [18944/70488 (27%)]\tLoss: 1.285004\n",
      "Train Epoch: 5 [19456/70488 (28%)]\tLoss: 1.284588\n",
      "Train Epoch: 5 [19968/70488 (28%)]\tLoss: 1.284750\n",
      "Train Epoch: 5 [20480/70488 (29%)]\tLoss: 1.284360\n",
      "Train Epoch: 5 [20992/70488 (30%)]\tLoss: 1.282560\n",
      "Train Epoch: 5 [21504/70488 (30%)]\tLoss: 1.282383\n",
      "Train Epoch: 5 [22016/70488 (31%)]\tLoss: 1.282818\n",
      "Train Epoch: 5 [22528/70488 (32%)]\tLoss: 1.285382\n",
      "Train Epoch: 5 [23040/70488 (33%)]\tLoss: 1.284689\n",
      "Train Epoch: 5 [23552/70488 (33%)]\tLoss: 1.284126\n",
      "Train Epoch: 5 [24064/70488 (34%)]\tLoss: 1.284023\n",
      "Train Epoch: 5 [24576/70488 (35%)]\tLoss: 1.283154\n",
      "Train Epoch: 5 [25088/70488 (36%)]\tLoss: 1.284101\n",
      "Train Epoch: 5 [25600/70488 (36%)]\tLoss: 1.283151\n",
      "Train Epoch: 5 [26112/70488 (37%)]\tLoss: 1.282023\n",
      "Train Epoch: 5 [26624/70488 (38%)]\tLoss: 1.284292\n",
      "Train Epoch: 5 [27136/70488 (38%)]\tLoss: 1.283933\n",
      "Train Epoch: 5 [27648/70488 (39%)]\tLoss: 1.284558\n",
      "Train Epoch: 5 [28160/70488 (40%)]\tLoss: 1.284326\n",
      "Train Epoch: 5 [28672/70488 (41%)]\tLoss: 1.283748\n",
      "Train Epoch: 5 [29184/70488 (41%)]\tLoss: 1.284440\n",
      "Train Epoch: 5 [29696/70488 (42%)]\tLoss: 1.284949\n",
      "Train Epoch: 5 [30208/70488 (43%)]\tLoss: 1.282545\n",
      "Train Epoch: 5 [30720/70488 (43%)]\tLoss: 1.283506\n",
      "Train Epoch: 5 [31232/70488 (44%)]\tLoss: 1.283421\n",
      "Train Epoch: 5 [31744/70488 (45%)]\tLoss: 1.284671\n",
      "Train Epoch: 5 [32256/70488 (46%)]\tLoss: 1.283768\n",
      "Train Epoch: 5 [32768/70488 (46%)]\tLoss: 1.284294\n",
      "Train Epoch: 5 [33280/70488 (47%)]\tLoss: 1.283189\n",
      "Train Epoch: 5 [33792/70488 (48%)]\tLoss: 1.283784\n",
      "Train Epoch: 5 [34304/70488 (49%)]\tLoss: 1.284000\n",
      "Train Epoch: 5 [34816/70488 (49%)]\tLoss: 1.284391\n",
      "Train Epoch: 5 [35328/70488 (50%)]\tLoss: 1.283477\n",
      "Train Epoch: 5 [35840/70488 (51%)]\tLoss: 1.284502\n",
      "Train Epoch: 5 [36352/70488 (51%)]\tLoss: 1.284867\n",
      "Train Epoch: 5 [36864/70488 (52%)]\tLoss: 1.283033\n",
      "Train Epoch: 5 [37376/70488 (53%)]\tLoss: 1.283368\n",
      "Train Epoch: 5 [37888/70488 (54%)]\tLoss: 1.284835\n",
      "Train Epoch: 5 [38400/70488 (54%)]\tLoss: 1.284282\n",
      "Train Epoch: 5 [38912/70488 (55%)]\tLoss: 1.284707\n",
      "Train Epoch: 5 [39424/70488 (56%)]\tLoss: 1.284925\n",
      "Train Epoch: 5 [39936/70488 (57%)]\tLoss: 1.284034\n",
      "Train Epoch: 5 [40448/70488 (57%)]\tLoss: 1.284123\n",
      "Train Epoch: 5 [40960/70488 (58%)]\tLoss: 1.283275\n",
      "Train Epoch: 5 [41472/70488 (59%)]\tLoss: 1.284697\n",
      "Train Epoch: 5 [41984/70488 (59%)]\tLoss: 1.283427\n",
      "Train Epoch: 5 [42496/70488 (60%)]\tLoss: 1.283752\n",
      "Train Epoch: 5 [43008/70488 (61%)]\tLoss: 1.283209\n",
      "Train Epoch: 5 [43520/70488 (62%)]\tLoss: 1.284637\n",
      "Train Epoch: 5 [44032/70488 (62%)]\tLoss: 1.283921\n",
      "Train Epoch: 5 [44544/70488 (63%)]\tLoss: 1.284530\n",
      "Train Epoch: 5 [45056/70488 (64%)]\tLoss: 1.283167\n",
      "Train Epoch: 5 [45568/70488 (64%)]\tLoss: 1.283209\n",
      "Train Epoch: 5 [46080/70488 (65%)]\tLoss: 1.284699\n",
      "Train Epoch: 5 [46592/70488 (66%)]\tLoss: 1.283688\n",
      "Train Epoch: 5 [47104/70488 (67%)]\tLoss: 1.283746\n",
      "Train Epoch: 5 [47616/70488 (67%)]\tLoss: 1.284115\n",
      "Train Epoch: 5 [48128/70488 (68%)]\tLoss: 1.283425\n",
      "Train Epoch: 5 [48640/70488 (69%)]\tLoss: 1.283161\n",
      "Train Epoch: 5 [49152/70488 (70%)]\tLoss: 1.283621\n",
      "Train Epoch: 5 [49664/70488 (70%)]\tLoss: 1.283830\n",
      "Train Epoch: 5 [50176/70488 (71%)]\tLoss: 1.284461\n",
      "Train Epoch: 5 [50688/70488 (72%)]\tLoss: 1.283269\n",
      "Train Epoch: 5 [51200/70488 (72%)]\tLoss: 1.283992\n",
      "Train Epoch: 5 [51712/70488 (73%)]\tLoss: 1.284197\n",
      "Train Epoch: 5 [52224/70488 (74%)]\tLoss: 1.284607\n",
      "Train Epoch: 5 [52736/70488 (75%)]\tLoss: 1.283018\n",
      "Train Epoch: 5 [53248/70488 (75%)]\tLoss: 1.283982\n",
      "Train Epoch: 5 [53760/70488 (76%)]\tLoss: 1.284197\n",
      "Train Epoch: 5 [54272/70488 (77%)]\tLoss: 1.283974\n",
      "Train Epoch: 5 [54784/70488 (78%)]\tLoss: 1.284505\n",
      "Train Epoch: 5 [55296/70488 (78%)]\tLoss: 1.284490\n",
      "Train Epoch: 5 [55808/70488 (79%)]\tLoss: 1.283205\n",
      "Train Epoch: 5 [56320/70488 (80%)]\tLoss: 1.284661\n",
      "Train Epoch: 5 [56832/70488 (80%)]\tLoss: 1.284289\n",
      "Train Epoch: 5 [57344/70488 (81%)]\tLoss: 1.284067\n",
      "Train Epoch: 5 [57856/70488 (82%)]\tLoss: 1.283074\n",
      "Train Epoch: 5 [58368/70488 (83%)]\tLoss: 1.283024\n",
      "Train Epoch: 5 [58880/70488 (83%)]\tLoss: 1.284861\n",
      "Train Epoch: 5 [59392/70488 (84%)]\tLoss: 1.284569\n",
      "Train Epoch: 5 [59904/70488 (85%)]\tLoss: 1.284299\n",
      "Train Epoch: 5 [60416/70488 (86%)]\tLoss: 1.282954\n",
      "Train Epoch: 5 [60928/70488 (86%)]\tLoss: 1.282899\n",
      "Train Epoch: 5 [61440/70488 (87%)]\tLoss: 1.284356\n",
      "Train Epoch: 5 [61952/70488 (88%)]\tLoss: 1.284089\n",
      "Train Epoch: 5 [62464/70488 (88%)]\tLoss: 1.284441\n",
      "Train Epoch: 5 [62976/70488 (89%)]\tLoss: 1.283571\n",
      "Train Epoch: 5 [63488/70488 (90%)]\tLoss: 1.284342\n",
      "Train Epoch: 5 [64000/70488 (91%)]\tLoss: 1.282653\n",
      "Train Epoch: 5 [64512/70488 (91%)]\tLoss: 1.284335\n",
      "Train Epoch: 5 [65024/70488 (92%)]\tLoss: 1.282791\n",
      "Train Epoch: 5 [65536/70488 (93%)]\tLoss: 1.283617\n",
      "Train Epoch: 5 [66048/70488 (93%)]\tLoss: 1.283945\n",
      "Train Epoch: 5 [66560/70488 (94%)]\tLoss: 1.283859\n",
      "Train Epoch: 5 [67072/70488 (95%)]\tLoss: 1.283906\n",
      "Train Epoch: 5 [67584/70488 (96%)]\tLoss: 1.283827\n",
      "Train Epoch: 5 [68096/70488 (96%)]\tLoss: 1.283917\n",
      "Train Epoch: 5 [68608/70488 (97%)]\tLoss: 1.283257\n",
      "Train Epoch: 5 [69120/70488 (98%)]\tLoss: 1.284397\n",
      "Train Epoch: 5 [69632/70488 (99%)]\tLoss: 1.284466\n",
      "Train Epoch: 5 [47128/70488 (99%)]\tLoss: 1.284102\n",
      "====> Epoch: 5 Average loss: 0.00502742\n",
      "====> Test set loss: 0.00515302\n",
      "Train Epoch: 6 [0/70488 (0%)]\tLoss: 1.283879\n",
      "Train Epoch: 6 [512/70488 (1%)]\tLoss: 1.283825\n",
      "Train Epoch: 6 [1024/70488 (1%)]\tLoss: 1.284428\n",
      "Train Epoch: 6 [1536/70488 (2%)]\tLoss: 1.283522\n",
      "Train Epoch: 6 [2048/70488 (3%)]\tLoss: 1.284160\n",
      "Train Epoch: 6 [2560/70488 (4%)]\tLoss: 1.283593\n",
      "Train Epoch: 6 [3072/70488 (4%)]\tLoss: 1.282987\n",
      "Train Epoch: 6 [3584/70488 (5%)]\tLoss: 1.282789\n",
      "Train Epoch: 6 [4096/70488 (6%)]\tLoss: 1.284417\n",
      "Train Epoch: 6 [4608/70488 (7%)]\tLoss: 1.283602\n",
      "Train Epoch: 6 [5120/70488 (7%)]\tLoss: 1.283803\n",
      "Train Epoch: 6 [5632/70488 (8%)]\tLoss: 1.284106\n",
      "Train Epoch: 6 [6144/70488 (9%)]\tLoss: 1.284149\n",
      "Train Epoch: 6 [6656/70488 (9%)]\tLoss: 1.283670\n",
      "Train Epoch: 6 [7168/70488 (10%)]\tLoss: 1.283375\n",
      "Train Epoch: 6 [7680/70488 (11%)]\tLoss: 1.284075\n",
      "Train Epoch: 6 [8192/70488 (12%)]\tLoss: 1.282730\n",
      "Train Epoch: 6 [8704/70488 (12%)]\tLoss: 1.283874\n",
      "Train Epoch: 6 [9216/70488 (13%)]\tLoss: 1.284713\n",
      "Train Epoch: 6 [9728/70488 (14%)]\tLoss: 1.283543\n",
      "Train Epoch: 6 [10240/70488 (14%)]\tLoss: 1.283757\n",
      "Train Epoch: 6 [10752/70488 (15%)]\tLoss: 1.284570\n",
      "Train Epoch: 6 [11264/70488 (16%)]\tLoss: 1.284532\n",
      "Train Epoch: 6 [11776/70488 (17%)]\tLoss: 1.284172\n",
      "Train Epoch: 6 [12288/70488 (17%)]\tLoss: 1.284609\n",
      "Train Epoch: 6 [12800/70488 (18%)]\tLoss: 1.284481\n",
      "Train Epoch: 6 [13312/70488 (19%)]\tLoss: 1.285172\n",
      "Train Epoch: 6 [13824/70488 (20%)]\tLoss: 1.283366\n",
      "Train Epoch: 6 [14336/70488 (20%)]\tLoss: 1.283145\n",
      "Train Epoch: 6 [14848/70488 (21%)]\tLoss: 1.284133\n",
      "Train Epoch: 6 [15360/70488 (22%)]\tLoss: 1.283537\n",
      "Train Epoch: 6 [15872/70488 (22%)]\tLoss: 1.284529\n",
      "Train Epoch: 6 [16384/70488 (23%)]\tLoss: 1.284094\n",
      "Train Epoch: 6 [16896/70488 (24%)]\tLoss: 1.285129\n",
      "Train Epoch: 6 [17408/70488 (25%)]\tLoss: 1.283414\n",
      "Train Epoch: 6 [17920/70488 (25%)]\tLoss: 1.284255\n",
      "Train Epoch: 6 [18432/70488 (26%)]\tLoss: 1.283827\n",
      "Train Epoch: 6 [18944/70488 (27%)]\tLoss: 1.283877\n",
      "Train Epoch: 6 [19456/70488 (28%)]\tLoss: 1.284868\n",
      "Train Epoch: 6 [19968/70488 (28%)]\tLoss: 1.283704\n",
      "Train Epoch: 6 [20480/70488 (29%)]\tLoss: 1.284402\n",
      "Train Epoch: 6 [20992/70488 (30%)]\tLoss: 1.284221\n",
      "Train Epoch: 6 [21504/70488 (30%)]\tLoss: 1.284543\n",
      "Train Epoch: 6 [22016/70488 (31%)]\tLoss: 1.284124\n",
      "Train Epoch: 6 [22528/70488 (32%)]\tLoss: 1.284118\n",
      "Train Epoch: 6 [23040/70488 (33%)]\tLoss: 1.283618\n",
      "Train Epoch: 6 [23552/70488 (33%)]\tLoss: 1.284238\n",
      "Train Epoch: 6 [24064/70488 (34%)]\tLoss: 1.283405\n",
      "Train Epoch: 6 [24576/70488 (35%)]\tLoss: 1.283872\n",
      "Train Epoch: 6 [25088/70488 (36%)]\tLoss: 1.284109\n",
      "Train Epoch: 6 [25600/70488 (36%)]\tLoss: 1.283711\n",
      "Train Epoch: 6 [26112/70488 (37%)]\tLoss: 1.283374\n",
      "Train Epoch: 6 [26624/70488 (38%)]\tLoss: 1.284564\n",
      "Train Epoch: 6 [27136/70488 (38%)]\tLoss: 1.284028\n",
      "Train Epoch: 6 [27648/70488 (39%)]\tLoss: 1.283457\n",
      "Train Epoch: 6 [28160/70488 (40%)]\tLoss: 1.284398\n",
      "Train Epoch: 6 [28672/70488 (41%)]\tLoss: 1.283679\n",
      "Train Epoch: 6 [29184/70488 (41%)]\tLoss: 1.284002\n",
      "Train Epoch: 6 [29696/70488 (42%)]\tLoss: 1.283365\n",
      "Train Epoch: 6 [30208/70488 (43%)]\tLoss: 1.283822\n",
      "Train Epoch: 6 [30720/70488 (43%)]\tLoss: 1.284231\n",
      "Train Epoch: 6 [31232/70488 (44%)]\tLoss: 1.283072\n",
      "Train Epoch: 6 [31744/70488 (45%)]\tLoss: 1.283589\n",
      "Train Epoch: 6 [32256/70488 (46%)]\tLoss: 1.283602\n",
      "Train Epoch: 6 [32768/70488 (46%)]\tLoss: 1.283014\n",
      "Train Epoch: 6 [33280/70488 (47%)]\tLoss: 1.284931\n",
      "Train Epoch: 6 [33792/70488 (48%)]\tLoss: 1.285132\n",
      "Train Epoch: 6 [34304/70488 (49%)]\tLoss: 1.285115\n",
      "Train Epoch: 6 [34816/70488 (49%)]\tLoss: 1.283878\n",
      "Train Epoch: 6 [35328/70488 (50%)]\tLoss: 1.284469\n",
      "Train Epoch: 6 [35840/70488 (51%)]\tLoss: 1.284509\n",
      "Train Epoch: 6 [36352/70488 (51%)]\tLoss: 1.283993\n",
      "Train Epoch: 6 [36864/70488 (52%)]\tLoss: 1.283968\n",
      "Train Epoch: 6 [37376/70488 (53%)]\tLoss: 1.283906\n",
      "Train Epoch: 6 [37888/70488 (54%)]\tLoss: 1.284239\n",
      "Train Epoch: 6 [38400/70488 (54%)]\tLoss: 1.284567\n",
      "Train Epoch: 6 [38912/70488 (55%)]\tLoss: 1.283347\n",
      "Train Epoch: 6 [39424/70488 (56%)]\tLoss: 1.284146\n",
      "Train Epoch: 6 [39936/70488 (57%)]\tLoss: 1.284757\n",
      "Train Epoch: 6 [40448/70488 (57%)]\tLoss: 1.283863\n",
      "Train Epoch: 6 [40960/70488 (58%)]\tLoss: 1.283615\n",
      "Train Epoch: 6 [41472/70488 (59%)]\tLoss: 1.283638\n",
      "Train Epoch: 6 [41984/70488 (59%)]\tLoss: 1.284494\n",
      "Train Epoch: 6 [42496/70488 (60%)]\tLoss: 1.284364\n",
      "Train Epoch: 6 [43008/70488 (61%)]\tLoss: 1.282958\n",
      "Train Epoch: 6 [43520/70488 (62%)]\tLoss: 1.284658\n",
      "Train Epoch: 6 [44032/70488 (62%)]\tLoss: 1.283482\n",
      "Train Epoch: 6 [44544/70488 (63%)]\tLoss: 1.284234\n",
      "Train Epoch: 6 [45056/70488 (64%)]\tLoss: 1.284204\n",
      "Train Epoch: 6 [45568/70488 (64%)]\tLoss: 1.284174\n",
      "Train Epoch: 6 [46080/70488 (65%)]\tLoss: 1.284182\n",
      "Train Epoch: 6 [46592/70488 (66%)]\tLoss: 1.284207\n",
      "Train Epoch: 6 [47104/70488 (67%)]\tLoss: 1.284773\n",
      "Train Epoch: 6 [47616/70488 (67%)]\tLoss: 1.284134\n",
      "Train Epoch: 6 [48128/70488 (68%)]\tLoss: 1.283396\n",
      "Train Epoch: 6 [48640/70488 (69%)]\tLoss: 1.284675\n",
      "Train Epoch: 6 [49152/70488 (70%)]\tLoss: 1.284300\n",
      "Train Epoch: 6 [49664/70488 (70%)]\tLoss: 1.283407\n",
      "Train Epoch: 6 [50176/70488 (71%)]\tLoss: 1.283992\n",
      "Train Epoch: 6 [50688/70488 (72%)]\tLoss: 1.284382\n",
      "Train Epoch: 6 [51200/70488 (72%)]\tLoss: 1.283714\n",
      "Train Epoch: 6 [51712/70488 (73%)]\tLoss: 1.283270\n",
      "Train Epoch: 6 [52224/70488 (74%)]\tLoss: 1.282597\n",
      "Train Epoch: 6 [52736/70488 (75%)]\tLoss: 1.283842\n",
      "Train Epoch: 6 [53248/70488 (75%)]\tLoss: 1.283366\n",
      "Train Epoch: 6 [53760/70488 (76%)]\tLoss: 1.283577\n",
      "Train Epoch: 6 [54272/70488 (77%)]\tLoss: 1.284045\n",
      "Train Epoch: 6 [54784/70488 (78%)]\tLoss: 1.283966\n",
      "Train Epoch: 6 [55296/70488 (78%)]\tLoss: 1.284094\n",
      "Train Epoch: 6 [55808/70488 (79%)]\tLoss: 1.283976\n",
      "Train Epoch: 6 [56320/70488 (80%)]\tLoss: 1.283122\n",
      "Train Epoch: 6 [56832/70488 (80%)]\tLoss: 1.284042\n",
      "Train Epoch: 6 [57344/70488 (81%)]\tLoss: 1.284935\n",
      "Train Epoch: 6 [57856/70488 (82%)]\tLoss: 1.284039\n",
      "Train Epoch: 6 [58368/70488 (83%)]\tLoss: 1.283457\n",
      "Train Epoch: 6 [58880/70488 (83%)]\tLoss: 1.284500\n",
      "Train Epoch: 6 [59392/70488 (84%)]\tLoss: 1.282839\n",
      "Train Epoch: 6 [59904/70488 (85%)]\tLoss: 1.284115\n",
      "Train Epoch: 6 [60416/70488 (86%)]\tLoss: 1.283631\n",
      "Train Epoch: 6 [60928/70488 (86%)]\tLoss: 1.283850\n",
      "Train Epoch: 6 [61440/70488 (87%)]\tLoss: 1.284374\n",
      "Train Epoch: 6 [61952/70488 (88%)]\tLoss: 1.283813\n",
      "Train Epoch: 6 [62464/70488 (88%)]\tLoss: 1.284047\n",
      "Train Epoch: 6 [62976/70488 (89%)]\tLoss: 1.284175\n",
      "Train Epoch: 6 [63488/70488 (90%)]\tLoss: 1.283558\n",
      "Train Epoch: 6 [64000/70488 (91%)]\tLoss: 1.284282\n",
      "Train Epoch: 6 [64512/70488 (91%)]\tLoss: 1.283975\n",
      "Train Epoch: 6 [65024/70488 (92%)]\tLoss: 1.284666\n",
      "Train Epoch: 6 [65536/70488 (93%)]\tLoss: 1.282988\n",
      "Train Epoch: 6 [66048/70488 (93%)]\tLoss: 1.283988\n",
      "Train Epoch: 6 [66560/70488 (94%)]\tLoss: 1.283759\n",
      "Train Epoch: 6 [67072/70488 (95%)]\tLoss: 1.284080\n",
      "Train Epoch: 6 [67584/70488 (96%)]\tLoss: 1.283750\n",
      "Train Epoch: 6 [68096/70488 (96%)]\tLoss: 1.283996\n",
      "Train Epoch: 6 [68608/70488 (97%)]\tLoss: 1.284189\n",
      "Train Epoch: 6 [69120/70488 (98%)]\tLoss: 1.283567\n",
      "Train Epoch: 6 [69632/70488 (99%)]\tLoss: 1.283096\n",
      "Train Epoch: 6 [47128/70488 (99%)]\tLoss: 1.284286\n",
      "====> Epoch: 6 Average loss: 0.00502743\n",
      "====> Test set loss: 0.00515285\n",
      "Train Epoch: 7 [0/70488 (0%)]\tLoss: 1.283297\n",
      "Train Epoch: 7 [512/70488 (1%)]\tLoss: 1.284337\n",
      "Train Epoch: 7 [1024/70488 (1%)]\tLoss: 1.283540\n",
      "Train Epoch: 7 [1536/70488 (2%)]\tLoss: 1.283181\n",
      "Train Epoch: 7 [2048/70488 (3%)]\tLoss: 1.282743\n",
      "Train Epoch: 7 [2560/70488 (4%)]\tLoss: 1.284648\n",
      "Train Epoch: 7 [3072/70488 (4%)]\tLoss: 1.284357\n",
      "Train Epoch: 7 [3584/70488 (5%)]\tLoss: 1.283918\n",
      "Train Epoch: 7 [4096/70488 (6%)]\tLoss: 1.284507\n",
      "Train Epoch: 7 [4608/70488 (7%)]\tLoss: 1.284396\n",
      "Train Epoch: 7 [5120/70488 (7%)]\tLoss: 1.283948\n",
      "Train Epoch: 7 [5632/70488 (8%)]\tLoss: 1.285246\n",
      "Train Epoch: 7 [6144/70488 (9%)]\tLoss: 1.283547\n",
      "Train Epoch: 7 [6656/70488 (9%)]\tLoss: 1.284367\n",
      "Train Epoch: 7 [7168/70488 (10%)]\tLoss: 1.284691\n",
      "Train Epoch: 7 [7680/70488 (11%)]\tLoss: 1.283649\n",
      "Train Epoch: 7 [8192/70488 (12%)]\tLoss: 1.283988\n",
      "Train Epoch: 7 [8704/70488 (12%)]\tLoss: 1.284121\n",
      "Train Epoch: 7 [9216/70488 (13%)]\tLoss: 1.283532\n",
      "Train Epoch: 7 [9728/70488 (14%)]\tLoss: 1.283675\n",
      "Train Epoch: 7 [10240/70488 (14%)]\tLoss: 1.283893\n",
      "Train Epoch: 7 [10752/70488 (15%)]\tLoss: 1.284255\n",
      "Train Epoch: 7 [11264/70488 (16%)]\tLoss: 1.283587\n",
      "Train Epoch: 7 [11776/70488 (17%)]\tLoss: 1.283677\n",
      "Train Epoch: 7 [12288/70488 (17%)]\tLoss: 1.284180\n",
      "Train Epoch: 7 [12800/70488 (18%)]\tLoss: 1.283619\n",
      "Train Epoch: 7 [13312/70488 (19%)]\tLoss: 1.283920\n",
      "Train Epoch: 7 [13824/70488 (20%)]\tLoss: 1.283305\n",
      "Train Epoch: 7 [14336/70488 (20%)]\tLoss: 1.283909\n",
      "Train Epoch: 7 [14848/70488 (21%)]\tLoss: 1.284037\n",
      "Train Epoch: 7 [15360/70488 (22%)]\tLoss: 1.283795\n",
      "Train Epoch: 7 [15872/70488 (22%)]\tLoss: 1.283979\n",
      "Train Epoch: 7 [16384/70488 (23%)]\tLoss: 1.284492\n",
      "Train Epoch: 7 [16896/70488 (24%)]\tLoss: 1.283894\n",
      "Train Epoch: 7 [17408/70488 (25%)]\tLoss: 1.283984\n",
      "Train Epoch: 7 [17920/70488 (25%)]\tLoss: 1.284152\n",
      "Train Epoch: 7 [18432/70488 (26%)]\tLoss: 1.284564\n",
      "Train Epoch: 7 [18944/70488 (27%)]\tLoss: 1.283390\n",
      "Train Epoch: 7 [19456/70488 (28%)]\tLoss: 1.284500\n",
      "Train Epoch: 7 [19968/70488 (28%)]\tLoss: 1.284104\n",
      "Train Epoch: 7 [20480/70488 (29%)]\tLoss: 1.284176\n",
      "Train Epoch: 7 [20992/70488 (30%)]\tLoss: 1.284352\n",
      "Train Epoch: 7 [21504/70488 (30%)]\tLoss: 1.284181\n",
      "Train Epoch: 7 [22016/70488 (31%)]\tLoss: 1.284308\n",
      "Train Epoch: 7 [22528/70488 (32%)]\tLoss: 1.284216\n",
      "Train Epoch: 7 [23040/70488 (33%)]\tLoss: 1.283324\n",
      "Train Epoch: 7 [23552/70488 (33%)]\tLoss: 1.283830\n",
      "Train Epoch: 7 [24064/70488 (34%)]\tLoss: 1.284248\n",
      "Train Epoch: 7 [24576/70488 (35%)]\tLoss: 1.284811\n",
      "Train Epoch: 7 [25088/70488 (36%)]\tLoss: 1.284193\n",
      "Train Epoch: 7 [25600/70488 (36%)]\tLoss: 1.283966\n",
      "Train Epoch: 7 [26112/70488 (37%)]\tLoss: 1.283436\n",
      "Train Epoch: 7 [26624/70488 (38%)]\tLoss: 1.282576\n",
      "Train Epoch: 7 [27136/70488 (38%)]\tLoss: 1.284605\n",
      "Train Epoch: 7 [27648/70488 (39%)]\tLoss: 1.284084\n",
      "Train Epoch: 7 [28160/70488 (40%)]\tLoss: 1.283451\n",
      "Train Epoch: 7 [28672/70488 (41%)]\tLoss: 1.284220\n",
      "Train Epoch: 7 [29184/70488 (41%)]\tLoss: 1.284524\n",
      "Train Epoch: 7 [29696/70488 (42%)]\tLoss: 1.283429\n",
      "Train Epoch: 7 [30208/70488 (43%)]\tLoss: 1.283918\n",
      "Train Epoch: 7 [30720/70488 (43%)]\tLoss: 1.284269\n",
      "Train Epoch: 7 [31232/70488 (44%)]\tLoss: 1.284884\n",
      "Train Epoch: 7 [31744/70488 (45%)]\tLoss: 1.283467\n",
      "Train Epoch: 7 [32256/70488 (46%)]\tLoss: 1.283921\n",
      "Train Epoch: 7 [32768/70488 (46%)]\tLoss: 1.284893\n",
      "Train Epoch: 7 [33280/70488 (47%)]\tLoss: 1.283278\n",
      "Train Epoch: 7 [33792/70488 (48%)]\tLoss: 1.284325\n",
      "Train Epoch: 7 [34304/70488 (49%)]\tLoss: 1.283549\n",
      "Train Epoch: 7 [34816/70488 (49%)]\tLoss: 1.284305\n",
      "Train Epoch: 7 [35328/70488 (50%)]\tLoss: 1.284340\n",
      "Train Epoch: 7 [35840/70488 (51%)]\tLoss: 1.284335\n",
      "Train Epoch: 7 [36352/70488 (51%)]\tLoss: 1.283305\n",
      "Train Epoch: 7 [36864/70488 (52%)]\tLoss: 1.283942\n",
      "Train Epoch: 7 [37376/70488 (53%)]\tLoss: 1.285387\n",
      "Train Epoch: 7 [37888/70488 (54%)]\tLoss: 1.284631\n",
      "Train Epoch: 7 [38400/70488 (54%)]\tLoss: 1.284427\n",
      "Train Epoch: 7 [38912/70488 (55%)]\tLoss: 1.282677\n",
      "Train Epoch: 7 [39424/70488 (56%)]\tLoss: 1.284203\n",
      "Train Epoch: 7 [39936/70488 (57%)]\tLoss: 1.283948\n",
      "Train Epoch: 7 [40448/70488 (57%)]\tLoss: 1.283323\n",
      "Train Epoch: 7 [40960/70488 (58%)]\tLoss: 1.283883\n",
      "Train Epoch: 7 [41472/70488 (59%)]\tLoss: 1.283319\n",
      "Train Epoch: 7 [41984/70488 (59%)]\tLoss: 1.284059\n",
      "Train Epoch: 7 [42496/70488 (60%)]\tLoss: 1.284454\n",
      "Train Epoch: 7 [43008/70488 (61%)]\tLoss: 1.283920\n",
      "Train Epoch: 7 [43520/70488 (62%)]\tLoss: 1.284299\n",
      "Train Epoch: 7 [44032/70488 (62%)]\tLoss: 1.284617\n",
      "Train Epoch: 7 [44544/70488 (63%)]\tLoss: 1.284303\n",
      "Train Epoch: 7 [45056/70488 (64%)]\tLoss: 1.284097\n",
      "Train Epoch: 7 [45568/70488 (64%)]\tLoss: 1.284460\n",
      "Train Epoch: 7 [46080/70488 (65%)]\tLoss: 1.283534\n",
      "Train Epoch: 7 [46592/70488 (66%)]\tLoss: 1.284439\n",
      "Train Epoch: 7 [47104/70488 (67%)]\tLoss: 1.284391\n",
      "Train Epoch: 7 [47616/70488 (67%)]\tLoss: 1.284725\n",
      "Train Epoch: 7 [48128/70488 (68%)]\tLoss: 1.283529\n",
      "Train Epoch: 7 [48640/70488 (69%)]\tLoss: 1.283857\n",
      "Train Epoch: 7 [49152/70488 (70%)]\tLoss: 1.283665\n",
      "Train Epoch: 7 [49664/70488 (70%)]\tLoss: 1.283358\n",
      "Train Epoch: 7 [50176/70488 (71%)]\tLoss: 1.283774\n",
      "Train Epoch: 7 [50688/70488 (72%)]\tLoss: 1.283745\n",
      "Train Epoch: 7 [51200/70488 (72%)]\tLoss: 1.283988\n",
      "Train Epoch: 7 [51712/70488 (73%)]\tLoss: 1.284124\n",
      "Train Epoch: 7 [52224/70488 (74%)]\tLoss: 1.284564\n",
      "Train Epoch: 7 [52736/70488 (75%)]\tLoss: 1.282182\n",
      "Train Epoch: 7 [53248/70488 (75%)]\tLoss: 1.283950\n",
      "Train Epoch: 7 [53760/70488 (76%)]\tLoss: 1.282657\n",
      "Train Epoch: 7 [54272/70488 (77%)]\tLoss: 1.284023\n",
      "Train Epoch: 7 [54784/70488 (78%)]\tLoss: 1.283247\n",
      "Train Epoch: 7 [55296/70488 (78%)]\tLoss: 1.284038\n",
      "Train Epoch: 7 [55808/70488 (79%)]\tLoss: 1.284049\n",
      "Train Epoch: 7 [56320/70488 (80%)]\tLoss: 1.283301\n",
      "Train Epoch: 7 [56832/70488 (80%)]\tLoss: 1.283940\n",
      "Train Epoch: 7 [57344/70488 (81%)]\tLoss: 1.282776\n",
      "Train Epoch: 7 [57856/70488 (82%)]\tLoss: 1.285001\n",
      "Train Epoch: 7 [58368/70488 (83%)]\tLoss: 1.284018\n",
      "Train Epoch: 7 [58880/70488 (83%)]\tLoss: 1.283360\n",
      "Train Epoch: 7 [59392/70488 (84%)]\tLoss: 1.284020\n",
      "Train Epoch: 7 [59904/70488 (85%)]\tLoss: 1.284393\n",
      "Train Epoch: 7 [60416/70488 (86%)]\tLoss: 1.284487\n",
      "Train Epoch: 7 [60928/70488 (86%)]\tLoss: 1.284154\n",
      "Train Epoch: 7 [61440/70488 (87%)]\tLoss: 1.283493\n",
      "Train Epoch: 7 [61952/70488 (88%)]\tLoss: 1.284261\n",
      "Train Epoch: 7 [62464/70488 (88%)]\tLoss: 1.284060\n",
      "Train Epoch: 7 [62976/70488 (89%)]\tLoss: 1.283846\n",
      "Train Epoch: 7 [63488/70488 (90%)]\tLoss: 1.283306\n",
      "Train Epoch: 7 [64000/70488 (91%)]\tLoss: 1.283190\n",
      "Train Epoch: 7 [64512/70488 (91%)]\tLoss: 1.284721\n",
      "Train Epoch: 7 [65024/70488 (92%)]\tLoss: 1.284246\n",
      "Train Epoch: 7 [65536/70488 (93%)]\tLoss: 1.283633\n",
      "Train Epoch: 7 [66048/70488 (93%)]\tLoss: 1.284960\n",
      "Train Epoch: 7 [66560/70488 (94%)]\tLoss: 1.283097\n",
      "Train Epoch: 7 [67072/70488 (95%)]\tLoss: 1.283803\n",
      "Train Epoch: 7 [67584/70488 (96%)]\tLoss: 1.283435\n",
      "Train Epoch: 7 [68096/70488 (96%)]\tLoss: 1.284713\n",
      "Train Epoch: 7 [68608/70488 (97%)]\tLoss: 1.283436\n",
      "Train Epoch: 7 [69120/70488 (98%)]\tLoss: 1.283257\n",
      "Train Epoch: 7 [69632/70488 (99%)]\tLoss: 1.284335\n",
      "Train Epoch: 7 [47128/70488 (99%)]\tLoss: 1.283341\n",
      "====> Epoch: 7 Average loss: 0.00502741\n",
      "====> Test set loss: 0.00515276\n",
      "Train Epoch: 8 [0/70488 (0%)]\tLoss: 1.284227\n",
      "Train Epoch: 8 [512/70488 (1%)]\tLoss: 1.283633\n",
      "Train Epoch: 8 [1024/70488 (1%)]\tLoss: 1.284251\n",
      "Train Epoch: 8 [1536/70488 (2%)]\tLoss: 1.283134\n",
      "Train Epoch: 8 [2048/70488 (3%)]\tLoss: 1.283285\n",
      "Train Epoch: 8 [2560/70488 (4%)]\tLoss: 1.283769\n",
      "Train Epoch: 8 [3072/70488 (4%)]\tLoss: 1.284639\n",
      "Train Epoch: 8 [3584/70488 (5%)]\tLoss: 1.283314\n",
      "Train Epoch: 8 [4096/70488 (6%)]\tLoss: 1.282806\n",
      "Train Epoch: 8 [4608/70488 (7%)]\tLoss: 1.283132\n",
      "Train Epoch: 8 [5120/70488 (7%)]\tLoss: 1.282845\n",
      "Train Epoch: 8 [5632/70488 (8%)]\tLoss: 1.283435\n",
      "Train Epoch: 8 [6144/70488 (9%)]\tLoss: 1.283490\n",
      "Train Epoch: 8 [6656/70488 (9%)]\tLoss: 1.283577\n",
      "Train Epoch: 8 [7168/70488 (10%)]\tLoss: 1.283024\n",
      "Train Epoch: 8 [7680/70488 (11%)]\tLoss: 1.284563\n",
      "Train Epoch: 8 [8192/70488 (12%)]\tLoss: 1.284280\n",
      "Train Epoch: 8 [8704/70488 (12%)]\tLoss: 1.284585\n",
      "Train Epoch: 8 [9216/70488 (13%)]\tLoss: 1.284217\n",
      "Train Epoch: 8 [9728/70488 (14%)]\tLoss: 1.284571\n",
      "Train Epoch: 8 [10240/70488 (14%)]\tLoss: 1.283492\n",
      "Train Epoch: 8 [10752/70488 (15%)]\tLoss: 1.284580\n",
      "Train Epoch: 8 [11264/70488 (16%)]\tLoss: 1.284128\n",
      "Train Epoch: 8 [11776/70488 (17%)]\tLoss: 1.284510\n",
      "Train Epoch: 8 [12288/70488 (17%)]\tLoss: 1.285008\n",
      "Train Epoch: 8 [12800/70488 (18%)]\tLoss: 1.285112\n",
      "Train Epoch: 8 [13312/70488 (19%)]\tLoss: 1.284271\n",
      "Train Epoch: 8 [13824/70488 (20%)]\tLoss: 1.285126\n",
      "Train Epoch: 8 [14336/70488 (20%)]\tLoss: 1.283198\n",
      "Train Epoch: 8 [14848/70488 (21%)]\tLoss: 1.284037\n",
      "Train Epoch: 8 [15360/70488 (22%)]\tLoss: 1.284359\n",
      "Train Epoch: 8 [15872/70488 (22%)]\tLoss: 1.284055\n",
      "Train Epoch: 8 [16384/70488 (23%)]\tLoss: 1.283580\n",
      "Train Epoch: 8 [16896/70488 (24%)]\tLoss: 1.284637\n",
      "Train Epoch: 8 [17408/70488 (25%)]\tLoss: 1.284062\n",
      "Train Epoch: 8 [17920/70488 (25%)]\tLoss: 1.283531\n",
      "Train Epoch: 8 [18432/70488 (26%)]\tLoss: 1.284366\n",
      "Train Epoch: 8 [18944/70488 (27%)]\tLoss: 1.284559\n",
      "Train Epoch: 8 [19456/70488 (28%)]\tLoss: 1.284230\n",
      "Train Epoch: 8 [19968/70488 (28%)]\tLoss: 1.284124\n",
      "Train Epoch: 8 [20480/70488 (29%)]\tLoss: 1.283688\n",
      "Train Epoch: 8 [20992/70488 (30%)]\tLoss: 1.284563\n",
      "Train Epoch: 8 [21504/70488 (30%)]\tLoss: 1.284972\n",
      "Train Epoch: 8 [22016/70488 (31%)]\tLoss: 1.284288\n",
      "Train Epoch: 8 [22528/70488 (32%)]\tLoss: 1.284852\n",
      "Train Epoch: 8 [23040/70488 (33%)]\tLoss: 1.283973\n",
      "Train Epoch: 8 [23552/70488 (33%)]\tLoss: 1.284562\n",
      "Train Epoch: 8 [24064/70488 (34%)]\tLoss: 1.283790\n",
      "Train Epoch: 8 [24576/70488 (35%)]\tLoss: 1.283088\n",
      "Train Epoch: 8 [25088/70488 (36%)]\tLoss: 1.284234\n",
      "Train Epoch: 8 [25600/70488 (36%)]\tLoss: 1.284124\n",
      "Train Epoch: 8 [26112/70488 (37%)]\tLoss: 1.284764\n",
      "Train Epoch: 8 [26624/70488 (38%)]\tLoss: 1.284176\n",
      "Train Epoch: 8 [27136/70488 (38%)]\tLoss: 1.284072\n",
      "Train Epoch: 8 [27648/70488 (39%)]\tLoss: 1.284439\n",
      "Train Epoch: 8 [28160/70488 (40%)]\tLoss: 1.284776\n",
      "Train Epoch: 8 [28672/70488 (41%)]\tLoss: 1.284670\n",
      "Train Epoch: 8 [29184/70488 (41%)]\tLoss: 1.283011\n",
      "Train Epoch: 8 [29696/70488 (42%)]\tLoss: 1.284762\n",
      "Train Epoch: 8 [30208/70488 (43%)]\tLoss: 1.283486\n",
      "Train Epoch: 8 [30720/70488 (43%)]\tLoss: 1.284126\n",
      "Train Epoch: 8 [31232/70488 (44%)]\tLoss: 1.284063\n",
      "Train Epoch: 8 [31744/70488 (45%)]\tLoss: 1.283545\n",
      "Train Epoch: 8 [32256/70488 (46%)]\tLoss: 1.283855\n",
      "Train Epoch: 8 [32768/70488 (46%)]\tLoss: 1.284128\n",
      "Train Epoch: 8 [33280/70488 (47%)]\tLoss: 1.284669\n",
      "Train Epoch: 8 [33792/70488 (48%)]\tLoss: 1.283020\n",
      "Train Epoch: 8 [34304/70488 (49%)]\tLoss: 1.283997\n",
      "Train Epoch: 8 [34816/70488 (49%)]\tLoss: 1.283214\n",
      "Train Epoch: 8 [35328/70488 (50%)]\tLoss: 1.284257\n",
      "Train Epoch: 8 [35840/70488 (51%)]\tLoss: 1.284233\n",
      "Train Epoch: 8 [36352/70488 (51%)]\tLoss: 1.284782\n",
      "Train Epoch: 8 [36864/70488 (52%)]\tLoss: 1.283899\n",
      "Train Epoch: 8 [37376/70488 (53%)]\tLoss: 1.284800\n",
      "Train Epoch: 8 [37888/70488 (54%)]\tLoss: 1.284423\n",
      "Train Epoch: 8 [38400/70488 (54%)]\tLoss: 1.284202\n",
      "Train Epoch: 8 [38912/70488 (55%)]\tLoss: 1.283606\n",
      "Train Epoch: 8 [39424/70488 (56%)]\tLoss: 1.284789\n",
      "Train Epoch: 8 [39936/70488 (57%)]\tLoss: 1.284307\n",
      "Train Epoch: 8 [40448/70488 (57%)]\tLoss: 1.284346\n",
      "Train Epoch: 8 [40960/70488 (58%)]\tLoss: 1.283659\n",
      "Train Epoch: 8 [41472/70488 (59%)]\tLoss: 1.284397\n",
      "Train Epoch: 8 [41984/70488 (59%)]\tLoss: 1.283515\n",
      "Train Epoch: 8 [42496/70488 (60%)]\tLoss: 1.283830\n",
      "Train Epoch: 8 [43008/70488 (61%)]\tLoss: 1.283382\n",
      "Train Epoch: 8 [43520/70488 (62%)]\tLoss: 1.284364\n",
      "Train Epoch: 8 [44032/70488 (62%)]\tLoss: 1.284375\n",
      "Train Epoch: 8 [44544/70488 (63%)]\tLoss: 1.284452\n",
      "Train Epoch: 8 [45056/70488 (64%)]\tLoss: 1.284263\n",
      "Train Epoch: 8 [45568/70488 (64%)]\tLoss: 1.284092\n",
      "Train Epoch: 8 [46080/70488 (65%)]\tLoss: 1.285114\n",
      "Train Epoch: 8 [46592/70488 (66%)]\tLoss: 1.284129\n",
      "Train Epoch: 8 [47104/70488 (67%)]\tLoss: 1.284429\n",
      "Train Epoch: 8 [47616/70488 (67%)]\tLoss: 1.283083\n",
      "Train Epoch: 8 [48128/70488 (68%)]\tLoss: 1.283870\n",
      "Train Epoch: 8 [48640/70488 (69%)]\tLoss: 1.283395\n",
      "Train Epoch: 8 [49152/70488 (70%)]\tLoss: 1.282974\n",
      "Train Epoch: 8 [49664/70488 (70%)]\tLoss: 1.284175\n",
      "Train Epoch: 8 [50176/70488 (71%)]\tLoss: 1.284263\n",
      "Train Epoch: 8 [50688/70488 (72%)]\tLoss: 1.283807\n",
      "Train Epoch: 8 [51200/70488 (72%)]\tLoss: 1.283757\n",
      "Train Epoch: 8 [51712/70488 (73%)]\tLoss: 1.284089\n",
      "Train Epoch: 8 [52224/70488 (74%)]\tLoss: 1.283432\n",
      "Train Epoch: 8 [52736/70488 (75%)]\tLoss: 1.283761\n",
      "Train Epoch: 8 [53248/70488 (75%)]\tLoss: 1.284831\n",
      "Train Epoch: 8 [53760/70488 (76%)]\tLoss: 1.283181\n",
      "Train Epoch: 8 [54272/70488 (77%)]\tLoss: 1.284378\n",
      "Train Epoch: 8 [54784/70488 (78%)]\tLoss: 1.282934\n",
      "Train Epoch: 8 [55296/70488 (78%)]\tLoss: 1.284227\n",
      "Train Epoch: 8 [55808/70488 (79%)]\tLoss: 1.283734\n",
      "Train Epoch: 8 [56320/70488 (80%)]\tLoss: 1.284290\n",
      "Train Epoch: 8 [56832/70488 (80%)]\tLoss: 1.283732\n",
      "Train Epoch: 8 [57344/70488 (81%)]\tLoss: 1.283658\n",
      "Train Epoch: 8 [57856/70488 (82%)]\tLoss: 1.283768\n",
      "Train Epoch: 8 [58368/70488 (83%)]\tLoss: 1.283993\n",
      "Train Epoch: 8 [58880/70488 (83%)]\tLoss: 1.282604\n",
      "Train Epoch: 8 [59392/70488 (84%)]\tLoss: 1.283580\n",
      "Train Epoch: 8 [59904/70488 (85%)]\tLoss: 1.282924\n",
      "Train Epoch: 8 [60416/70488 (86%)]\tLoss: 1.282760\n",
      "Train Epoch: 8 [60928/70488 (86%)]\tLoss: 1.284053\n",
      "Train Epoch: 8 [61440/70488 (87%)]\tLoss: 1.285039\n",
      "Train Epoch: 8 [61952/70488 (88%)]\tLoss: 1.283373\n",
      "Train Epoch: 8 [62464/70488 (88%)]\tLoss: 1.283189\n",
      "Train Epoch: 8 [62976/70488 (89%)]\tLoss: 1.283663\n",
      "Train Epoch: 8 [63488/70488 (90%)]\tLoss: 1.283102\n",
      "Train Epoch: 8 [64000/70488 (91%)]\tLoss: 1.283086\n",
      "Train Epoch: 8 [64512/70488 (91%)]\tLoss: 1.283046\n",
      "Train Epoch: 8 [65024/70488 (92%)]\tLoss: 1.284339\n",
      "Train Epoch: 8 [65536/70488 (93%)]\tLoss: 1.284703\n",
      "Train Epoch: 8 [66048/70488 (93%)]\tLoss: 1.284062\n",
      "Train Epoch: 8 [66560/70488 (94%)]\tLoss: 1.284333\n",
      "Train Epoch: 8 [67072/70488 (95%)]\tLoss: 1.283272\n",
      "Train Epoch: 8 [67584/70488 (96%)]\tLoss: 1.282926\n",
      "Train Epoch: 8 [68096/70488 (96%)]\tLoss: 1.283436\n",
      "Train Epoch: 8 [68608/70488 (97%)]\tLoss: 1.284180\n",
      "Train Epoch: 8 [69120/70488 (98%)]\tLoss: 1.283811\n",
      "Train Epoch: 8 [69632/70488 (99%)]\tLoss: 1.284030\n",
      "Train Epoch: 8 [47128/70488 (99%)]\tLoss: 1.283091\n",
      "====> Epoch: 8 Average loss: 0.00502742\n",
      "====> Test set loss: 0.00515287\n"
     ]
    }
   ],
   "source": [
    "def train(epoch,recode_dict):\n",
    "    global step\n",
    "    model.train()\n",
    "    writer = SummaryWriter()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb = data[0]\n",
    "        target = data[1]\n",
    "        target = recode_tags(target,recode_dict)\n",
    "        batch_size = target.shape[0]\n",
    "        #preds = F.one_hot(preds.to(torch.int64))\n",
    "        target = target.reshape(batch_size,128,128)\n",
    "        target = target.to(device,dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        #dont need latent space output while training\n",
    "        y_batch,_ = model(rgb)\n",
    "        loss = loss_fn(y_batch,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_size * batch_idx, len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))\n",
    "\n",
    "        writer.add_scalar(\"AE Loss\", loss.item() / len(data[0]),step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "          epoch, avg_loss))\n",
    "    writer.flush()\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def test(epoch,recode_dict):\n",
    "    model.eval()\n",
    "    writer = SummaryWriter()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            rgb = data[0]\n",
    "            target = data[1]\n",
    "            batch_size = target.shape[0]\n",
    "            target = recode_tags(target,recode_dict)\n",
    "            #preds = F.one_hot(preds.to(torch.int64))\n",
    "            target = target.permute(0,3,1,2)\n",
    "            target= target.reshape(batch_size,128,128)\n",
    "            target = target.to(device,dtype=torch.long)\n",
    "            y_batch,_ = model(rgb)\n",
    "            test_loss += loss_fn(y_batch,target).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.8f}'.format(test_loss))\n",
    "    writer.add_scalar(\"AE test Loss\", test_loss,epoch)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "for epoch in range(1, 8 + 1):\n",
    "    step = 0\n",
    "    smallest_loss = 1000\n",
    "    \n",
    "    avg_loss = train(epoch,recode_dict)\n",
    "    if avg_loss < smallest_loss:\n",
    "        torch.save(model.state_dict(), './AE_params/model_48.best')\n",
    "    test(epoch,recode_dict)\n",
    "    torch.save(model.state_dict(), './AE_params/model_48.final')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b638e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_dict = {\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:2,\n",
    "    3:3,\n",
    "    4:4,\n",
    "    5:5,\n",
    "    6:6,\n",
    "    7:7,\n",
    "    8:8,\n",
    "    9:9,\n",
    "    10:10,\n",
    "    11:11,\n",
    "    12:12,\n",
    "    13:0,\n",
    "    14:3,\n",
    "    15:1,\n",
    "    16:3,\n",
    "    17:2,\n",
    "    18:5,\n",
    "    19:3,\n",
    "    20:4,\n",
    "    21:3,\n",
    "    22:9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07047bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_tags(sem_image,recode_dict):\n",
    "    for value in recode_dict.keys():\n",
    "        sem_image[sem_image==value] = recode_dict[value]\n",
    "    \n",
    "    return sem_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5052998d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./AE_params/model_46.best'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae97504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54563\n",
      "torch.Size([1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = random.randint(1,60000)\n",
    "print(num)\n",
    "data = train_data.__getitem__(num)\n",
    "imgs = []\n",
    "org = data[0]\n",
    "sem = replace(data[1].numpy().transpose(1,2,0))\n",
    "imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "imgs.append(Image.fromarray(sem))\n",
    "imgs.append(generate_semantic_im(data[0]))\n",
    "fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "for i, img in enumerate(imgs):\n",
    "    axs[0, i].imshow(np.asarray(img))\n",
    "    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "691a80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[70,130,180],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f6cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = RGB_image.reshape(1,3,128,128)\n",
    "    out,_ = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    print(sample.shape)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be49b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = T.ToPILImage()(img.to('cpu'))\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    num = random.randint(0,6000)\n",
    "    data = town03_data.__getitem__(num)\n",
    "    imgs = []\n",
    "    org = data[0]\n",
    "    org = TF.resized_crop(org,64,14,50,100,(128,128))\n",
    "    sem = TF.resized_crop(torch.tensor(data[1]).permute(2,0,1),64,14,50,100,(128,128))\n",
    "    \n",
    "    sem = replace(data[1])\n",
    "\n",
    "\n",
    "    imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "    imgs.append(Image.fromarray(sem.reshape(128,128,3)))\n",
    "    imgs.append(generate_semantic_im(org))\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453cf6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
