{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ec9984",
   "metadata": {},
   "source": [
    "# Learning to Drive in Adverse Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07881483",
   "metadata": {},
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "This project is designed to explore using Reinforcement learning to teach an autonomous agent to drive in adverse weather conditions.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- The project will be performed using an autonomous drving simulator called CARLA.\n",
    "- Python 3.8\n",
    "- Anconda\n",
    "\n",
    "[Project Github](https://github.com/rbuckley25/Tempestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdd1a5",
   "metadata": {},
   "source": [
    "### Image Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc38f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import carla\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "from models import PerceptionNet\n",
    "from utils import CropResizeTransform,Hflip,recode_tags\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58bddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, weather, town, test=False , transform=None, target_transform=None):\n",
    "        dirt = './Data/'+weather+'/'+town\n",
    "        if test:\n",
    "            dirt = dirt+'/test'\n",
    "        \n",
    "        self.sem_dir = dirt+'/Semantic'\n",
    "        self.rgb_dir = dirt+'/RGB'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.names = os.listdir(self.rgb_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.sem_dir))\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = os.path.join(self.rgb_dir, self.names[idx])\n",
    "        image = read_image(img_path)\n",
    "        label_name = self.names[idx].split('.')[0]+'.npy'\n",
    "        label = np.load(os.path.join(self.sem_dir, label_name))\n",
    "        label = torch.tensor(label).permute(2,0,1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fda9eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerceptionNet(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e4d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Conv2d) or isinstance(layer,torch.nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050d50fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initalize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138d98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights = np.array([ 426548.,  103471.,   59734.,   61954.,    3507.,   17818.,\n",
    "         40000., 2355300.,   28709.,  355806.,    4624.,   49085.,\n",
    "         21033.])\n",
    "\n",
    "# Some tags are almost empty so any sub 10,000 need to be brougt up otherwise to a baseline\n",
    "# If not the weighting is unusable \n",
    "clean_lables = np.where(label_freq_weights < 10000, 15000, label_freq_weights)\n",
    "\n",
    "inverse_lables = 1/clean_lables\n",
    "normalized_lables = inverse_lables/sum(inverse_lables)\n",
    "lable_weights = torch.tensor(normalized_lables,dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46cb67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=lable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337d41d",
   "metadata": {},
   "source": [
    "Reference: https://github.com/pytorch/examples/blob/master/vae/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd868ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = CropResizeTransform(64,14,50,100,(128,128))\n",
    "town03_data = CustomImageDataset('ClearNoon','Town03',test=False)\n",
    "town03_test_data = CustomImageDataset('ClearNoon','Town03',test=True)\n",
    "town04_data = CustomImageDataset('ClearNoon','Town04',test=False)\n",
    "town04_test_data = CustomImageDataset('ClearNoon','Town04',test=True)\n",
    "town07_data = CustomImageDataset('ClearNoon','Town07',test=False)\n",
    "#flipped data\n",
    "town03_data_hf = CustomImageDataset('ClearNoon','Town03',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town03_test_data_hf = CustomImageDataset('ClearNoon','Town03',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town04_data_hf = CustomImageDataset('ClearNoon','Town04',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "town04_test_data_hf = CustomImageDataset('ClearNoon','Town04',test=True,transform=Hflip(),target_transform=Hflip())\n",
    "town07_data_hf = CustomImageDataset('ClearNoon','Town07',test=False,transform=Hflip(),target_transform=Hflip())\n",
    "\n",
    "#town03_data_cropped = CustomImageDataset('Town03','.',test=False,transform=crop,target_transform=crop)\n",
    "#town03_test_data_cropped = CustomImageDataset('Town03','.',test=True,transform=crop,target_transform=crop)\n",
    "#town04_data_cropped = CustomImageDataset('Town04','.',test=False,transform=crop,target_transform=crop)\n",
    "#town04_test_data_cropped = CustomImageDataset('Town04','.',test=True,transform=crop,target_transform=crop)\n",
    "\n",
    "\n",
    "train_data = ConcatDataset([town03_data,town04_data,town07_data,town03_data_hf,town04_data_hf,town07_data_hf,\n",
    "                            ])\n",
    "#town03_data_cropped, town04_data_cropped\n",
    "test_data = ConcatDataset([town03_test_data,town04_test_data,town03_test_data_hf,town04_test_data_hf,\n",
    "                            ])\n",
    "#town03_test_data_cropped, town04_test_data_cropped\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fdde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(normalized_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d760b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,writer):\n",
    "    global step\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb = data[0]\n",
    "        target = data[1]\n",
    "        target = recode_tags(target)\n",
    "        batch_size = target.shape[0]\n",
    "        #preds = F.one_hot(preds.to(torch.int64))\n",
    "        target = target.reshape(batch_size,128,128)\n",
    "        target = target.to(device,dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        #dont need latent space output while training\n",
    "        y_batch,_  = model(rgb)\n",
    "        loss = loss_fn(y_batch,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_size * batch_idx, len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))\n",
    "\n",
    "        print(((epoch-1)*10)+step)\n",
    "        writer.add_scalar(\"AE Loss\", loss.item(), step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "          epoch, avg_loss))\n",
    "    writer.flush()\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def test(epoch,writer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            rgb = data[0]\n",
    "            target = data[1]\n",
    "            batch_size = target.shape[0]\n",
    "            target = recode_tags(target)\n",
    "            #preds = F.one_hot(preds.to(torch.int64))\n",
    "            target = target.permute(0,3,1,2)\n",
    "            target= target.reshape(batch_size,128,128)\n",
    "            target = target.to(device,dtype=torch.long)\n",
    "            y_batch,_   = model(rgb)\n",
    "            test_loss += loss_fn(y_batch,target).item()\n",
    "\n",
    "    print('====> Test set loss: {:.8f}'.format(test_loss))\n",
    "    writer.add_scalar(\"AE test Loss\", test_loss,epoch)\n",
    "    writer.flush()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "for epoch in range(1, 20 + 1):\n",
    "    \n",
    "    smallest_loss = 1000\n",
    "    \n",
    "    avg_loss = train(epoch,writer)\n",
    "    if avg_loss < smallest_loss:\n",
    "        torch.save(model.state_dict(), './AE_params/model_55.best')\n",
    "    test(epoch,writer)\n",
    "    torch.save(model.state_dict(), './AE_params/model_55.final')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./AE_params/model_4.best'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = town03_data = CustomImageDataset('.','Town03',test=False)\n",
    "old_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = random.randint(1,70000)\n",
    "print(num)\n",
    "data = train_data.__getitem__(num)\n",
    "imgs = []\n",
    "org = data[0]\n",
    "sem = replace(data[1].numpy().transpose(1,2,0))\n",
    "imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "imgs.append(Image.fromarray(sem))\n",
    "imgs.append(generate_semantic_im(data[0],model))\n",
    "fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "for i, img in enumerate(imgs):\n",
    "    axs[0, i].imshow(np.asarray(img))\n",
    "    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    #img.save('./AE_results/AE/'+str(num)+'_'+str(i)+'.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[70,130,180],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = RGB_image.reshape(1,3,128,128)\n",
    "    out,_ ,_,_ = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    print(sample.shape)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be49b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = T.ToPILImage()(img.to('cpu'))\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    num = random.randint(0,6000)\n",
    "    data = town03_data.__getitem__(num)\n",
    "    imgs = []\n",
    "    org = data[0]\n",
    "    org = TF.resized_crop(org,64,14,50,100,(128,128))\n",
    "    sem = TF.resized_crop(torch.tensor(data[1]).permute(2,0,1),64,14,50,100,(128,128))\n",
    "    \n",
    "    sem = replace(data[1])\n",
    "\n",
    "\n",
    "    imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "    imgs.append(Image.fromarray(sem.reshape(128,128,3)))\n",
    "    imgs.append(generate_semantic_im(org))\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453cf6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
