{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ec9984",
   "metadata": {},
   "source": [
    "# Learning to Drive in Adverse Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6519255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07881483",
   "metadata": {},
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "This project is designed to explore using Reinforcement learning to teach an autonomous agent to drive in adverse weather conditions.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- The project will be performed using an autonomous drving simulator called CARLA.\n",
    "- Python 3.8\n",
    "- Anconda\n",
    "\n",
    "[Project Github](https://github.com/rbuckley25/Tempestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdd1a5",
   "metadata": {},
   "source": [
    "### Image Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc38f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import carla\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f58bddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, weather, town, test=False , transform=None, target_transform=None):\n",
    "        dirt = './Datasets/'+weather+'/'+town\n",
    "        if test:\n",
    "            dirt = dirt+'/test'\n",
    "        \n",
    "        self.sem_dir = dirt+'/Semantic'\n",
    "        self.rgb_dir = dirt+'/RGB'\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.names = os.listdir(self.rgb_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.sem_dir))\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = os.path.join(self.rgb_dir, self.names[idx])\n",
    "        image = read_image(img_path)\n",
    "        label_name = self.names[idx].split('.')[0]+'.npy'\n",
    "        label = np.load(os.path.join(self.sem_dir, label_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "378cfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptionNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PerceptionNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv6a = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        self.conv6b = nn.Conv2d(512, 64, kernel_size=4, stride=1)\n",
    "        \n",
    "        self.conv7 = torch.nn.ConvTranspose2d(64,512, kernel_size =4, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = torch.nn.ConvTranspose2d(512,256, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = torch.nn.ConvTranspose2d(256,128, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv10 = torch.nn.ConvTranspose2d(128,64, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv11 = torch.nn.ConvTranspose2d(64,32, kernel_size =4, stride=2, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv12 = torch.nn.ConvTranspose2d(32,13, kernel_size =4, stride=2,padding=1)\n",
    "        \n",
    "            \n",
    "    def encode(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)),negative_slope=0.02)\n",
    "        return self.conv6a(x), self.conv6b(x)\n",
    "\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.leaky_relu(self.bn6(self.conv7(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn7(self.conv8(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn8(self.conv9(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn9(self.conv10(x)),negative_slope=0.02)\n",
    "        x = F.leaky_relu(self.bn10(self.conv11(x)),negative_slope=0.02)\n",
    "        return F.softmax(self.conv12(x),dim=1)\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent_sample = mu + eps*std\n",
    "        return latent_sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        mu,logvar = self.encode(x)\n",
    "        latent = self.reparameterize(mu,logvar)\n",
    "        out = self.decode(latent)\n",
    "        return out, latent , mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fda9eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerceptionNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e4d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(layer):\n",
    "    if isinstance(layer, torch.nn.Conv2d) or isinstance(layer,torch.nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(layer.weight.data,nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "050d50fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceptionNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6a): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv6b): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv7): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (bn10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv12): ConvTranspose2d(32, 13, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initalize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46cb67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_weights = torch.tensor(normalized_lables,dtype=torch.float32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=lable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc66f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_batch,pred,mu,logvar,weights):\n",
    "    cross = nn.CrossEntropyLoss(weights,reduction='sum')\n",
    "    rec_loss = cross(y_batch,pred)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rec_loss + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337d41d",
   "metadata": {},
   "source": [
    "Reference: https://github.com/pytorch/examples/blob/master/vae/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd868ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "town03_data = CustomImageDataset('ClearNoon','Town03',test=False)\n",
    "town03_test_data = CustomImageDataset('ClearNoon','Town03',test=True)\n",
    "town04_data = CustomImageDataset('ClearNoon','Town04',test=False)\n",
    "town04_test_data = CustomImageDataset('ClearNoon','Town04',test=True)\n",
    "train_data = ConcatDataset([town03_data,town04_data])\n",
    "test_data = ConcatDataset([town03_test_data,town04_test_data])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate label frequency for loss weigthing\n",
    "label_freq_weights = np.zeros(13)\n",
    "count = 0\n",
    "for index, image in enumerate(train_data):\n",
    "    if index % 100 == 0:\n",
    "        im = recode_tags(image[1],recode_dict)\n",
    "        values, counts = np.unique(im,return_counts=True)\n",
    "        label_freq_weights[values] += counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb8dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq_weights = np.array([2198123.,  321519.,   74114.,  139571.,    9590.,   49337.,\n",
    "         94720., 1339440.,   78098.,  407971., 1242280.,  132659.,\n",
    "         40194.])\n",
    "clean_lables = np.where(label_freq_weights < 10000, 15000, label_freq_weights)\n",
    "\n",
    "inverse_lables = 1/clean_lables\n",
    "normalized_lables = inverse_lables/sum(inverse_lables)\n",
    "lable_weights = torch.tensor(normalized_lables,dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9fdde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3527936150>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(normalized_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to caculate an individual weight for each sample for BCELoss\n",
    "def calculate_weights(im_batch):\n",
    "    weights = torch.empty(13)\n",
    "    counts = torch.bincount(im_batch[i].reshape(128*128),minlength=13).numpy()\n",
    "    counts = np.where(counts < 1, 1, counts)\n",
    "    weights = torch.tensor((1/counts)/sum(1/counts))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d760b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23454/1653795861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0msmallest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mlable_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_lables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecode_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msmallest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./AE_params/model_36.best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23454/1653795861.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, recode_dict, weigth)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecode_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecode_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#preds = F.one_hot(preds.to(torch.int64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23454/3900303582.py\u001b[0m in \u001b[0;36mrecode_tags\u001b[0;34m(sem_image, recode_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecode_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msem_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecode_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecode_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msem_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msem_image\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msem_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch,recode_dict,weigth):\n",
    "    global step\n",
    "    model.train()\n",
    "    writer = SummaryWriter()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb = data[0]\n",
    "        target = data[1]\n",
    "        target = recode_tags(target,recode_dict)\n",
    "        batch_size = target.shape[0]\n",
    "        #preds = F.one_hot(preds.to(torch.int64))\n",
    "        target = target.permute(0,3,1,2).reshape(batch_size,128,128)\n",
    "        target = target.to(device,dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        #dont need latent space output while training\n",
    "        y_batch,_ ,mu,logvar= model(rgb)\n",
    "        loss = loss_fn(y_batch,target,mu,logvar,weigth)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        writer.add_scalar(\"AE Loss\", loss.item() / len(data[0]),step)\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.8f}'.format(\n",
    "          epoch, avg_loss))\n",
    "    writer.flush()\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def test(epoch,recode_dict,weight):\n",
    "    model.eval()\n",
    "    writer = SummaryWriter()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            rgb = data[0]\n",
    "            target = data[1]\n",
    "            batch_size = target.shape[0]\n",
    "            target = recode_tags(target,recode_dict)\n",
    "            #preds = F.one_hot(preds.to(torch.int64))\n",
    "            target = target.permute(0,3,1,2)\n",
    "            target= target.reshape(batch_size,128,128)\n",
    "            target = target.to(device,dtype=torch.long)\n",
    "            y_batch,_ ,mu,logvar= model(rgb)\n",
    "            test_loss += loss_fn(y_batch,target,mu,logvar,weight).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.8f}'.format(test_loss))\n",
    "    writer.add_scalar(\"AE test Loss\", test_loss,epoch)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "for epoch in range(1, 1 + 1):\n",
    "    step = 0\n",
    "    smallest_loss = 1000\n",
    "    lable_weights = torch.tensor(normalized_lables,dtype=torch.float32).to(device)\n",
    "    avg_loss = train(epoch,recode_dict,lable_weights)\n",
    "    if avg_loss < smallest_loss:\n",
    "        torch.save(model.state_dict(), './AE_params/model_36.best')\n",
    "    test(epoch,recode_dict,lable_weights)\n",
    "    torch.save(model.state_dict(), './AE_params/model_36.final')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b638e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_dict = {\n",
    "    0:0,\n",
    "    1:1,\n",
    "    2:2,\n",
    "    3:3,\n",
    "    4:4,\n",
    "    5:5,\n",
    "    6:6,\n",
    "    7:7,\n",
    "    8:8,\n",
    "    9:9,\n",
    "    10:10,\n",
    "    11:11,\n",
    "    12:12,\n",
    "    13:0,\n",
    "    14:3,\n",
    "    15:1,\n",
    "    16:3,\n",
    "    17:2,\n",
    "    18:5,\n",
    "    19:3,\n",
    "    20:4,\n",
    "    21:3,\n",
    "    22:9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07047bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_tags(sem_image,recode_dict):\n",
    "    for value in recode_dict.keys():\n",
    "        sem_image[sem_image==value] = recode_dict[value]\n",
    "    \n",
    "    return sem_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./AE_params/model_34.best'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = random.randint(0,6000)\n",
    "print(num)\n",
    "data = train_data.__getitem__(num)\n",
    "imgs = []\n",
    "org = data[0]\n",
    "sem = replace(data[1])\n",
    "imgs.append(Image.fromarray(org.numpy().transpose(1,2,0)))\n",
    "imgs.append(Image.fromarray(sem.reshape(128,128,3)))\n",
    "imgs.append(generate_semantic_im(data[0]))\n",
    "fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "for i, img in enumerate(imgs):\n",
    "    axs[0, i].imshow(np.asarray(img))\n",
    "    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_convert_dict = {0:[0,0,0],\n",
    "                   1:[70,70,70],\n",
    "                   2:[100,40,40],\n",
    "                   3:[55,90,80],\n",
    "                   4:[220,20,60],\n",
    "                   5:[153,153,153],\n",
    "                   6:[157,234,50],\n",
    "                   7:[128,64,128],\n",
    "                   8:[244,35,232],\n",
    "                   9:[107,142,35],\n",
    "                   10:[0,0,142],\n",
    "                   11:[102,102,156],\n",
    "                   12:[220,220,0],\n",
    "                   13:[70,130,180],\n",
    "                   14:[81,0,81],\n",
    "                   15:[150,100,100],\n",
    "                   16:[230,150,140],\n",
    "                   17:[180,165,180],\n",
    "                   18:[250,170,30],\n",
    "                   19:[110,190,160],\n",
    "                   20:[170,120,50],\n",
    "                   21:[45,60,150],\n",
    "                   22:[145,170,100],\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82f6cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_im(RGB_image):\n",
    "    new_obs = RGB_image\n",
    "    new_obs = new_obs.permute(2,0,1).reshape(1,3,128,128)\n",
    "    out,_ = model(new_obs)\n",
    "    sample = out.cpu().argmax(dim=1)\n",
    "    pic = replace(sample.numpy())\n",
    "    return Image.fromarray(pic,'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7be49b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(a):\n",
    "    a = a.reshape(128,128)\n",
    "    pic = np.zeros((128,128,3),dtype='uint8')\n",
    "    for x, y in np.ndindex(a.shape):\n",
    "        value = a[x,y]\n",
    "        RGB_values = tag_convert_dict[value]\n",
    "        pic[x,y,0] = RGB_values[0]\n",
    "        pic[x,y,1] = RGB_values[1]\n",
    "        pic[x,y,2] = RGB_values[2]\n",
    "    return pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = T.ToPILImage()(img.to('cpu'))\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(t_im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
